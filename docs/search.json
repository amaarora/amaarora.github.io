[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nGemma 2\n\n\nImproving Open Language Models at a Practical Size\n\n\n\nLLM\n\n\n\nIn this post, we take a deep dive into the architectural components of Gemma 2 such as Grouped Query Attention, Sliding Window Attention, RoPE Embeddings, Logit soft-capping & Model-merging! \n\n\n\n\n\nJul 9, 2024\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSliding Window Attention\n\n\nLongformer - The Long-Document Transformer\n\n\n\nLLM\n\n\n\nIn this post, we take a deep dive into Sliding Window Attention that allowed transformers to have long context length. We do this with the help of animations and also implement it from scrath in PyTorch code. \n\n\n\n\n\nJul 4, 2024\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nImage retrieval app using Apple’s 4M-21 any-to-any vision model\n\n\n4M-21 An Any-to-Any Vision Model for Tens of Tasks and Modalities\n\n\n\nVLM\n\n\n\nAs part of this blog post we are going to build an image retriever app that can take in three inputs - caption, brightness and number of items per image to retrieve the most similar image from a database based on their values. \n\n\n\n\n\nJul 1, 2024\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSupport bot with Claude 3.5 Sonnet using Claudette and Slack-SDK\n\n\nCreating a support bot that supports API calls using Claudette\n\n\n\nLLM\n\n\n\nAs part of this blog post we will build a support bot on Slack that can respond to queries in a slack channel using Claudette (a thin python wrapper on top of Anthropic CLI) \n\n\n\n\n\nJun 22, 2024\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain\n\n\n\n\n\nEmbark on an enlightening journey through the world of document-based question-answering chatbots using langchain! With a keen focus on detailed explanations and code walk-throughs, you’ll gain a deep understanding of each component - from creating a vector database to response generation.\n\n\n\n\n\nJul 28, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nDeciphering LangChain: A Deep Dive into Code Complexity\n\n\n\n\n\nAnalyzing LangChain’s source code reveals impressive modularity but also surprising complexity in executing simple text generation. The deep call stack makes tracing execution flow challenging.\n\n\n\n\n\nJul 25, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAhead of Times - Issue 2 (May 01 - May 07)\n\n\nSecond issue of the weekly newsletter to help you stay ahead of the times with latest news & updates in the field of AI.\n\n\n\nNewsletter\n\n\n\nAs part of this newsletter, I share with you key updates, projects, GitHub repos, research trends, research papers in the field of Computer Vision, Large Language Models and Stable Diffusion. \n\n\n\n\n\nMay 8, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAhead of Times - Issue 1 (Apr 24 - Apr 30)\n\n\nFirst issue of the weekly newsletter to help you stay ahead of the times with latest news & updates in the field of AI.\n\n\n\nNewsletter\n\n\n\nAs part of this newsletter, I share with you key updates, projects, GitHub repos, research trends, research papers in the field of Computer Vision, Large Language Models and Stable Diffusion. \n\n\n\n\n\nMay 2, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Review - ‘LaMini-LM’\n\n\nPaper review of “LaMini-LM - A Diverse Herd of Distilled Models from Large-Scale Instructions” and analysis on released 2.58M instruction dataset.\n\n\n\nLarge Language Models\n\n\nPaper Review\n\n\n\nAs part of this blog post, we regenerate a small sample of the 2.58M shared Instruction Dataset and also perform human evaluation on some of the generated models shared in the research paper. \n\n\n\n\n\nMay 1, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated CLIP (Part-2)\n\n\nLearning Transferable Visual Models From Natural Language Supervision\n\n\n\nMultimodal\n\n\nTransformers\n\n\nClip\n\n\n\nThis post is part-2 of the two series blog posts on CLIP (for part-1, please refer to my previous blog post). In this blog, we present the PyTorch code behind CLIP for model building and training. This blog post is in itself a working Jupyter Notebook. \n\n\n\n\n\nMar 11, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated CLIP (Part-1)\n\n\nLearning Transferable Visual Models From Natural Language Supervision\n\n\n\nMultimodal\n\n\nTransformers\n\n\n\nThis post is part-1 of the two series blog posts on CLIP. In this blog, we present an Introduction to CLIP in an easy to digest manner. We also compare CLIP to other research papers and look at the background and inspiration behind CLIP. \n\n\n\n\n\nMar 3, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSwin Transformer\n\n\nHierarchical Vision Transformer using Shifted Windows\n\n\n\nComputer Vision\n\n\nModel Architecure\n\n\nTransformers\n\n\n\nSwin Transformer Model Architecture explained with PyTorch implementation line-by-line. \n\n\n\n\n\nJul 4, 2022\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated DETR\n\n\nEnd-to-End Object Detection with Transformers\n\n\n\nComputer Vision\n\n\nModel Architecure\n\n\nObject Detection\n\n\nTransformers\n\n\n\nDETR Model Architecture explained with PyTorch implementation line-by-line. \n\n\n\n\n\nJul 26, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe sad state of AI and tech startups in Australia today and what can we do about it\n\n\n\n\n\n\nAI\n\n\nJeremy Howard\n\n\n\n\n\n\n\n\n\nMay 15, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAdam and friends\n\n\nAdam, SGD, RMSProp from scratch in PyTorch.\n\n\n\nComputer Vision\n\n\n\nBasic optimizers from scratch in PyTorch with working notebook. \n\n\n\n\n\nMar 13, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nVision Transformer\n\n\nAn Image is Worth 16x16 Words - Transformers for Image Recognition at Scale\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nTransformers\n\n\n\nIn this blog post, we will be looking at the Vision Transformer architectures in detail, and also re-implement in PyTorch from scratch. \n\n\n\n\n\nJan 18, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe EfficientDet Architecture in PyTorch\n\n\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nObject Detection\n\n\n\nIn this blog post, we will look at how to implement the EfficientDet architecture in PyTorch from scratch. \n\n\n\n\n\nJan 13, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nEfficientDet - Scalable and Efficient Object Detection\n\n\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nObject Detection\n\n\n\nAs part of this blog post I will explain how EfficientDets work step-by-step. \n\n\n\n\n\nJan 11, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nTop 100 solution - SIIM-ACR Pneumothorax Segmentation\n\n\n\n\n\n\nComputer Vision\n\n\nKaggle\n\n\nImage Segmentation\n\n\n\nIn this blog post, we will looking at Image Segmentation based problem in Pytorch with SIIM-ACR Pneumothorax Segmentation competition serving as a useful example and create a solution that will get us to the top-100 leaderboard position on Kaggle. \n\n\n\n\n\nSep 6, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nGeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval\n\n\n\n\n\n\nComputer Vision\n\n\n\nAs part of this blog post we will be looking at GeM pooling and also look at the research paper Fine-tuning CNN Image Retrieval with No Human Annotation. We also implement GeM Pooling from scratch in PyTorch. \n\n\n\n\n\nAug 30, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net A PyTorch Implementation in 60 lines of Code\n\n\nU-Net Convolutional Networks for Biomedical Image Segmentation\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nImage Segmentation\n\n\n\nAs part of this blog post we will implement the U-Net architecture in PyTorch in 60 lines of code. \n\n\n\n\n\nAug 30, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle\n\n\nWinning solution for SIIM-ISIC Melanoma Classification\n\n\n\nComputer Vision\n\n\nKaggle\n\n\n\nAs part of this blog post I share my winning solution for SIIM-ISIC Melanoma Classification Kaggle Competition. \n\n\n\n\n\nAug 23, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nEfficientNet\n\n\nRethinking Model Scaling for Convolutional Neural Networks\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\nLook at the current SOTA, with top-1 accuracy of 88.5% on ImageNet. \n\n\n\n\n\nAug 13, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nGroup Normalization\n\n\n\n\n\n\nComputer Vision\n\n\n\nIn this blog post, we will look at Group Normalization research paper and also implement Group Normalization in PyTorch from scratch. \n\n\n\n\n\nAug 9, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nDenseNet Architecture Explained with PyTorch Implementation from TorchVision\n\n\nDensely Connected Convolutional Networks\n\n\n\nProgramming\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\nIn this blog post, we introduce dense blocks, transition layers and look at the TorchVision implementation of DenseNet step-by-step. \n\n\n\n\n\nAug 2, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSqueeze and Excitation Networks Explained with PyTorch Implementation\n\n\nSqueeze-and-Excitation Networks\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\nIn this blogpost, we re-implement the Squeeze-and-Excitation networks in PyTorch step-by-step with very minor updates to ResNet implementation in torchvision. \n\n\n\n\n\nJul 24, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nLabel Smoothing Explained using Microsoft Excel\n\n\nBetter language models and their implications\n\n\n\nComputer Vision\n\n\n\nIn this blogpost, we re-implement Label Smoothing in Microsoft Excel step by step. \n\n\n\n\n\nJul 18, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to PyTorch Lightning with comparisons to PyTorch\n\n\nBetter language models and their implications\n\n\n\nProgramming\n\n\nComputer Vision\n\n\n\nIn this blogpost, we will be going through an introduction to Pytorch Lightning and implement all the cool tricks like - Gradient Accumulation, 16-bit precision training, and also add TPU/multi-gpu support - all in a few lines of code. We will use Pytorch Lightning to work on SIIM-ISIC Melanoma Classification challenge on Kaggle. \n\n\n\n\n\nJul 12, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Focal Loss and when should you use it?\n\n\nBetter language models and their implications\n\n\n\nComputer Vision\n\n\nLoss Function\n\n\n\nIn this blogpost, we will understand what Focal Loss and when is it used. We will also take a dive into its math and implement step-by-step in PyTorch. \n\n\n\n\n\nJun 29, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated GPT-2\n\n\nBetter language models and their implications\n\n\n\nNLP\n\n\nTransformers\n\n\n\nThis post presents an annotated version of the paper in the form of a line-by-line implementation in PyTorch. This document itself is a working notebook, and should be a completely usable implementation. \n\n\n\n\n\nFeb 18, 2020\n\n\nAman Arora\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n    \n        \n        Subscribe\n        * indicates required\n\n    Email Address  *"
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "",
    "text": "Recently I presented at REA Unstack’d on Large Language Models. It was mostly a demo about a ChatBot that I’ve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - PropTrack.\nLater on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment & all things LLM.\n\n\n\nREA Unstack’d\n\n\nMeet Sachin Abeywardana & Ned Letcher, our panelists.\nThere are many tutorials available today that showcase how to build a Q/A ChatBot, and most (if not all) use LangChain. Over the past few months, this framework has become extremely popular among all who want to use LLMs. But, its source code is hard to read and if you are trying to do something that’s not within the capabilities of the framework, it becomes extremely difficult.\n\n\nHere's a few thoughts on (LangChainAI?), the problems I see with it currently, and how I think it could improve. This was originally formatted as a message to (hwchase17?):Here's a few things off the top of my head – 1. Heavy use of OOP. Having multiple layers of abstraction…\n\n— Sam Hogan ((0xSamHogan?)) July 12, 2023\n\n\nI recently wrote about LLMChains in langchain too, and found the same to true. You can find the previous blog post here. I would highly recommend the readers to give the previous blog post a read, it will explain LLMChains and Chains in langchain, that will be instrumental in understanding conversational chatbot that we are building today.\n\n\n\n\n\n\nNote\n\n\n\n👉 This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. The blog post in itself is a completely working jupyter notebook with code-snippets."
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#introduction",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#introduction",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "",
    "text": "Recently I presented at REA Unstack’d on Large Language Models. It was mostly a demo about a ChatBot that I’ve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - PropTrack.\nLater on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment & all things LLM.\n\n\n\nREA Unstack’d\n\n\nMeet Sachin Abeywardana & Ned Letcher, our panelists.\nThere are many tutorials available today that showcase how to build a Q/A ChatBot, and most (if not all) use LangChain. Over the past few months, this framework has become extremely popular among all who want to use LLMs. But, its source code is hard to read and if you are trying to do something that’s not within the capabilities of the framework, it becomes extremely difficult.\n\n\nHere's a few thoughts on (LangChainAI?), the problems I see with it currently, and how I think it could improve. This was originally formatted as a message to (hwchase17?):Here's a few things off the top of my head – 1. Heavy use of OOP. Having multiple layers of abstraction…\n\n— Sam Hogan ((0xSamHogan?)) July 12, 2023\n\n\nI recently wrote about LLMChains in langchain too, and found the same to true. You can find the previous blog post here. I would highly recommend the readers to give the previous blog post a read, it will explain LLMChains and Chains in langchain, that will be instrumental in understanding conversational chatbot that we are building today.\n\n\n\n\n\n\nNote\n\n\n\n👉 This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. The blog post in itself is a completely working jupyter notebook with code-snippets."
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#chatbot-implementation-in-langchain",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#chatbot-implementation-in-langchain",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "2 Chatbot: Implementation in langchain",
    "text": "2 Chatbot: Implementation in langchain\nLet’s say you have a number of documents, in my case, I have a bunch of markdown documents. And we want to build a question answering chatbot that can take in a question, and find the answer based on the documents.\n\n\n\n\n\n\nFigure 1: Chatbot architecture\n\n\n\nIn essence, the chatbot looks something like above. We pass the documents through an “embedding model”. It is easy enough to use OpenAI’s embedding API to convert documents, or chunks of documents to embeddings. These embeddings can be stored in a vector database such as Chroma, Faiss or Lance.\nThe user interacts through a “chat interface” and enters a question/query. This query can also be converted to an embedding using the embedding model. Next, we can find the nearest chunks (similar to the query) using similarity search, then pass these nearest chunks (referred to as “context”) to a Large Language Model such as ChatGPT.\nFinally, we retrieve an answer and this answer get’s passed back to the user in the chat interfact. We store this interaction in chat history and continue.\nThat is all in theory, in code, using langchain, above would look like:\n\n%load_ext autoreload\n%autoreload 2\n\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\n# directory to store vector database\npersist_directory = \".db/\"\nopenai_api_key = os.environ['OPENAI_API_KEY']\n# loader that loads `markdown` documents\nloader = DirectoryLoader(\"./output/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n# text splitter converts documents to chunks\ndocs = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nchunks = text_splitter.split_documents(docs)\n# embedding model to convert chunks to embeddings\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n\n# load vector database, uncomment below two lines if you'd like to create it\n\n#################### run only once at beginning ####################\n# db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\n# db.persist()\n####################################################################\ndb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", output_key='answer', return_messages=False)\n\n# create QA chain using `langchain`, database is used as vector store retriever to find \"context\" (using similarity search)\nqa = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    get_chat_history=lambda o:o,\n    memory=memory,\n    return_generated_question=True,\n    verbose=False,\n)\n\n\n# let's ask a question\nqa({\"question\": \"Why is it so hard to find a rental property in Australia in June 2023?\", \"chat_history\": []})\n\n{'question': 'Why is it so hard to find a rental property in Australia in June 2023?',\n 'chat_history': '',\n 'answer': 'In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.',\n 'generated_question': 'Why is it so hard to find a rental property in Australia in June 2023?'}\n\n\nLooking at the answer above, it really answers the question - “Why is it so hard to find a rental property in Australia in June 2023?” very well. Above might only be a few lines of code, but there is actually quite a lot going on underneath. Refer to Figure 1 for everything that’s going on underneath.\nBut, as a recap, and matching our steps with code shared above:\n\nLoad markdown files in a list loader = DirectoryLoader(\"./output/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\nCreate a splitter that can split documents to chunks text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nConvert each chunk and store as Embeddings in a Chroma DB Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\nUse the database as retriever to get relevant text (context), and based on ‘question’, use OpenAI’s gpt-3.5-turbo (ChatGPT) model to answer question based on context.\n\nConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    memory=memory,\n    verbose=False,\n)\n\nAlso store conversation as chat history in memory memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n\n\n2.1 Text splitter\nFor our simple usecase, we are using a text splitter of type CharacterTextSplitter.\ntext_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nWe are using a chunk_size of 1024, which means that the document will be divided into chunks of size 1024, and there will be 128 character overlap between each of the chunks.\nThe CharacterTextSplitter used above splits texts based using regex and a separator. The separator in this case is '\\n\\n'. Thus, anytime there are two line breaks, our text splitter will split documents. Internally, in LangChain to split a text, _split_text_with_regex is being called.\n# simplified version without `keep_separator`\ndef _split_text_with_regex(\n    text: str, separator: str, keep_separator: bool\n) -&gt; List[str]:\n    # Now that we have the separator, split the text\n    if separator:\n                splits = re.split(separator, text)\n    else:\n        splits = list(text)\n    return [s for s in splits if s != \"\"]\nThere are many other text splitters that we could have also used. For a complete list - refer here.\n\n\n\n\n\n\nNote\n\n\n\nOne good one to further try would be - MarkdownHeaderTextSplitter. This particular splitter splits based on markdown headings, and it might be more useful for our usecase.\n\nRemember, the idea of chunking is to keep text with common context together.\n\n\n\nNow, that we have created our first bit, a text splitter that can split documents to chunks, let’s move on to the embedding model.\n\n\n2.2 Embedding model\nAlso, for our embedding model - we are using OpenAIEmbeddings. The main idea for the embedding model is to convert the chunks from before to embeddings.\nRemember, an embedding is only a vector representation of the text.\nSo, how do we convert our chunks (few sentences long) to a bunch of numbers? We can use openai’s embeddings API. Without langchain, this looks something like:\n\nimport openai\nchunk = \"This is a sample chunk consisting of few sentences.\"\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\nemb = get_embedding(chunk, model='text-embedding-ada-002')\nlen(emb)\n\n1536\n\n\nIn langchain, to achieve the same we instantiate from OpenAIEmbeddings.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\ntext = \"This is a test document.\"\nquery_result = embeddings.embed_query(text)\nlen(query_result)\n\n1536\n\n\nNow, to embed all chunks at once, OpenAIEmbeddings has a method called embed_documents.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nimport numpy as np\nembeddings = OpenAIEmbeddings()\ndocs = [\"This is test document 1.\", \"This is test document 2.\"]\nembs = embeddings.embed_documents(docs)\nnp.array(embs).shape\n\n(2, 1536)\n\n\nGreat, now that we have a way to embed all documents, let’s look at vector database next.\n\n\n2.3 Vector database\nConsider the vector database to a repository of knowledge. All our chunks get converted to embeddings and get stored in a vector-db. In our case, we are using chroma-db.\nLooking at the documentation, we start by creating a client, and then a collection. Once we have a collection ready, it is very simple to query the collection to get back the results.\nresults = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2\n)\nWhat goes under the hood inside langchain, is that we first instantiate a chroma-db collection. Next, we use collection’s upsert method passing in embeddings and texts. And this way, we have created our vector database that can be used to find nearest chunks from our documents based on “query” using similarity-search.\n\n\n\n\n\n\nNote\n\n\n\n❓ Some questions here to ask would be\n\nWould results look different or better if we used Cohere Embeddings? What would be the price difference?\nWhat would the quality of results be like if we used open source models like Llama-v2 released a few days ago?\nWhat if we used sentence-transformers?\nDo we really need a vector database? Can we store the embeddings as a np.array and use cosine-similarity to find nearest embeddings?\n\n\n\n\n\n2.4 Q&A ChatBot\nSo far we have looked at text-splitter, embedding model and vector database. These are the building blocks of the chatbot. But, how do we bring the building blocks together?\nIn langchain, all the pieces come together in ConversationalRetrievalChain which is the main topic of this blog post too. We instantiate an instance of the class using @classmethod called from_llm.\nqa = ConversationalRetrievalChain.from_llm(\n    llm=OpenAIChat(temperature=0, max_tokens=-1),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    memory=memory,\n    get_chat_history=lambda x: x,\n    verbose=True,\n)\nresponse = qa({\n    \"question\": \"Why is it so hard to find a rental property in Australia in June 2023?\", \n    \"chat_history\": []\n})\nThere are two main things that go on inside a conversational retrieval chain.\nA conversational retrieval chain can take in a query, and based on the input query (question) and chat-history, it updates it to a new question.\nThis new question is then passed to a second document chain, to find the nearest chunks (based on question) - referred to as “context”, and this context alongside the new question get’s passed to a large language model (such as gpt-3.5-turbo or ChatGPT), to retrieve the answer.\nSo, internally - ConversationalRetrievalChain consists of two chains:\n\nA question generator chain, which updates input query/question based on chat history (LLMChain)\nAnd a document chain to join retrieved documents/chunks together (StuffDocumentsChain)\n\n\n\n\n\n\n\nOn LLMChains\n\n\n\nGood news! We have already covered LLMChains in our previous blog post before here. In essence, given a prompt, the LLMChain can be used to generate an answer based on the prompt.\nGoing forward, I am going to assume that the reader has read the previous blog post and has a solid understanding of LLMChains & Chains in general.\n\n\nFrom our previous blog post, we know that anytime we call any chain in langchain, the __call__ method from Chain class gets invoked which in turn makes a call to _call method of derived class.\nThe ConversationalRetrievalChain is a subclass of BaseConversationalRetrievalChain which in turn is a subclass of Chain.\nThe _call method is implemented inside BaseConversationalRetrievalChain and it looks like below:\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; Dict[str, Any]:\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        question = inputs[\"question\"]\n        get_chat_history = self.get_chat_history or _get_chat_history\n        chat_history_str = get_chat_history(inputs[\"chat_history\"])\n\n        if chat_history_str:\n            callbacks = _run_manager.get_child()\n            new_question = self.question_generator.run(\n                question=question, chat_history=chat_history_str, callbacks=callbacks\n            )\n        else:\n            new_question = question\n        accepts_run_manager = (\n            \"run_manager\" in inspect.signature(self._get_docs).parameters\n        )\n        if accepts_run_manager:\n            docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\n        else:\n            docs = self._get_docs(new_question, inputs)  # type: ignore[call-arg]\n        new_inputs = inputs.copy()\n        if self.rephrase_question:\n            new_inputs[\"question\"] = new_question\n        new_inputs[\"chat_history\"] = chat_history_str\n        answer = self.combine_docs_chain.run(\n            input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\n        )\n        output: Dict[str, Any] = {self.output_key: answer}\n        if self.return_source_documents:\n            output[\"source_documents\"] = docs\n        if self.return_generated_question:\n            output[\"generated_question\"] = new_question\n        return output\nIn simple terms, first, the question_generator chain is called that updates the input question/query based on chat history.\nNext, we retrieve the documents based on our new_question using similarity search.\nThese retrieved docs, then get passed to combine_docs_chain which combines the retrieved chunks and passes them over to a large language model (in this case gpt-3.5-turbo) to get back the answer.\nLet’s understand both chains one by one in the next two sections. That way, we will be able to have a solid understanding of our conversational retrieval chain.\n\n2.4.1 Question generator chain\nLet’s start out with the question generator. Remeber, the question generator takes in the user question and a chat history, and based on chat history, it updates the question to a new question.\nWhy does it do that? The question generator rephrases the original question to be a standalone question. So if it is a follow up question like “Why did that happen?” from the user, remember, we do not know what “that” is in this particular question.\nSo, what the question generator will do, is that it will look at the chat history, and fill information for the word “that” to update the question to be a standalone question. So the new question could be “Why did the rental prices increase in Australia?” based on chat history.\nWe will also be looking at a working example of this in our code in this section.\nFrom a code perspective, in langchain, the question_generator is an instance of LLMChain.\nIn this case the prompt for the question generator (LLMChain) is CONDENSE_QUESTION_PROMPT which looks like:\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n{chat_history}\n\nFollow Up Input: {question}\nStandalone question:\"\"\"\n\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\nSo taking in a chat_history and the original question (from the user), internally a new question get’s generated! This new question is a standalone question as discussed at the start of this section.\nLet’s see it in action. Let’s see how the original question get’s updated to a new question based on chat_history. Remember, the first time we interact with the question answer bot, chat history is NULL, so no new question is generated. But, it works from the second time forward.\nWe can get langchain to return the newly generated question by passing in return_generated_question=True to the ConversationRetrievalChain.\n\nqa.memory.chat_memory.messages\n\n[HumanMessage(content='Why is it so hard to find a rental property in Australia in June 2023?', additional_kwargs={}, example=False),\n AIMessage(content='In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.', additional_kwargs={}, example=False)]\n\n\nSo far, we have the above chat history. Let’s now ask a follow up question about the home price index and say “How has the pandemic affected this?” and we can see the question generator in action.\n\nqa(\"How has the pandemic affected this?\")\n\n{'question': 'How has the pandemic affected this?',\n 'chat_history': 'Human: Why is it so hard to find a rental property in Australia in June 2023?\\nAI: In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.',\n 'answer': 'The given context does not provide specific information about the rental property market in Australia in June 2023.',\n 'generated_question': 'How has the pandemic affected the rental property market in Australia in June 2023?'}\n\n\nAs can be seen above the original question was “How has the pandemic affected this?” which got updated to the generated_question - “How has the pandemic impacted the difficulty in finding a rental property in Australia in June 2023?”. This was done based on the chat history.\nAnd that’s all that there is to know about the question generator! We can now move on the document chain which is StuffDocumentsChain.\n\n\n2.4.2 Document chain\nThe stuff documents chain is available as combine_docs_chain attribute from the conversational retrieval chain.\nThe StuffDocumentsChain itself has a LLMChain of it’s own with the prompt\nsystem_template = \"\"\"Use the following pieces of context to answer the users question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}\"\"\"\nmessages = [\n    SystemMessagePromptTemplate.from_template(system_template),\n    HumanMessagePromptTemplate.from_template(\"{question}\"),\n]\nCHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n\n\nPROMPT_SELECTOR = ConditionalPromptSelector(\n    default_prompt=PROMPT, conditionals=[(is_chat_model, CHAT_PROMPT)]\n)\nSo, we to our prompt, we pass in the context and a follow up question. It specifically says “just say that you don’t know, don’t try to make up an answer.” This is good to limit hallucination.\nWhen we call the StuffDocumentsChain, it does two things - first it calls combine_docs. This method first combines the given input chunks by using separator \\n\\n to generate context.\n    def _get_inputs(self, docs: List[Document], **kwargs: Any) -&gt; dict:\n        # Format each document according to the prompt\n        doc_strings = [format_document(doc, self.document_prompt) for doc in docs]\n        # Join the documents together to put them in the prompt.\n        inputs = {\n            k: v\n            for k, v in kwargs.items()\n            if k in self.llm_chain.prompt.input_variables\n        }\n        inputs[self.document_variable_name] = self.document_separator.join(doc_strings)\n        return inputs\n\n\n    def combine_docs(\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\n    ) -&gt; Tuple[str, dict]:\n        inputs = self._get_inputs(docs, **kwargs)\n        # Call predict on the LLM.\n        return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\nGiven our question, remember, we first find the closest chunks to the question. These chunks are then joined together using \\n\\n separator.\n\n\n\n\n\n\nNote\n\n\n\n❓ I wonder how things would look like if we numbered the various chunks and passed in the context as bullet points?\n\n\nNext, we just call LLMChain’s predict method, this generates an answer using a prompt and returns the answer.\nYou know what? That’s really it! I hope that now you understand completely how context based question answering chatbots work when using langchain. :)"
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#conclusion",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#conclusion",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nIn langchain, once we have a vector database, below lines of code are enough to create a chatbot, that can answer user questions based on some “context”.\nimport os\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\npersist_directory = \"./db/\"\nopenai_api_key = os.environ['OPENAI_API_KEY']\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\ndb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", output_key='answer', return_messages=False)\n\n# create QA chain using `langchain`, database is used as vector store retriever to find \"context\" (using similarity search)\nqa = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    get_chat_history=lambda o:o,\n    memory=memory,\n    return_generated_question=True,\n    verbose=False,\n)\nWe saw all the steps in detail as part of this blog post. We also saw that the ConversationalRetrievalChain consists of two chains:\n\nQuestion generator chain (to generate a new standalone question based on chat history)\nDocuments chain (to combine chunks as context and answer question based on context)\n\nWe saw that both chains consist of llm_chain with different prompts. We even saw the two prompts in detail.\nAnd thus, we uncovered all the magic behind a conversational retrieval chain in langchain. I hope you enjoyed reading this blog post.\nPlease feel to reach out to me on twitter for any follow-up questions!"
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html",
    "href": "posts/2024-06-22 Support bot.html",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "",
    "text": "Problem Statement: You are the owner of a graphical and tech company called “DRAMA77IC” that creates dramatic and graphical visualisations for games for users all over the world. You have an API that contains information about various games such as genre, date of release, description and so on. You want to create a support channel, so your users can directly ask questions about your offerings, place orders and also return games through this Slack channel.\nNow that we have a well defined problem statement, let’s go about creating a solution using Claudette!\nRecently Answer.AI team released Claudette. It is built on top of Claude 3.5 Sonnet - the most powerful language model at the time of writing this blog post.\nAs part of this blog post, I will show you how to use Claudette to create a support bot built on top of Slack. You should be able to easily integrate the steps shown below to respond to user queries by calling any function. Claudette also supports multiple function calls, so you can call a chain of functions to respond to user queries.\nThere are two parts to this blog post.\nFinally, we woll test this out and showcase a demo."
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html#creating-a-slack-app",
    "href": "posts/2024-06-22 Support bot.html#creating-a-slack-app",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "1 Creating a slack APP",
    "text": "1 Creating a slack APP\nFor the purpose of this blog post, I created a new workspace on Slack and also created a new app called “support-bot”.\nYou can go to https://api.slack.com/apps and create a new app.\n\n\n\nsupport-bot\n\n\nNext, here’s what you want to do:\n\nEnable socket mode, when you do this, a new APP level token will also be created with scope connections:write.\nGo over to OAuth & Permissions and add the following scopes:\n\napp_mentions:read\nchannels:history\nchannels:read\nchat:write\nim:history\nim:read\nim:write\nreactions:write\n\nEnable event subscriptions.\nInstall your bot to workspace and add it to your support channel.\n\n\n\n\nscopes\n\n\nWe are now ready to start sending messages to public slack channels using our bot. Copy over your SLACK_APP_TOKEN and SLACK_BOT_TOKEN to a .env file and let’s use dotenv to load them.\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nTrue\n\n\nNow let’s make some imports and get our BOT_USER_ID. Each user in Slack has a user_id. To read more about the slack client, refer here.\n\nimport os\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\n\n\nclient = WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\nBOT_USER_ID = client.auth_test()[\"user_id\"]\nBOT_USER_ID\n\n'U07932L0L5U'\n\n\nYou can also add the channel ID to your dotenv, and we can load it like below:\n\nchannel_id = os.environ['LISTEN_CHANNEL_ID']\nchannel_id\n\n'C078V28044F'\n\n\nTo post the message to your channel, simply use client.chat_postMessage. But, before you do that, make sure to add the support-bot to your channel.\nI created a new channel called #blog and added support-bot to it.\n\nresponse = client.chat_postMessage(\n        channel=channel_id, \n        text=\"Bonjour! My name is Claudia, I am your support-bot for DRAMA77IC, a made-up company name for blogging purposes.\", \n)\n\n\n\n\nweclome-message\n\n\nA little bit more about slack - each message in Slack has a timestamp represented by ts.\n\nmessage = client.conversations_history(channel=channel_id)['messages'][0]\nmessage\n\n{'user': 'U07932L0L5U',\n 'type': 'message',\n 'ts': '1719101633.566529',\n 'bot_id': 'B079C5SV99A',\n 'app_id': 'A079C53DT9A',\n 'text': 'Bonjour! My name is Claudia, I am your support-bot for DRAMA77IC, a made-up company name for blogging purposes.',\n 'team': 'T079C2R49GC',\n 'bot_profile': {'id': 'B079C5SV99A',\n  'deleted': False,\n  'name': 'support-bot',\n  'updated': 1719017903,\n  'app_id': 'A079C53DT9A',\n  'icons': {'image_36': 'https://a.slack-edge.com/80588/img/plugins/app/bot_36.png',\n   'image_48': 'https://a.slack-edge.com/80588/img/plugins/app/bot_48.png',\n   'image_72': 'https://a.slack-edge.com/80588/img/plugins/app/service_72.png'},\n  'team_id': 'T079C2R49GC'},\n 'blocks': [{'type': 'rich_text',\n   'block_id': 'Xwfz',\n   'elements': [{'type': 'rich_text_section',\n     'elements': [{'type': 'text',\n       'text': 'Bonjour! My name is Claudia, I am your support-bot for DRAMA77IC, a made-up company name for blogging purposes.'}]}]}]}\n\n\nTo respond to this very message, we can pass in the timestamp as a thread_ts parameter. This allows to respond to the message in a thread rather than posting a new message on the Slack channel.\n\nresponse = client.chat_postMessage(\n        channel=channel_id, \n        text=\"I was just told to respond to my own message. So I am doing that.\", \n        thread_ts=message['ts']\n)\n\n\n\n\n\nresponse\n\n\nNow we have the basics in place to start working on our support-bot using Claudette.\nEssentially what we want to do is to allow Claude 3.5 Sonnet to talk to the customers a customer support agent. To do that, we want to automate the process of reading new slack messages, sharing them with Claude 3.5 Sonnet, getting a response and posting it back to the user."
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html#support-bot-using-claudette",
    "href": "posts/2024-06-22 Support bot.html#support-bot-using-claudette",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "2 Support Bot using Claudette",
    "text": "2 Support Bot using Claudette\nFirst things first, let’s install the library.\npip install claudette\n\nimport os\nfrom slack_sdk.web import WebClient\nfrom slack_sdk.socket_mode import SocketModeClient\nfrom slack_sdk.socket_mode.response import SocketModeResponse\nfrom slack_sdk.socket_mode.request import SocketModeRequest\nimport dotenv\nimport logging\nfrom datetime import datetime, timedelta\nimport time\nfrom claudette import *\n\n\n# claude's latest and most powerful version\nmodel='claude-3-5-sonnet-20240620'\n\n\n# `sp` stands for system prompt\nchat = Chat(model=model, \n            sp=\"You are Claudia. Do not share what tools you use to respond to user requests.\")\nchat(\"Hi, I'm Alice.\")\n\nHello Alice! It’s nice to meet you. I’m Claudia. How are you doing today? Is there anything in particular you’d like to chat about?\n\n\nid: msg_01HzdXGnHQFFtHAGMZfqU8Fj\ncontent: [{‘text’: “Hello Alice! It’s nice to meet you. I’m Claudia. How are you doing today? Is there anything in particular you’d like to chat about?”, ‘type’: ‘text’}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {‘input_tokens’: 31, ‘output_tokens’: 36}\n\n\n\n\nNow, the best part about claudette is that it allows function calling and it has been made really simple.\nIf you have used function calling before, you would know that OpenAI and Anthropic expect functions to be defined in a certain manner.\nFor example, from Anthropic docs:\n      {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\nTo define a function get_weather that takes in an input parameter location, this is a rather tedious way of having to write the function.\nEspecially when we write all functions in Python itself. Having to convert a function like below:\ndef get_weather(\n    location: str\n):\n    weather_in_celsius = API_CALL(location)\n    return weather_in_celsius\nHaving to convert a simple Python function like above to the required format is rather tedious. Enter claudette!\nClaudette has this function called get_schema that is able to convert a python function to the desired format.|\n\nfrom toolslm.funccall import get_schema\n\ndef get_weather(\n    location: str # The city and state, eg. San Francisco, CA\n):\n    \"Get the current weather in a given location\"\n    weather_in_celsius = API_CALL(location)\n    return weather_in_celsius\n\nget_schema(get_weather)\n\n{'name': 'get_weather',\n 'description': 'Get the current weather in a given location',\n 'input_schema': {'type': 'object',\n  'properties': {'location': {'type': 'string',\n    'description': 'The city and state, eg. San Francisco, CA'}},\n  'required': ['location']}}\n\n\nThis is really handy especially when we want to pass in multiple functions to Claude to choose from.\nAs part of this blog post, let’s demo function calling with a dummy example. This data has been modified from Anthropic’s example here.\nLet’s say the company has the following five games - G1 to G5 and two customers C1 & C2.\n\ncustomers = {\n    \"C1\": dict(name=\"Alice Johnson\", email=\"alice@example.com\", phone=\"123-456-7890\",\n               games=[\"G1\", \"G2\", \"G3\"]),\n    \"C2\": dict(name=\"Bob Smith\", email=\"bob@example.com\", phone=\"987-654-3210\",\n               games=[\"G4\", \"G5\"])\n}\n\ngames = {\n    \"G1\": dict(id=\"G1\", name=\"Shadow Realms\", release_date=\"2023-03-15\", description=\"Navigate enchanted forests and haunted castles.\", status=\"Shipped\"),\n    \"G2\": dict(id=\"G2\", name=\"Solar Winds\", release_date=\"2023-07-22\", description=\"Explore space with stunning visuals and alien planets.\", status=\"Shipped\"),\n    \"G3\": dict(id=\"G3\", name=\"Mystic Legends\", release_date=\"2023-11-10\", description=\"Epic fantasy RPG with beautiful landscapes.\", status=\"Shipped\"),\n    \"G4\": dict(id=\"G4\", name=\"Cyber Revolution\", release_date=\"2024-02-28\", description=\"Dystopian future with advanced technology and cyber warfare.\", status=\"Shipped\"),\n    \"G5\": dict(id=\"G5\", name=\"Desert Storm\", release_date=\"2024-05-05\", description=\"Tactical shooter in a war-torn desert.\", status=\"Processing\")\n}\n\nLet’s now define some functions to get customer information, game information and also return games if needed.\n\ndef get_customer_info(\n    customer_id: str  # ID of the customer\n):  # Customer's name, email, phone number, and list of games\n    \"Retrieves a customer's information and their orders based on the customer ID\"\n    print(f'- Retrieving customer {customer_id}')\n    return customers.get(customer_id, \"Customer not found\")\n\ndef get_game_details(\n    game_id: str  # ID of the game\n):  # Game's ID, name, release date, description & status\n    \"Retrieves the details of a specific game based on the game ID\"\n    print(f'- Retrieving game {game_id}')\n    return games.get(game_id, \"Game not found\")\n\ndef return_game(\n    game_id:str # ID of the order to cancel\n)-&gt;bool: # True if the return is successful\n    \"Returns a game to the cmpany based on game ID.\"\n    print(f'- Returning game {game_id}')\n    if game_id not in games: return False\n    games[game_id]['status'] = 'Returned'\n    return True\n\nNow we can simply define these tools with claudette. Note, as previously mentioned, we no longer need to provide the chunky json version, claudette automatically handles that for us using docments.\n\ntools = [get_customer_info, get_game_details, return_game]\nchat = Chat(model, tools=tools)\n\nLet’s now do a function call as customer C1 and return one of the games.\n\nr = chat('Hi! How are you? This is Alice Johnson. (Customer ID: \"C1\")')\nprint(r.stop_reason)\nr.content\n\nend_turn\n\n\n[TextBlock(text=\"Hello Alice Johnson! It's great to hear from you. I'm doing well, thank you for asking. I hope you're doing well too. \\n\\nI see that you've provided your Customer ID. That's very helpful! Would you like me to retrieve your customer information and order details? I can do that for you using the Customer ID you've provided. This will allow me to assist you better with any questions or concerns you might have. \\n\\nShall I go ahead and fetch your customer information?\", type='text')]\n\n\n\nr = chat('Can you tell me more about the games I currently have? Just give me a list of games I own.')\nprint(r.stop_reason)\nr.content\n\n- Retrieving customer C1\ntool_use\n\n\n[TextBlock(text=\"Certainly, Alice! I'd be happy to help you with that. To get the information about the games you currently have, I'll need to retrieve your customer information first. I'll use the Customer ID you provided to do this.\", type='text'),\n ToolUseBlock(id='toolu_01TrdXW3C3VfJpcLi9UMeyYp', input={'customer_id': 'C1'}, name='get_customer_info', type='tool_use')]\n\n\nClaude recognises that we are doing a function call to retrieve information about C1. Claudette let’s you call the function automatically by simply calling it again.\n\nr = chat()\nprint(contents(r))\n\nThank you for your patience, Alice. I've retrieved your customer information, including the list of games you currently own. Here's a list of the games associated with your account:\n\n1. Game ID: G1\n2. Game ID: G2\n3. Game ID: G3\n\nThese are the games you currently have in your possession. Would you like more detailed information about any of these games? I can provide you with specific details for each game if you're interested. Just let me know which game(s) you'd like to know more about, and I'll be happy to fetch that information for you.\n\n\n\nr = chat(\"No, that's fine. Can you just return my game G2? I don't want it anymore.\")\nprint(r.stop_reason)\nr.content\n\n- Returning game G2\ntool_use\n\n\n[TextBlock(text=\"Certainly, Alice. I understand that you want to return the game with ID G2. I'd be happy to help you process that return. I'll use the return_game function to do this for you right away.\", type='text'),\n ToolUseBlock(id='toolu_018X45fRcYQ69TL4MUZS5rXg', input={'game_id': 'G2'}, name='return_game', type='tool_use')]\n\n\n\nr = chat()\nprint(contents(r))\n\nGreat news, Alice! The return for game G2 has been successfully processed. The system confirms that the return was completed successfully.\n\nTo summarize:\n- You've returned the game with ID G2.\n- The return has been recorded in our system.\n- You should no longer have this game in your possession.\n\nIs there anything else you'd like me to help you with regarding your games or account? Perhaps you'd like to know more about the remaining games you have, or if you have any other questions, I'm here to assist.\n\n\n\nr = chat(\"That's it. Thank you Claudia.\")\nprint(r.stop_reason)\nr.content\n\nend_turn\n\n\n[TextBlock(text=\"You're welcome, Alice! I'm glad I could help you with returning game G2. \\n\\nJust a small correction: my name isn't Claudia. I'm an AI assistant without a specific name. But I'm always here to help you with any questions or concerns you might have about your games or account.\\n\\nIs there anything else you need assistance with today? If not, I hope you have a wonderful day!\", type='text')]\n\n\nWe can also check the total token use as claudette automatically monitors that for us.\n\n# we can also check the total tokens used in our conversation\nchat.use\n\nIn: 5609; Out: 658; Total: 6267\n\n\nAlso, in the example above we have just returned one game. What if we wanted to return multiple games? We would have to call the return_game function in a loop. This is rather tedious.\nClaudette has a function called toolloop, this allows to call multiple functions (you can define maximum number of multiple function calls) until the model has completed the request. Let’s see it in action.\n\nchat.toolloop(\"Hey Claudia. Can you return all the games for me?\")\n\n- Returning game G1\n- Returning game G3\n\n\nGreat news! I’ve successfully processed the returns for both of your remaining games. Here’s a summary:\n\nGame G1: Successfully returned\nGame G3: Successfully returned\n\nAll of your games have now been returned to the company. Your account should no longer have any active game rentals.\nIs there anything else you would like me to help you with regarding your account or our services?\n\n\nid: msg_01SKrBoyXMbFNUF2uWGFErZK\ncontent: [{‘text’: “Great news! I’ve successfully processed the returns for both of your remaining games. Here’s a summary:. Game G1: Successfully returned. Game G3: Successfully returnedof your games have now been returned to the company. Your account should no longer have any active game rentals.there anything else you would like me to help you with regarding your account or our services?”, ‘type’: ‘text’}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {‘input_tokens’: 1633, ‘output_tokens’: 88}\n\n\n\n\nThere you go! Now, we were able to call multiple functions in a loop. Which is great. To confirm let’s check the games dict and confirm that the order status has changed.\n\ngames\n\n{'G1': {'id': 'G1',\n  'name': 'Shadow Realms',\n  'release_date': '2023-03-15',\n  'description': 'Navigate enchanted forests and haunted castles.',\n  'status': 'Returned'},\n 'G2': {'id': 'G2',\n  'name': 'Solar Winds',\n  'release_date': '2023-07-22',\n  'description': 'Explore space with stunning visuals and alien planets.',\n  'status': 'Returned'},\n 'G3': {'id': 'G3',\n  'name': 'Mystic Legends',\n  'release_date': '2023-11-10',\n  'description': 'Epic fantasy RPG with beautiful landscapes.',\n  'status': 'Returned'},\n 'G4': {'id': 'G4',\n  'name': 'Cyber Revolution',\n  'release_date': '2024-02-28',\n  'description': 'Dystopian future with advanced technology and cyber warfare.',\n  'status': 'Shipped'},\n 'G5': {'id': 'G5',\n  'name': 'Desert Storm',\n  'release_date': '2024-05-05',\n  'description': 'Tactical shooter in a war-torn desert.',\n  'status': 'Processing'}}\n\n\nAs can be seen from the dictionary above, we can see that games G1, G2 & G3 have been returned.\nNow, that’s a good looking customer support conversation but we want to have this in Slack instead. For that case, we will write a small Python script that constantly monitors the channel and looks for new messages. Claudette only responds if the bot has been mentioned with “(support-bot?)”. Let’s go ahead and write that script now and show it in action."
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html#claudette-in-slack",
    "href": "posts/2024-06-22 Support bot.html#claudette-in-slack",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "3 Claudette in Slack",
    "text": "3 Claudette in Slack\nNow that we have a good idea on how to use claudette for function calling, let’s integrate it with Slack so that we can allow our support-bot to respond to user queries in a thread.\nMostly all, we need is a process function like below:\ndef process(client: SocketModeClient, req: SocketModeRequest):\n    print(req.payload)\n    if req.type == \"events_api\" or req.type == \"event_callback\":\n        response = SocketModeResponse(envelope_id=req.envelope_id)\n        client.send_socket_mode_response(response)\n        if (\n            req.payload[\"event\"][\"type\"] == \"message\"\n            and req.payload[\"event\"].get(\"subtype\") is None\n            and \"bot_profile\" not in req.payload[\"event\"].keys()\n        ):\n            thread_ts = req.payload[\"event\"][\"ts\"]\n            if \"thread_ts\" in req.payload[\"event\"].keys():\n                thread_ts = req.payload[\"event\"][\"thread_ts\"]\n            text = req.payload[\"event\"][\"text\"]\n            r = chat.toolloop(text, maxtok=200)\n            response = _client.chat_postMessage(\n                channel=CHANNEL_ID, text=contents(r), thread_ts=thread_ts\n            )\nUsing claudette has made this function really easy. Because claudette already takes care of state, and past messages, that is something we don’t have to worry about and can simply delegate to Claudette to take care of it all.\nOnce we get a request, we can get a timestamp, and if the user responds in a thread itself, then we get the timestamp from the thread. Next, we extract the message as a string and pass it over to claudette.\nUsing toolloop allows claudette to make function calls directly to Claude and return the answer. A sample conversation on Slack using this setup looks something like below.\n\nThe complete python script can be found in the gist - here."
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html",
    "href": "posts/2023-07-25-llmchain.html",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "",
    "text": "With large language models taking the world by storm ever since the release of ChatGPT, one framework that has been ubiquitous has been LangChain. Recently, I was myself working on building an economic chatbot using the framework and wanted to look into the source code of what goes inside this complex framework. As part of this blog post, we start small. We pick the simplest use-case of LLMChain and look at the source code to understand what goes inside the framework.\nLet’s say that we want to hear a joke about any product. We can use the LLMChain for this.\n# https://python.langchain.com/docs/modules/chains/foundational/llm_chain#get-started\nfrom langchain import LLMChain, OpenAI, PromptTemplate\nprompt_template = \"Tell me a joke that includes {product}?\"\nllm = OpenAI(temperature=0, openai_api_key=&lt;openai_api_key&gt;)\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate.from_template(prompt_template),\n    return_final_only=True,\n)\nprint(llm_chain(\"colorful socks\")['text'])\nThe above code is internally calls the OpenAI chat completion API to tell a joke about “colorful socks”. Here is the output from the model.\nQ: What did the sock say to the other sock when it was feeling blue?\nA: \"Cheer up, it could be worse, at least we're not white socks!\"\nAnd with that joke, let’s start looking into the source code of LangChain and understand everything that there is to know about LLMChain.\n\n\n\n\n\n\nNote\n\n\n\nAll code below has been copied from from LangChain. At the time of writing, this was the GIT commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. LangChain’s code might change in the future."
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#introduction",
    "href": "posts/2023-07-25-llmchain.html#introduction",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "",
    "text": "With large language models taking the world by storm ever since the release of ChatGPT, one framework that has been ubiquitous has been LangChain. Recently, I was myself working on building an economic chatbot using the framework and wanted to look into the source code of what goes inside this complex framework. As part of this blog post, we start small. We pick the simplest use-case of LLMChain and look at the source code to understand what goes inside the framework.\nLet’s say that we want to hear a joke about any product. We can use the LLMChain for this.\n# https://python.langchain.com/docs/modules/chains/foundational/llm_chain#get-started\nfrom langchain import LLMChain, OpenAI, PromptTemplate\nprompt_template = \"Tell me a joke that includes {product}?\"\nllm = OpenAI(temperature=0, openai_api_key=&lt;openai_api_key&gt;)\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate.from_template(prompt_template),\n    return_final_only=True,\n)\nprint(llm_chain(\"colorful socks\")['text'])\nThe above code is internally calls the OpenAI chat completion API to tell a joke about “colorful socks”. Here is the output from the model.\nQ: What did the sock say to the other sock when it was feeling blue?\nA: \"Cheer up, it could be worse, at least we're not white socks!\"\nAnd with that joke, let’s start looking into the source code of LangChain and understand everything that there is to know about LLMChain.\n\n\n\n\n\n\nNote\n\n\n\nAll code below has been copied from from LangChain. At the time of writing, this was the GIT commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. LangChain’s code might change in the future."
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#code-deep-dive",
    "href": "posts/2023-07-25-llmchain.html#code-deep-dive",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "2 Code: Deep-dive",
    "text": "2 Code: Deep-dive\nCalling any class in Python requires the __call__ method to be implemented. LLMChain in itself is a subclass of Chain which has the __call__ method implemented that looks like below:\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/chains/base.py#L185-L250\ndef __call__(\n        self,\n        inputs: Union[Dict[str, Any], Any],\n        return_only_outputs: bool = False,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        include_run_info: bool = False,\n    ) -&gt; Dict[str, Any]:\n        inputs = self.prep_inputs(inputs)\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\n        run_manager = callback_manager.on_chain_start(\n            dumpd(self),\n            inputs,\n        )\n        try:\n            outputs = (\n                self._call(inputs, run_manager=run_manager)\n                if new_arg_supported\n                else self._call(inputs)\n            )\nLooking at the above code we can see that it calls self.prep_inputs(inputs) and then calls self._call method. We wil ignore the self.prep_inputs part for now, as otherwise, this blog post will become too long. The self._call method inside the Chain class is an abstract method. Therefore, it must be implemented in LLMChain.\nLet’s look at the definition of it in LLMChain.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/chains/llm.py#L87-L93\ndef _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; Dict[str, str]:\n        response = self.generate([inputs], run_manager=run_manager)\n        return self.create_outputs(response)[0]\nOkay, great! Now, we see that the _call method calling self.generate passing in the [inputs]. Remember, the inputs were prepared in self.prep_inputs(inputs) step inside __call__ of Chain.\nBelow I have shared the source code of generate method from LLMChain.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/chains/llm.py#L95-L107\ndef generate(\n        self,\n        input_list: List[Dict[str, Any]],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; LLMResult:\n        \"\"\"Generate LLM result from inputs.\"\"\"\n        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n        return self.llm.generate_prompt(\n            prompts,\n            stop,\n            callbacks=run_manager.get_child() if run_manager else None,\n            **self.llm_kwargs,\n        )\nSo, it’s calling self.llm.generate_prompt. Great! At this point, I am starting to wonder, what’s the point of LLMChain at all? Also, let’s skip the self.prep_prompts part. Otherwise the blog post will be too long. Rather than looking at the source code of self.prep_prompts, let’s just look at the output of it.\n&gt;&gt; input_list\n[{'product': 'colorful socks'}]\n\n&gt;&gt; prompts, stop\n([StringPromptValue(text='Tell me a joke that includes colorful socks?')], None)\nSo now all that has happened is that the above code has added product value to out input prompt. Why is this not an f-string I wonder?\nBefore we go any further, because, now we are starting to look at self.llm’s source code and not so much on Chains let’s just look at the reponse from response = self.generate([inputs], run_manager=run_manager). Below is what the response looks like:\n&gt;&gt; response\nLLMResult(generations=[[Generation(text='\\n\\nQ: What did the sock say to the other sock when it was feeling blue?\\nA: \"Cheer up, it could be worse, at least we\\'re not white socks!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 9, 'total_tokens': 49, 'completion_tokens': 40}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('5afa5d25-802a-49eb-b147-0f708d207a33'))])\nNow we are ready to look into self.llm.generate_prompt, how did this method create the response that we see above?"
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#openai-llm",
    "href": "posts/2023-07-25-llmchain.html#openai-llm",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "3 OpenAI LLM",
    "text": "3 OpenAI LLM\nSo far we were looking at the LLMChain source code. But, internally that in-itself is calling self.llm.generate_prompt. If you remember from the top of the blog post, self.llm was an instance of OpenAI class.\nThe OpenAI class is a subclass of BaseOpenAI and that in itself is a subclass of BaseLLM and the generate_prompt method that was called from inside the generate method of ChainLLM is implemented in BaseLLM. A bit complicated, isn’t it?\n# \n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/base.py#L178-L186\ndef generate_prompt(\n        self,\n        prompts: List[PromptValue],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        prompt_strings = [p.to_string() for p in prompts]\n        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\nSo, now we can see that the generate_prompt method calls self.generate again. Remember, the last time we called self.generate it was for the LLMChain, but this time, it is for the OpenAI LLM. The p.to_string() part? That’s just converting our prompt to string. This is the output of prompt_strings looks like below:\n&gt;&gt; prompt_strings\n['Tell me a joke that includes colorful socks?']\n\n\n\n\n\n\nNote\n\n\n\n🤔 So far so good? Yes and No. I am a bit baffled at the amount of complexity in the code. I still don’t know why we had a Chain and a LLMChain. I guess we are looking at just one use-case of LLMChain which is generation, LLMChains might also be supporting other use cases where this complexity might be needed.\n\n\nTime to look at the generate method. It is again implemented in BaseLLM.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/base.py#L233-L302\ndef generate(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n        if not isinstance(prompts, list):\n            raise ValueError(\n                \"Argument 'prompts' is expected to be of type List[str], received\"\n                f\" argument of type {type(prompts)}.\"\n            )\n        params = self.dict()\n        params[\"stop\"] = stop\n        options = {\"stop\": stop}\n        (\n            existing_prompts,\n            llm_string,\n            missing_prompt_idxs,\n            missing_prompts,\n        ) = get_prompts(params, prompts)\n        disregard_cache = self.cache is not None and not self.cache\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n        new_arg_supported = inspect.signature(self._generate).parameters.get(\n            \"run_manager\"\n        )\n        if langchain.llm_cache is None or disregard_cache:\n            if self.cache is not None and self.cache:\n                raise ValueError(\n                    \"Asked to cache, but no cache found at `langchain.cache`.\"\n                )\n            run_managers = callback_manager.on_llm_start(\n                dumpd(self), prompts, invocation_params=params, options=options\n            )\n            output = self._generate_helper(\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            return output\n        if len(missing_prompts) &gt; 0:\n            run_managers = callback_manager.on_llm_start(\n                dumpd(self), missing_prompts, invocation_params=params, options=options\n            )\n            new_results = self._generate_helper(\n                missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            llm_output = update_cache(\n                existing_prompts, llm_string, missing_prompt_idxs, new_results, prompts\n            )\n            run_info = (\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\n                if run_managers\n                else None\n            )\n        else:\n            llm_output = {}\n            run_info = None\n        generations = [existing_prompts[i] for i in range(len(prompts))]\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)\nOkay, well, let’s look at the outputs step-by-step. This again is a majorly over-complicated piece of code. But, for what? Why do we need such complication?\n&gt;&gt; params\n{'model_name': 'text-davinci-003', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}, '_type': 'openai', 'stop': None}\nHonestly, looking at the above source code, I am still a bit confused at which point do we even call the model and call the create API. I can’t count how many layers down we are in code, and we still haven’t called the openai.Completion.create method when generating an output from the prompt should be as simple as:\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)\nI believe the part where the generations actually happen is inside the following piece of code.\noutput = self._generate_helper(\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n\n# Values of `output` (This is the same as `response` from before)\n&gt;&gt; output\nLLMResult(generations=[[Generation(text='\\n\\nQ: What did the sock say to the other sock when it was feeling blue?\\nA: \"Cheer up, it could be worse, at least we\\'re not white socks!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 9, 'total_tokens': 49, 'completion_tokens': 40}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('b32869ec-43a3-4780-805e-be482f1fe05b'))])\nSo, now we need to look at another method self._generate_helper and who knows what other methods that method will call. Let’s dig down more into the source code a bit further and look at self._generate_helper.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/base.py#L200-L231\ndef _generate_helper(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]],\n        run_managers: List[CallbackManagerForLLMRun],\n        new_arg_supported: bool,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        try:\n            output = (\n                self._generate(\n                    prompts,\n                    stop=stop,\n                    # TODO: support multiple run managers\n                    run_manager=run_managers[0] if run_managers else None,\n                    **kwargs,\n                )\n                if new_arg_supported\n                else self._generate(prompts, stop=stop)\n            )\n        except (KeyboardInterrupt, Exception) as e:\n            for run_manager in run_managers:\n                run_manager.on_llm_error(e)\n            raise e\n        flattened_outputs = output.flatten()\n        for manager, flattened_output in zip(run_managers, flattened_outputs):\n            manager.on_llm_end(flattened_output)\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\nUnbelievable! The self._generate_helper is further calling self._generate method which is an abstract method in BaseLLM but implemented in BaseOpenAI!\n\n\n\n\n\n\nNote\n\n\n\n😡 Are you still with me yet? We got to finish this. But, yes I am a bit frustrated. This whole part seems so complex and is so hard to follow and explain. Let’s recap what has happened so far. We wanted to look at what goes inside LangChain to create outputs from a prompt. What we have discovered is extremely puzzling. And I am being euphemistic here.\nWe started with LLMChain and called it using __call__ method that was implemented in Chain which called_call method of LLMChain that in-turn called self.generate which further called self.llm.generate_prompt.\nself.llm is an instance of OpenAI class which is subclass of BaseOpenAI which is subclass of BaseLLM and generate_prompt method is implemented there. We are not done yet.\nThe generate_prompt implemented in BaseLLM calls self.generate which in turn calls self._generate_helper that in turn calls self._generate of BaseOpenAI!\nIsn’t that a lot of code? And we still haven’t called the OpenAI API yet.\n\n\nOkay, I am normally breathing again. Let’s continue. We are still not done yet and haven’t generated our results. Let’s continue and look at the self._generate method which is implemented in BaseOpenAI.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/openai.py#L272-L325\ndef _generate(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        params = self._invocation_params\n        params = {**params, **kwargs}\n        sub_prompts = self.get_sub_prompts(params, prompts, stop)\n        choices = []\n        token_usage: Dict[str, int] = {}\n        # Get the token usage from the response.\n        # Includes prompt, completion, and total tokens used.\n        _keys = {\"completion_tokens\", \"prompt_tokens\", \"total_tokens\"}\n        for _prompts in sub_prompts:\n            if self.streaming:\n                if len(_prompts) &gt; 1:\n                    raise ValueError(\"Cannot stream results with multiple prompts.\")\n                params[\"stream\"] = True\n                response = _streaming_response_template()\n                for stream_resp in completion_with_retry(\n                    self, prompt=_prompts, **params\n                ):\n                    if run_manager:\n                        run_manager.on_llm_new_token(\n                            stream_resp[\"choices\"][0][\"text\"],\n                            verbose=self.verbose,\n                            logprobs=stream_resp[\"choices\"][0][\"logprobs\"],\n                        )\n                    _update_response(response, stream_resp)\n                choices.extend(response[\"choices\"])\n            else:\n                response = completion_with_retry(self, prompt=_prompts, **params)\n                choices.extend(response[\"choices\"])\n            if not self.streaming:\n                # Can't update token usage if streaming\n                update_token_usage(_keys, response, token_usage)\n        return self.create_llm_result(choices, prompts, token_usage)\nDo we get to the part where we have results yet? Yes! The completion_with_retry function looks like it! Let’s look at the inputs to this function. It looks like the _generate also supports streaming.\n&gt;&gt; _prompts\n['Tell me a joke that includes colorful socks?']\n\n&gt;&gt; params\n{'model': 'text-davinci-003', 'api_key': '&lt;openai_api_key&gt;', 'api_base': '', 'organization': '', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\nWe are now ready to call the completion_with_retry passing in above as inputs. The _prompts is just the Prompt Template converted to a string. And the params are defined as defaults in BaseOpenAI.\ndef completion_with_retry(llm: Union[BaseOpenAI, OpenAIChat], **kwargs: Any) -&gt; Any:\n    \"\"\"Use tenacity to retry the completion call.\"\"\"\n    retry_decorator = _create_retry_decorator(llm)\n\n    @retry_decorator\n    def _completion_with_retry(**kwargs: Any) -&gt; Any:\n        return llm.client.create(**kwargs)\n\n    return _completion_with_retry(**kwargs)\nNow, self get’s passed as an argument to the function and finally we call llm.client.create(kwargs).\nBelow is what the kwargs look like. Remember, they are just _prompts & params mergfed together into a single dictionary.\n&gt;&gt; kwargs\n{'prompt': ['Tell me a joke that includes colorful socks?'], 'model': 'text-davinci-003', 'api_key': '&lt;api_key&gt;', 'api_base': '', 'organization': '', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}"
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#complexity-and-readability-in-langchain",
    "href": "posts/2023-07-25-llmchain.html#complexity-and-readability-in-langchain",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "4 Complexity and Readability in LangChain",
    "text": "4 Complexity and Readability in LangChain\nGoing through the LangChain code, I find myself filled with a mix of admiration and confusion. It’s clear that the creators of LangChain put a lot of thought into building a flexible architecture. The design, which involves multiple layers of abstraction and many separate components, suggests that the tool is built to handle a wide range of use cases beyond the one we’ve examined today. However, in this particular scenario, we’ve seen how complexity can make code harder to follow and understand.\nLet’s start with the positives. The modular design of LangChain allows for easy extension and modification.\nOn the other hand, the complexity of the code made it difficult to trace the flow of execution. We found ourselves diving deeper and deeper into the call stack, and it took a considerable amount of time just to locate the point where the OpenAI API is actually called.\nAnother point of confusion was the usage of the self.dict() method. It seems that this method is intended to create a dictionary representation of an object’s attributes, but it’s not immediately clear why this is necessary. In some cases, it seemed that a simpler approach, such as using f-strings for prompt generation, could have achieved the same result with less code.\nIn conclusion, examining the LangChain code has provided valuable insights into the design decisions that go into creating a complex tool like this. While the abstraction and modularity are commendable, the complexity of the code can potentially make it harder for anyone to understand.\nWhat are your thoughts?"
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html#conclusion",
    "href": "posts/2024-06-22 Support bot.html#conclusion",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nAs part of this blog post, we explored integrating Claudette with slack-sdk, to use Claude Sonnet 3.5 as a customer support agent.\nUsing claudette for function calling made this process much easier and allowed Claude to make multiple function calls using a simple method called toolloop. We no longer have to worry about having to define functions as well, as claudette already takes care of it all for us.\nBy the way, claudette also support images, and there is an example to create a simple code-interpreter in the docs. Be sure to check them here.\nThanks for reading!"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html",
    "href": "posts/2024-06-28 ml-4M.html",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "",
    "text": "Figure 1: Image retrieval using 4M-21 any-to-any vision model\n\n\n\nBefore, we get started, let’s take a moment to understand what’s going on in the demo video above:\n\nThis a demo of an image retrieval app that is capable of retrieving most similar images from any given database. It is built on top of gradio.\nAs part of this demo, we are able to retrieve images based on the following filters:\n\nCaption (description of the image)\nBrightness (brightness in the image, lower represents a darker image)\nNumber of items (lower represents fewer number of items in the image)\n\nStarting with a 5/255 brightness & a 5/50 items per image for dining room, we were able to retrieve an almost empty & dark image of a dining room.\nIncreasing the number of items to 50 retrieves a dark image of a dining room but with chairs and a dining table.\nAs you’ll later see, we can also add a lot more filters such as an input image, image segmentation mask, image boundary, number of humans and more but we have limited ourselves to three for the purpose of this demo.\n\nWith this understanding of the demo app, let’s get started and build one ourselves! If you’d like to skip over all the details, python code for this app has been shared in Section 2.3.\n\n\n\n\n\n\nNote\n\n\n\nThank you jarvislabs.ai for the compute, this blog post would not have been possible without the credits.\n\n\n\n\nAs part of this blog post, we are going to assume that the reader has a basic understanding of embeddings, Vision Language Models and image retreival using cosine similarity search.\nSome good resources to get the readers going are shared below:\n\nImage Similarity with Hugging Face Datasets and Transformers by Sayak Paul\nThe illustrated word2vec by Jay Alammar\nVision Language Models Explained by Merve Noyan & Edward Beeching"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#prerequisites",
    "href": "posts/2024-06-28 ml-4M.html#prerequisites",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "",
    "text": "As part of this blog post, we are going to assume that the reader has a basic understanding of embeddings, Vision Language Models and image retreival using cosine similarity search.\nSome good resources to get the readers going are shared below:\n\nImage Similarity with Hugging Face Datasets and Transformers by Sayak Paul\nThe illustrated word2vec by Jay Alammar\nVision Language Models Explained by Merve Noyan & Edward Beeching"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#m-massively-multimodal-masked-modeling",
    "href": "posts/2024-06-28 ml-4M.html#m-massively-multimodal-masked-modeling",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.3 4M: Massively Multimodal Masked Modeling",
    "text": "2.3 4M: Massively Multimodal Masked Modeling\nLet’s start digging into the paper first and understand more about the 4M-21 paper. It is the second in the 4M series by apple. The first paper 4M: Massively Multimodal Masked Modeling was released in Dec 2023.\nFrom the paper: &gt; To enable training a single Transformer on modalities with different formats, like text, bounding boxes, images, or neural network features, we choose to unify their representational spaces by mapping them into sets or sequences of discrete tokens using modality-specific tokenizers. This tokenization approach enhances compatibility, scalability, and sharing by removing the need for task-specific encoders and heads, allowing the Transformer to be compatible with all modalities and maintain full parameter-sharing.\nAlso from the paper: &gt; All modalities are mapped to sets or sequences of discrete tokens (indices of a vocabulary) through the use of modality-specific tokenizers. Captions and bounding boxes are both treated as text and encoded using WordPiece. For modeling bounding boxes, we follow the approach of Pix2Seq, that turns the task of object detection into a sequence prediction problem. RGB, depth, normals, semantic segmentation maps, and CLIP feature maps are tokenized using learned vector quantized autoencoders (VQ-VAE)\nTo say it in simple words - using modality specific tokenizers (for example, WordPiece for text inputs & VQ-VAE for feature maps), the authors found a way to make all modalities be represented in a single unified space. Once all inputs could be mapped to a unified space, then the encoder-decoder transformer was trained on top to perform any-to-any prediction using pre-training strategy similar to Multi-MAE.\n\n\n\nsupport-bot\n\n\nThe training can be visualised in the image above. First, modality specific tokenizers are utilised to map input modalities such as RGB, Segmentation, CLIP Features to a specific number of tokens.\nNext, select at random a specific number of input tokens (this is defined by parameter α, that defines proportion of tokens input & output tokens per modality), and use the random sample of input tokens to predict a random sample of output tokens in an auto-regressive manner using a transformer encoder-decoder architecture.\nOne thing you might ask:\nWhat happens if our target modalities increase? The authors decided to mask targets also and only predict a certain number of targets for randomly sampled input tokens. This allows training on multiple modalities while keeping the training costs low.\nBy training this way, the model learns how to predict target modalities given some number of input modalities. This is what we want to achieve our goal.\nGiven an input decsription of the image, and maybe a colour palette, we want to find the most similar images in our dataset. (Thus, given text & colour pallete as input modalities, we want to predict RGB pixels as output modality and use this to find similar images)\nLet’s see this in action with the help of code. All code, models and tokenizers have been open-sourced here."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#multimodal-retreival",
    "href": "posts/2024-06-28 ml-4M.html#multimodal-retreival",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.4 Multimodal retreival",
    "text": "2.4 Multimodal retreival\nBefore getting into the details into how this works, let me first show you how the results look like.\n\n2.4.1 Imports\nLet’s import all the required functions from the library.\n\nfrom fourm.vq.vqvae import VQVAE\nimport torch\nfrom fourm.models.fm import FM\nfrom fourm.models.generate import GenerationSampler, build_chained_generation_schedules, init_empty_target_modality, init_full_input_modality, custom_text\nfrom tokenizers import Tokenizer\nfrom fourm.data.modality_transforms import RGBTransform\nfrom torchvision.transforms.functional import center_crop\nfrom fourm.utils.plotting_utils import decode_dict\nfrom fourm.data.modality_info import MODALITY_INFO\n\nINFO:datasets:PyTorch version 2.3.1 available.\n\n\nNo module named 'smplx'\nHuman pose dependencies are not installed, hence poses will not be visualized. To visualize them (optional), you can do the following: \n1) Install via `pip install timm yacs smplx pyrender pyopengl==3.1.4` \n   You may need to follow the pyrender install instructions: https://pyrender.readthedocs.io/en/latest/install/index.html \n2) Download SMPL data from https://smpl.is.tue.mpg.de/. See https://github.com/shubham-goel/4D-Humans/ for an example. \n3) Copy the required SMPL files (smpl_mean_params.npz, SMPL_to_J19.pkl, smpl/SMPL_NEUTRAL.pkl) to fourm/utils/hmr2_utils/data .\n\n\nNext, if given some input caption and bounding box information, we want to perform image retrieval, then let’s load the text tokenizer.\n\n# load the tokenizer to extract DINO-V2 global embeddings\ndevice = \"cuda\"\ntok_dinov2_global = VQVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_DINOv2-B14-global_8k_16_224')\ntok_dinov2_global = tok_dinov2_global.to(device)"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#creating-a-demo-app",
    "href": "posts/2024-06-28 ml-4M.html#creating-a-demo-app",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.5 Creating a demo app",
    "text": "2.5 Creating a demo app\nThis is the same as creating a gradio app. For fun, how about we use Claude for this?"
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html",
    "title": "The Annotated CLIP (Part-2)",
    "section": "",
    "text": "As part of this blog post we will be uncovering the inner workings of CLIP - Learning Transferable Visual Models From Natural Language Supervision by looking at it’s PyTorch implementation. For a gentle introduction to CLIP, please refer to part-1 of the blog.\n\n\n\n\n\n\nFigure 1: Summary of CLIP approach\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll code referenced in this blog post has been copied (and sometimes modified) from the wonderful Open CLIP repository.\nAlso note that code-folding has been set in this blog post, so you will have to unfold code to have a look at it. :)\n\n\nIn this blog post, we will be covering the following with references to further resources where necessary:\n\nData download and preparation\nCLIP architecture in code\nCLIP image encoder\nCLIP text encoder\nCLIP loss function\n\n\nFrom the open clip repository, I found the most complex part to be data preparation. That in itself could be a separate blog post, and therefore, I have only covered it partly here as the main focus is to look at the CLIP architecture. As part of this blog post we are going to assume that there is some magic function that can read the input images and texts and return tensors of shape [N, 3, 224, 224] & [N, 77] respectively, where \\(N\\) is the batch size."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-intro",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-intro",
    "title": "The Annotated CLIP (Part-2)",
    "section": "",
    "text": "As part of this blog post we will be uncovering the inner workings of CLIP - Learning Transferable Visual Models From Natural Language Supervision by looking at it’s PyTorch implementation. For a gentle introduction to CLIP, please refer to part-1 of the blog.\n\n\n\n\n\n\nFigure 1: Summary of CLIP approach\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll code referenced in this blog post has been copied (and sometimes modified) from the wonderful Open CLIP repository.\nAlso note that code-folding has been set in this blog post, so you will have to unfold code to have a look at it. :)\n\n\nIn this blog post, we will be covering the following with references to further resources where necessary:\n\nData download and preparation\nCLIP architecture in code\nCLIP image encoder\nCLIP text encoder\nCLIP loss function\n\n\nFrom the open clip repository, I found the most complex part to be data preparation. That in itself could be a separate blog post, and therefore, I have only covered it partly here as the main focus is to look at the CLIP architecture. As part of this blog post we are going to assume that there is some magic function that can read the input images and texts and return tensors of shape [N, 3, 224, 224] & [N, 77] respectively, where \\(N\\) is the batch size."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#prerequisites",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#prerequisites",
    "title": "The Annotated CLIP (Part-2)",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nAs part of this blog post, I am going to assume that the reader has a good understanding of the ResNet architecture (K. He et al. (2015)) and Vision Transformer (Dosovitskiy et al. (2020)).\nI am also going to assume that the reader also has a good basic understanding of CLIP after having read part-1 of this blog series.\nIf the reader would like a refresher on the ResNet architecture, please refer to the following video from paper reading group, that I hosted at Weights and Biases.\n\n\n\n\nAlso, the authors use Vision Transformer as one of the backbones in Image Encoder. For complete understanding of ViT with PyTorch code implementation, refer to my previous blog post (in collaboration with Dr Habib Bukhari) - Vision Transformer. We won’t be covering ViT architecture as part of this blog post.\nFor the text encoder, the authors used the GPT-2 architecture. I have previously covered the entirety of the model with PyTorch code implementation at The annotated GPT-2.\nNow, with prerequisites and introductions out of the way, let’s get started with the first item which is “Data download and preparation”."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#data-download-using-img2dataset-and-preparation-using-webdataset",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#data-download-using-img2dataset-and-preparation-using-webdataset",
    "title": "The Annotated CLIP (Part-2)",
    "section": "3 Data download using img2dataset and preparation using webdataset",
    "text": "3 Data download using img2dataset and preparation using webdataset\nBefore we can start training any models, we need data. In this part of the blog post we are looking at data preparation part of CLIP. Remember, that CLIP was trained on 400M (image, text) pairs.\nFrom the paper:\nWe create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision.\nSo, how does one create these image text pairs in practice? One of the easiest ways to train CLIP using Open CLIP is to generate the dataset in the form of webdataset using img2dataset.\nWe will only be creating a tiny version consisting of only 1,000 (image, text) and not the complete 400M dataset used in CLIP.\n\n\nCode\n%%time\n# !pip install img2dataset \nfrom img2dataset import download\nimport shutil\nimport os\n\noutput_dir = os.path.abspath(\"sample\")\n\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\n\ndownload(\n    processes_count=os.cpu_count(),\n    thread_count=os.cpu_count()*2,\n    url_list=\"/home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_1000.parquet\",\n    image_size=256,\n    output_folder=output_dir,\n    output_format=\"webdataset\",\n    input_format=\"parquet\",\n    url_col=\"URL\",\n    caption_col=\"TEXT\",\n    enable_wandb=False,\n    number_sample_per_shard=100, \n    distributor=\"multiprocessing\",\n)\n\n\nStarting the downloading of this file\nSharding file number 1 of 1 called /home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_1000.parquet\n\n\n0it [00:00, ?it/s]\n\n\n\nFile sharded in 10 shards\nDownloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n\n\n10it [00:31,  3.19s/it]\n\n\nworker  - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 12 - count: 100\ntotal   - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 12 - count: 100\nworker  - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 12 - count: 100\ntotal   - success: 0.845 - failed to download: 0.145 - failed to resize: 0.010 - images per sec: 23 - count: 200\nworker  - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 8 - count: 100\ntotal   - success: 0.847 - failed to download: 0.143 - failed to resize: 0.010 - images per sec: 23 - count: 300\nworker  - success: 0.850 - failed to download: 0.150 - failed to resize: 0.000 - images per sec: 9 - count: 100\ntotal   - success: 0.848 - failed to download: 0.145 - failed to resize: 0.007 - images per sec: 30 - count: 400\nworker  - success: 0.840 - failed to download: 0.160 - failed to resize: 0.000 - images per sec: 10 - count: 100\ntotal   - success: 0.846 - failed to download: 0.148 - failed to resize: 0.006 - images per sec: 38 - count: 500\nworker  - success: 0.830 - failed to download: 0.160 - failed to resize: 0.010 - images per sec: 10 - count: 100\ntotal   - success: 0.843 - failed to download: 0.150 - failed to resize: 0.007 - images per sec: 31 - count: 600\nworker  - success: 0.830 - failed to download: 0.150 - failed to resize: 0.020 - images per sec: 9 - count: 100\ntotal   - success: 0.841 - failed to download: 0.150 - failed to resize: 0.009 - images per sec: 35 - count: 700\nworker  - success: 0.880 - failed to download: 0.100 - failed to resize: 0.020 - images per sec: 6 - count: 100\ntotal   - success: 0.846 - failed to download: 0.144 - failed to resize: 0.010 - images per sec: 40 - count: 800\nworker  - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 4 - count: 100\ntotal   - success: 0.846 - failed to download: 0.144 - failed to resize: 0.010 - images per sec: 34 - count: 900\nworker  - success: 0.900 - failed to download: 0.100 - failed to resize: 0.000 - images per sec: 4 - count: 100\ntotal   - success: 0.851 - failed to download: 0.140 - failed to resize: 0.009 - images per sec: 38 - count: 1000\nCPU times: user 71.6 ms, sys: 51 ms, total: 123 ms\nWall time: 32.6 s\n\n\nSo it takes ~35 seconds to create the tiny dataset on my 8 core machine. Please refer to img2dataset for information on other available (image, text) pair datasets.\nBut, what do the downloads look like? Let’s find out.\n\n\nCode\nimport numpy as np\nfrom pathlib import Path\n\nnp.array(sorted(list(Path('./sample/').glob('*tar'))))\n\n\narray([PosixPath('sample/00000.tar'), PosixPath('sample/00001.tar'),\n       PosixPath('sample/00002.tar'), PosixPath('sample/00003.tar'),\n       PosixPath('sample/00004.tar'), PosixPath('sample/00005.tar'),\n       PosixPath('sample/00006.tar'), PosixPath('sample/00007.tar'),\n       PosixPath('sample/00008.tar'), PosixPath('sample/00009.tar')],\n      dtype=object)\n\n\nAs we can see above, the script from img2dataset downloads {00000...00009).tar files. What’s in these .tar files? Answer lies in the documentation of webdataset. I won’t be covering more details as part of this blog post as we have a lot to cover stil!\n\n\n\n\n\n\nFurther reading\n\n\n\nOne key thing that I haven’t covered as part of this blog post, is that how do these .tar files get converted to inputs to the CLIP model? Let me share briefly here and point to the right directions.\nFirst, a data pipeline is created using wds (webdataset) package. You can find the pipeline being created here.\nThis pipeline takes in a tokenizer that’s HFTokenizer, see here. This HFTokenizer tokenizes the input and returns input_ids of context_length = 77."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#training",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#training",
    "title": "The Annotated CLIP (Part-2)",
    "section": "4 Training",
    "text": "4 Training\nNow, to train a CLIP model of your choice on a single GPU, simply clone the Open Clip repository and run the following command in your terminal in the src/ directory:\npython -m training.main \\\n    --save-frequency 1 \\\n    --zeroshot-frequency 1 \\\n    --train-data=\"/home/ubuntu/GIT_REPOS/amaarora.github.io/posts/sample/{00000..00009}.tar\" \\\n    --warmup 10 \\\n    --batch-size=16 \\\n    --lr=1e-3 \\\n    --wd=0.1 \\\n    --epochs=30 \\\n    --workers=8 \\\n    --model RN50 \\\n    --train-num-samples 852\nThis should kick off training on your machine. Now, that we can train CLIP models on our own machines, let’s look at some of the details of training scrip and the CLIP architecture."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#clip-architecture",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#clip-architecture",
    "title": "The Annotated CLIP (Part-2)",
    "section": "5 CLIP Architecture",
    "text": "5 CLIP Architecture\n\n\n\n\n\n\nFigure 2: Summary of CLIP approach\n\n\n\nFrom Figure 2, we can see that we have a text encoder and image encoder. These encoders are responsible for taking in the image and the text and and converting them to an embedding space.\nAs mentioned in Section 1, we will assume that there is some magic function that can read the .tar files and return tensors of shape [1, 3, 224, 224] for each image, and [1, 77], for each text.\nThese inputs then get encoded to embedding space using image and text encoder respectively.\nThe image encoder encodes images to embeddings \\(I_1, I_2, I_2 ... I_N\\), and the text encoder encodes respective image captions to \\(T_1, T_2, T_3 ... T_N\\).\n\n\n\n\n\n\nNote\n\n\n\nIn practice, the embedding size in CLIP is 1024. Therefore is our batch size \\(N = 8\\), the 8 input images will get encoded to a tensor of shape \\((8, 1024)\\), and also the 8 input texts will get encoded to a tensor of shape \\((8, 1024)\\).\n\n\nLet’s start by looking at the inputs and outputs of the overall CLIP model.\nFirst, we load the config, as part of this blog post we will work with ResNet-50 architecture as an example. So, let’s start by loading the corresponding config.\n\n\nCode\nimport json, torch\nfrom open_clip.model import CLIP\n\n\n\n\nCode\nwith open('../../open_clip/src/open_clip/model_configs/RN50.json') as o:\n    cfg = json.load(o)\ncfg\n\n\n{'embed_dim': 1024,\n 'vision_cfg': {'image_size': 224,\n  'layers': [3, 4, 6, 3],\n  'width': 64,\n  'patch_size': None},\n 'text_cfg': {'context_length': 77,\n  'vocab_size': 49408,\n  'width': 512,\n  'heads': 8,\n  'layers': 12}}\n\n\n\n\nCode\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = CLIP(**cfg).to(device)\nimage = torch.randn(1, 3, 224, 224).to(device)\ntext  = torch.randint(low=0, high=cfg['text_cfg']['vocab_size'], size=(1, 77)).to(device)\nimage_features, text_features, logit_scale   = model(image, text)\nimage_features.shape, text_features.shape, logit_scale\n\n\n(torch.Size([1, 1024]),\n torch.Size([1, 1024]),\n tensor(14.2857, device='cuda:0', grad_fn=&lt;ExpBackward0&gt;))\n\n\nAs can be seen above, the model expects image and text as inputs where in this case:\nimage = torch.randn(1, 3, 224, 224).to(device)\ntext  = torch.randint(low=0, high=cfg['text_cfg']['vocab_size'], size=(1, 77)).to(device)\nYou might recognize image shape easily - it represents 1 image with 3 channels (RGB) and (H,W) = 224. For text, each text is tokenized to max length 77. The integer numbers represent token_id with max value of cfg['text_cfg']['vocab_size'].\nMakes sense so far?\nAs for the outputs, the model returns three outputs - image_features, text_features and logit_scale.\nlogit_scale has been covered in more detail in Section 8 of this blog post. For now, think of it as a learnable parameter during model training.\nAs for image_features & text_features, these are the respective embeddings \\(I_1, I_2, I_2 ... I_N\\), and the text encoder encodes respective image captions to \\(T_1, T_2, T_3 ... T_N\\) with reference to Figure 2.\n\n\n\n\n\n\nTip\n\n\n\nBefore you proceed further, remember, the encoders are responsible for encoding the input image and text to embeddings of dimension - \\(1024\\).\nWe could have also done something like:\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = CLIP(**cfg).to(device)\nimage = torch.randn(1, 3, 224, 224).to(device)\ntext  = torch.randint(low=0, high=cfg['text_cfg']['vocab_size'], size=(1, 77)).to(device)\nimage_features = model.encode_image(image)\ntext_features = model.encode_text(text)\n\n\nNext, let’s look at the respective architectures of Image and Text encoders in more detail."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-img-encoder",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-img-encoder",
    "title": "The Annotated CLIP (Part-2)",
    "section": "6 Image Encoder",
    "text": "6 Image Encoder\nFrom the paper:\nWe consider two different architectures for the image encoder. For the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al. (2020)). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\n\n6.1 Modified ResNet\nLet’s start with the first architecture.\nFor the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” CoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\nThere are 3 major changes as mentioned to the ResNet architecture in CLIP:\n\nThere are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\nPerforms anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1\nThe final pooling layer is a QKV attention instead of an average pool\n\n\n6.1.1 ResNet stem\nLet’s look at all of them one by one in code. First, we start with There are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\n\n\n\n\n\n\nFigure 3: Overview of ResNet architecture\n\n\n\nIn the vanilla ResNet architecture, the stem consists of a 7x7 stride-2 convolution. This is what the stem looks like in the vanilla ResNet architecture.\nclass VanillaResNet:\n    def __init__(...):\n        self.stem = nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)\nHowever, in the paper T. He et al. (2018), where at the time, the authors raised ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet. From the paper, one of the tweaks used in the architecture:\nA 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions.\n\n\n\n\n\n\nFigure 4: Overview of ResNet tweak\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nOn top of replacing \\(7x7\\) stride-2 convolution with 3 consecutive \\(3x3\\) convolutions, the authors also replaced max pooling with average pooling, but image above shows max pooling as it has been directly copied from T. He et al. (2018).\n\n\n\nHe, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. “Bag of Tricks for Image Classification with Convolutional Neural Networks.” CoRR abs/1812.01187. http://arxiv.org/abs/1812.01187.\nIn code this looks like:\nclass ModifiedResNet:\n    def __init__(...):\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n    \n    def forward(self, x):\n        x = self.stem(x)\n\n\n6.1.2 Blur Pool\nThe next change is to use BlurPooling - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1. This change has been adopted from Zhang (2019).\nIn this section I will introduce BlurPooling and share how it is implemented in the ModifiedResNet architecture.\n\n\nFrom the research paper,\nModern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling.\nBlur Pooling in CLIP has been implemented inside the Bottleneck block as below:\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n        self.stem = create_stem() # stem consists of 3 3x3 convs instead of 1 7x7 stride-2 conv\n        self.downsample = None\n        self.stride = stride\n        if stride &gt; 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n        out = self.stem()\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\nNow the blurring occurs in downsample. Previously, as can be seen in Figure 3, this downsample layer would be a stride-2 \\(1x1\\) convolution.\nIn ModifiedResnet, this downsample consists of:\nself.downsample = nn.Sequential(OrderedDict([\n        (\"-1\", nn.AvgPool2d(stride)),\n        (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n        (\"1\", nn.BatchNorm2d(planes * self.expansion))\n    ]))\nWhere, the convolution is stride-1.\nThe blurring occurs in nn.AvgPool2d(stride). How? See example below:\n\n\nCode\nimport torch.nn as nn \nfrom PIL import Image\nimport numpy as np \nimport torch\nimport matplotlib.pyplot as plt\n\npool = nn.AvgPool2d(kernel_size=2)\nimg  = np.array(Image.open('../images/clip.png'))\nx    = torch.tensor(img, dtype=torch.float64).permute(2, 0, 1)\nout  = pool(pool(x))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8))\nax[0].imshow(x.int().permute(1,2,0))\nax[0].set_title(\"Input image before average pooling\");\nax[1].imshow(out.int().permute(1,2,0))\nax[1].set_title(\"Input image after average pooling\");\n\n\n\n\n\n\n\n\n\nAs can be seen above, passing the input image through a nn.AvgPool2d blurs the image, and therefore, anti-aliases the input as per Zhang (2019). As a result, the resulting model is translation invariant.\n\nZhang, Richard. 2019. “Making Convolutional Networks Shift-Invariant Again.” CoRR abs/1904.11486. http://arxiv.org/abs/1904.11486.\n\n\n6.1.3 Final pooling layer\nThis brings us to the final change in ModifiedResnet.\nThe last change in the network architecture is to use QKV attention instead of an average pool. From the paper:\nWe also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\n\n\nCode\nimport torch\nimport torch.nn as nn\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -&gt; (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x, key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n\n        return x[0]\n\n\nAs can be seen from the code above, we perform multi head self-attention from Vaswani et al. (2017), on x. One key thing to note above in the forward method is :\nx = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\nThis corresponds to “query is conditioned on the global average-pooled representation of the image” from the paper because the final output that is returned is x[0] and x[0] is global average pooled representation of the image.\n\n\n\n6.2 Modified ViT\nFrom the paper:\nFor the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al. (2020)). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” CoRR abs/2010.11929. https://arxiv.org/abs/2010.11929.\n\nSince the architecture is very similar to vanilla Vision Transformer, with a very minor change of adding LayerNorm after combining Patch embeddings and positional embeddings, I will not be covering the architecture in detail in this blog post.\nFor reference to ViT, please refer to my previous blog post that covers the architecture in detail with PyTorch code implementation - Vision Transformer\nHaving covered both Image encoders used in CLIP architecture, it is now time to move on to the text encoder."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#text-encoder",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#text-encoder",
    "title": "The Annotated CLIP (Part-2)",
    "section": "7 Text Encoder",
    "text": "7 Text Encoder\nIn this section, let’s look at the text encoder of CLIP architecture. From the paper:\nThe text encoder is a Transformer (Vaswani et al. (2017)) with the architecture modifications described in Radford et al. (2019). As a base size we use a 63M-parameter 12layer 512-wide model with 8 attention heads. The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015). For computational efficiency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary objective, though exploration of this is left as future work.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” CoRR abs/1706.03762. http://arxiv.org/abs/1706.03762.\n\nI have previously covered the complete GPT-2 architecture used as text encoder in my previous blog post at The annotated GPT-2 and therefore, won’t be covering it here in this blog post."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-contrastive-loss",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-contrastive-loss",
    "title": "The Annotated CLIP (Part-2)",
    "section": "8 Contrastive Loss",
    "text": "8 Contrastive Loss\nOne thing that I was most interested in when I started to write the CLIP blog post was to look at Contrastive Loss and understand it in PyTorch code.\nIn this section, that is exactly what we will be doing.\nIf you remember from Section 6, the images get encoded as image features to shape torch.Size([16, 1024]) and texts get encoded to text features of shape torch.Size([16, 1024]).\nLet’s look at the inputs and outputs of ClipLoss before implementing ourselves.\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport numpy as np \nimport torch.nn as nn\nfrom open_clip.loss import ClipLoss\n\n\n\n\nCode\nimage_features = torch.randn(16, 1024)\ntext_features  = torch.randn(16, 1024)\nloss_fn        = ClipLoss()\nlogit_scale    = nn.Parameter(torch.tensor(np.log(1/0.07)))\nloss = loss_fn(image_features, text_features, logit_scale)\nloss\n\n\ntensor(134.1310, grad_fn=&lt;DivBackward0&gt;)\n\n\nOne thing you’ll notice is that the ClipLoss expects a parameter called logit_scale. Now, what is this logit_scale parameter?\nFrom the paper:\nThe learnable temperature parameter \\(τ\\) was initialized to the equivalent of 0.07 from (Veeling et al. (2018)) and clipped to prevent scaling the logits by more than 100 which we found necessary to prevent training instability.\nVeeling, Bastiaan S., Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. 2018. “Rotation Equivariant CNNs for Digital Pathology.” https://doi.org/10.48550/ARXIV.1806.03962.\n\nBut, rather than being initialised to \\(0.07\\), this temperature parameter \\(τ\\) get’s initialized as nn.Parameter(torch.tensor(np.log(1/0.07))). For further explanation, see this issue here.\nNow, having looked at the inputs and outputs and also logit_scale, it is time to look at the source code. Remember contrastive loss and what it does from part-1 of the blog post? As a quick revision, let me re-post the image here too.\n\n\n\n\n\n\nFigure 5: Cosine similarity between text and image features\n\n\n\nContrastive loss is trying to maximise the cosine similarity on the diagonal and minimise it elsewhere. But, how? In pseudo-code this looked something like:\n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss = (loss_i + loss_t)/2\nOnce, we have logits, which is the \\(8 x 8\\) matrix as in Figure 5 above, we calculate Cross Entropy Loss once for axis=0 and once for axis=1, this way, we are trying to match the diagonal to corresponding image and text because the labels are aligned on both the axis.\nBut, how does this look like in code? Let’s see.\n\n\n\n\n\n\nNote\n\n\n\nThe below implementation of ClipLoss is a minimalistic version of the complete implementation from open clip.\n\n\n\n\nCode\nclass ClipLoss(nn.Module):\n    def __init__(\n            self,\n    ):\n        super().__init__()\n\n    def get_ground_truth(self, device, num_logits) -&gt; torch.Tensor:\n        labels = torch.arange(num_logits, device=device, dtype=torch.long)\n        return labels\n\n    def get_logits(self, image_features, text_features, logit_scale):\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logit_scale * text_features @ image_features.T        \n        return logits_per_image, logits_per_text\n\n    def forward(self, image_features, text_features, logit_scale, output_dict=False):\n        device = image_features.device\n        logits_per_image, logits_per_text = self.get_logits(image_features, text_features, logit_scale)\n        labels = self.get_ground_truth(device, logits_per_image.shape[0])\n        total_loss = (\n            F.cross_entropy(logits_per_image, labels) +\n            F.cross_entropy(logits_per_text, labels)\n        ) / 2\n        return {\"contrastive_loss\": total_loss}\n\n\n\n\nCode\nimage_features = torch.randn(16, 1024)\ntext_features  = torch.randn(16, 1024)\nloss_fn        = ClipLoss()\nlogit_scale    = nn.Parameter(torch.tensor(np.log(1/0.07)))\nloss = loss_fn(image_features, text_features, logit_scale)\nloss\n\n\n{'contrastive_loss': tensor(140.2225, grad_fn=&lt;DivBackward0&gt;)}\n\n\nSo, how does the above implementation match pseudo-code?\nLet’s start with labels. Since the labels are aligned, that is the \\(0th\\) image on axis=0 corresponds to \\(0th\\) text on axis=1, therefore, we can just say that labels = torch.arange(num_logits, device=device, dtype=torch.long). In this case the value of labels comes out to be tensor([ 0,  1,  2,  3,  4,  5,  6,  7], device='cuda:0') based on Figure 5. By minimising Cross Entropy loss for these labels once for axis=0 and once for axis=1, we are making sure that cosine-similarity on the diagonal is maximum and lower otherwise.\nIn code (as opposed to pseudo-code), rather than minimising cross entropy for axis=0, and axis=1, we calculate:\n    logits_per_image = logit_scale * image_features @ text_features.T\n    logits_per_text = logit_scale * text_features @ image_features.T    \nThis is same as calculating logits once for axis=1, and once for axis=0, therefore, our total loss is:\n    total_loss = (\n        F.cross_entropy(logits_per_image, labels) +\n        F.cross_entropy(logits_per_text, labels)\n    ) / 2\nThis is equivalent to pseudo code from paper:\n    labels = np.arange(n) \n    loss_i = cross_entropy_loss(logits, labels, axis=0) \n    loss_t = cross_entropy_loss(logits, labels, axis=1) \n    loss = (loss_i + loss_t)/2"
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#conclusion",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#conclusion",
    "title": "The Annotated CLIP (Part-2)",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nAs part of this blog post I built upon part-1 of this blog series on CLIP.\nWe also briefly looked at data preparation as (image, text) pairs for CLIP training using img2dataset and data loading using webdataset packages.\nWe took a deep dive into the Image Encoder section and looked at all the three tweaks in ModifiedResnet compared to vanilla ResNet architecture.\nFinally, we also took a deep dive in contrastive loss and compared the actual PyTorch implementation with pseudo-code from the CLIP research paper.\nIf you enjoyed reading this blog post, please consider subscribing to my blog for more!"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#image-retrieval-using-4m-21",
    "href": "posts/2024-06-28 ml-4M.html#image-retrieval-using-4m-21",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.2 Image retrieval using 4M-21",
    "text": "2.2 Image retrieval using 4M-21\n\n\n\n\n\n\nFigure 4: Different modes of multimodal retrieval\n\n\n\nAs can be seen above, the authors showcased that the model is capable of using any or subet of 21 modalities as input to retrieve similar images from a database. The query consist of one or more modalities from Figure 2. As part of this blog post, we will be focusing on the “caption + metadata -&gt; RGB” retrieval example.\nIn Figure 4, given the inputs “a fancy mansion” and “brightness 200/255”, the model was able to return very bright images of a mansion. Doing the same for “brightness 30/255” returns darker images of mansions. We will be replicating this functionality as part of this blog post.\n\n\n\n\n\n\nNote\n\n\n\nFigure 4 above is what prompted the idea for this blog post. What if we can utilise some or all of the modalities to query our own custom databases?\nThank you dear authors for answering all my questions.\n\n\nWe could have also added any of the input modalities from Figure 4 to our demo but we will leave this to the reader as an exercise to build on top of the code shared in this blog post. We have kept the input modalities limited to two as part of this blog post:\n\nImage description (caption)\nMetadata (such as brightness, number of items per image)\n\nAs part of writing this blog post, we did experiment with other modalities such as:\n\nInput image (using an image to find similar images)\nColour palette (using color palette to find similar images matching the colour schema)\n\nPlease refer to Section 2.4 for our findings on our custom database. Using color palette was not giving satisying results. We tried both EPFL-VILAB/4M-21_XL and EPFL-VILAB/4M-21_L models for the same.\n\n\n\n\n\n\nExpand to learn more about how to use color-palette and more metadata as inputs to the model\n\n\n\n\n\nIn Section 2.4 we will share with the reader how to extend the app to add color palette as an input the model on top of what has been shared in the demo.\nWe also showcase to the reader how to extend the app to use other metadata such as:\n\nCrowdedness score: number of humans\nSAM clutter score: number of SAM instances\nCOCO clutter score: number of COCO [55] instances\nCOCO instance diversity: number of unique COCO instance classes\nWalkability score: % of pixels belonging to walkable COCO semantic classes such as ‘road’\nSemantic diversity: number of unique COCO semantic classes\nCaption length: length of the caption in characters, words, and sentences\nGeometric complexity: angular variance of surface normals\nOcclusion score: % of occlusion edges over a fixed threshold\n\n\n\n\nHaving said that, let’s dig deep into the paper and understand how this model is able to distill information from multiple modalities.\nAs part of the training, each modality from Figure 2 was encoded using modality specific tokenizers. From the paper:\nWe employ suitable tokenization schemes for different modalities based on their format and performance. For image-like modalities and feature maps, we use spatial VQ-VAEs with optional diffusion decoders for detail rich modalities like RGB. For non-spatial modalities like global tokens or parameterized poses, we compress them to a fixed number of discrete tokens using Memcodes with MLP encoders and decoders. All sequence modalities are encoded as text using WordPiece.\n\n\n\n\n\n\nFigure 5: Tokenization overview\n\n\n\nWhat this means is that authors were able to represent information into a limited number of tokens for multiple modalities. By training modality specific tokenizers, the authors were able to transform different modalities into a common representation. After converting all modalities to a common representation, the authors were able to train a standard encoder-decoder transformer. During training, random subsets of these tokens are selected from all modalities as inputs and targets, and the objective is to predict one subset from the other.\n\n\n\n\n\n\nFigure 6: Method overview\n\n\n\nThe complete method overview was shared by the authors in the 4M: Massively Multimodal Masked Modeling Mizrahi et al. (2023) paper.\n\nMizrahi, David, Roman Bachmann, Oğuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 2023. “4M: Massively Multimodal Masked Modeling.” https://arxiv.org/abs/2312.06647.\nAs can be seen, here’s what’s exactly going on:\n\nFirst the different modalities are converted to a number of tokens using modality specific tokenizers\nA random subset of tokens are selected as input\nA random subset of tokens are selected as output\n\nBy doing so, the model learns to take in all or a subset of input modalities and predicts all or a subset of output modalities thus it is termed an “any-to-any vision model”.\nNow that we have a basic understanding of how the model works, let’ start building the retrieval app in Python."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#introduction",
    "href": "posts/2024-06-28 ml-4M.html#introduction",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nAs part of this blog post we will be utilising Apple’s 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities Bachmann et al. (2024) paper to build a real-time search engine that is capable of using caption, brightness & number of items per image as filters to query an image database of a total of 15 images. Though this technique can easily be expanded to a million or more images. If you have a big database of images, take a look at faiss for similarity search.\nWe will build an app using Gradio and also deploy it to Huggingface Hub for anyone to use.\nThe 4M-21 paper is the second in the 4M series (Massively Multimodal Masked Modeling) by Apple, the first paper was also an any-to-any vision model capable of working with 7 modalities - 4M: Massively Multimodal Masked Modeling Mizrahi et al. (2023).\n\n\n\n\n\n\nFigure 2: 4M-21 any-to-any vision model\n\n\n\nAs shown in the image above, the model can work with with multiple modalities. It can take all modalities as inputs and output any or all of the modalities using single or subset of modalities! Unbelievable right? Not anymore!\nSee the conditional generation example below as shared in the paper:\n\n\n\n\n\n\nFigure 3: One to all generation\n\n\n\nAs part of this blog post we will focus more on retrieval rather than generation. But, the basic concepts remain the same.\n\n\n\n\n\n\nNote\n\n\n\nThe authors have open sourced all code here - https://github.com/apple/ml-4m.\n\n\nAs part of this blog post we will focus more on retrieval rather than generation. But, the basic concepts remain the same. With that being said, let’s get started with image retrieval."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#appendix-a-appendin",
    "href": "posts/2024-06-28 ml-4M.html#appendix-a-appendin",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.7 Appendix A {appendin",
    "text": "2.7 Appendix A {appendin\n\n\n\n\n\n\nNote\n\n\n\nThank you jarvislabs.ai for the compute, this blog post would not have been possible without the credits."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#appendix-a-appendix-a",
    "href": "posts/2024-06-28 ml-4M.html#appendix-a-appendix-a",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.7 Appendix A {#appendix-a)",
    "text": "2.7 Appendix A {#appendix-a)\n\n\n\n\n\n\nNote\n\n\n\nThank you jarvislabs.ai for the compute, this blog post would not have been possible without the credits."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#appendix-a",
    "href": "posts/2024-06-28 ml-4M.html#appendix-a",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.7 Appendix A",
    "text": "2.7 Appendix A\n\n\n\n\n\n\nNote\n\n\n\nThank you jarvislabs.ai for the compute, this blog post would not have been possible without the credits."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#sec-appendixa",
    "href": "posts/2024-06-28 ml-4M.html#sec-appendixa",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.4 Appendix A",
    "text": "2.4 Appendix A\nWe used EPFL-VILAB/4M-21_L for all our experiments and image retrieval due to memory constraints. We found EPFL-VILAB/4M-21_XL requires around 28GB of VRAM along with respective tokenizers, and runtimes were slow on a A100 40GB instance.\n\n\n\n\n\n\nFigure 10: rgb-to-any retrieval using EPFL-VILAB/4M-21_L\n\n\n\n\n2.4.1 Adding color palette as inputs\nFrom the paper:\nFor every RGB image, we extract between one and seven color palettes using PyPalette. During training, we randomly sample one of the color palettes to enable users to input palettes with different levels of granularity.\ncolor palette sequence is formed as color = c R = r G = g B = b R = r, … where c takes a value between 1 and 7 and specifies the number of colors in the palette and r, g, b takes values between 0-255.\nWe can write a small python function to convert any of seaborn color palettes to the required format. Also, as per the color palette transform, the tokenizer expexts “color” to be replaced by “v1” and r,g,b with “v0”.\nTherefore, a color palette represented by color=1 r=166 g=206 b=227 should be transformed to v1=1 v0=166 v0=206 v0=227.\n\nimport seaborn as sns\n\n\ndef generate_color_palette(num_colors=2, palette_name=\"Paired\"):\n    palette = sns.color_palette(palette_name, num_colors)\n    rgb_values = [(int(r*255), int(g*255), int(b*255)) for r, g, b in palette]\n    color_strings = [f\"v0={r} v0={g} v0={b}\" for r, g, b in rgb_values]\n    color_palette = f\"v1={num_colors} \" + \" \".join(color_strings)\n    return palette, color_palette\n\npalette, color_palette_string = generate_color_palette(num_colors=2)\npalette\n\n\n\n\n\ncolor_palette_string\n\n'v1=2 v0=166 v0=206 v0=227 v0=31 v0=120 v0=180'\n\n\nNow we can simply pass in the string above as input and use custom_text function on our batched_sample to prepare batch for input to the model. We also need to add color_palette to the conditional domain as input.\ncond_domains = [\"caption\", \"metadata\", \"color_palette\"]\nOnce that’s done, we can now take our color_palette_string as input and created the batched_sample as before in Section 2.3.2.\nbatched_sample = custom_text(\n    batched_sample,\n    input_text=caption,\n    eos_token=\"[EOS]\",\n    key=\"caption\",\n    device=DEVICE,\n    text_tokenizer=text_tokenizer,\n)\nAnd that’s it! Everything else remains the same!\n\n\n2.4.2 Adding more metadata as input\nIn the demo application, we only utilised brightness and number of items as metadata inputs. But as descriped in the paper, we could have used many more metadata as inputs.\nTo pass in any of the metadata available here, just pass in v1=[key] v0=[val] to the input string.\nFor example, to add in metadata: “brightness 50/255 contrast 50/127 walkability 25/50”, simply write it as:\nv1=10 v0=50 v1=11 v0=50 v1=14 v0=25\nWe simply replace the words by their corresponding metadata key and add the value with v0=[val].\nAnd that’s it! Now the reader can also add any of the 20 metadata filters that the authors have trained the 4M-21 model on."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#python-code",
    "href": "posts/2024-06-28 ml-4M.html#python-code",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.3 Python Code",
    "text": "2.3 Python Code"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#python-code-for-image-retrieval",
    "href": "posts/2024-06-28 ml-4M.html#python-code-for-image-retrieval",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.3 Python code for image retrieval",
    "text": "2.3 Python code for image retrieval\nWe will follow the demo notebook shared by the authors and build the retrieval sytem ourselves using a custom database (in this case a sample of 10 images).\nAs part of the demo shown in Figure 1, we utilised metadata and caption similar to Figure 3 to build our demo. By providing a caption, and metadata we were able to retrieve the most similar images.\nLet’s start with building our dataset first.\n\n2.3.1 Building the database\nSince we wanted to showcase image description, brightness and\n\n\n2.3.2 Inference with model to get similar images\n\n\n2.3.3 Gradio app with required filters\n\n\n2.3.4 Deploy app to HuggingFace hub"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#building-the-database",
    "href": "posts/2024-06-28 ml-4M.html#building-the-database",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.4 Building the database",
    "text": "2.4 Building the database"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#inference-with-model-to-get-similar-images",
    "href": "posts/2024-06-28 ml-4M.html#inference-with-model-to-get-similar-images",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.5 Inference with model to get similar images",
    "text": "2.5 Inference with model to get similar images\n\n2.5.1 Caption + Metadata -&gt; RGB retrieval\nAs part of the demo shown in Figure 1, we utilised metadata and caption similar to Figure 3 to build our demo. By providing a caption, and metadata we were able to retrieve the most similar images.\nSo how do we achieve this?"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#sec-code",
    "href": "posts/2024-06-28 ml-4M.html#sec-code",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.3 Python code for image retrieval",
    "text": "2.3 Python code for image retrieval\nWe will closely follow the demo notebook shared by the authors and build the retrieval sytem on top of it using a custom database (in this case a sample of 15 images).\nAs also mentioned in the paper,\nOur model can also perform multimodal retrievals by predicting global embeddings of DINOv2 and ImageBind from any (subset) of the input modalities. Once the global embeddings are obtained, the retrieval is done by finding the retrieval set samples with the smallest cosine distance to the query.\nWe can utilize either Imagebind or Dino-V2 to encode images as embeddings, as part of this demo we utilise DINOv2 global embeddings for retrieval.\nImageBind: One Embedding Space To Bind Them All Girdhar et al. (2023) and DINOv2: Learning Robust Visual Features without Supervision Oquab et al. (2024) are both multi-modal vision models released previously by Meta. They are both capable of representing images to an embedding space. We donot dig deeper into these models as part of this blog post.\n\nGirdhar, Rohit, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. “ImageBind: One Embedding Space to Bind Them All.” https://arxiv.org/abs/2305.05665.\n\nOquab, Maxime, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, et al. 2024. “DINOv2: Learning Robust Visual Features Without Supervision.” https://arxiv.org/abs/2304.07193.\n\n2.3.1 Building the database\nSince we wanted to showcase image description, brightness and number of items, our database consists of 15 images downloaded manually using google image search. The complete database can be found - here.\n\n\n\n\n\n\nExpand to learn more about creating your own database\n\n\n\n\n\nCreating your own Huggingface dataset using an image folder is as simple as:\nfrom datasets import load_dataset\ndataset = load_dataset(\"imagefolder\", data_dir=\"path/to/data\")\ndataset.push_to_hub()\nYou can read more about it here.\n\n\n\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"aroraaman/4m-21-demo\")\nlen(dataset['train'])\n\n15\n\n\nThe dataset consists of a mix of dark and bright images of dining room and swimming pool. Some images contain lot of items and are cluttered while others look more “empty”. Images are of type .png, .jpg, .webp we & .jpeg.\n\ndataset['train']['image']\n\n[&lt;PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=852x1200&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x853&gt;,\n &lt;PIL.PngImagePlugin.PngImageFile image mode=P size=1500x1284&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=564x846&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=736x552&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=275x183&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x168&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=194x259&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=275x183&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=616x462&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=605x694&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=612x408&gt;,\n &lt;PIL.Image.Image image mode=RGB size=635x272&gt;,\n &lt;PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=800x533&gt;,\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2399x3229&gt;]\n\n\nNow that we have a list of images that we want to use as our database, let’s use DINOv2 to convert them to embeddings.\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom pathlib import Path\nimport numpy as np\nfrom PIL import Image\nimport albumentations as A\nimport torch\nfrom tqdm.notebook import tqdm\n\nNow, let’s load the DINOv2 model as our feature extractor. Speicifically we will be using the ViT-B14 version as mentioned in the Bachmann et al. (2024) paper.\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nfeature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\nfeature_extractor = feature_extractor.to(device)\n\nUsing cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main\nINFO:dinov2:using MLP layer as FFN\n\n\nWe transform every image by downsizing each image such that the shortest side is of size 224 pixels. We then center crop the image such that all images are of size 224x224. We use Albumentations library Buslaev et al. (2020) for the transforms.\n\nBuslaev, Alexander, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020. “Albumentations: Fast and Flexible Image Augmentations.” Information 11 (2). https://doi.org/10.3390/info11020125.\n\nclass FeatureExtractionDataset(Dataset):\n    def __init__(self, feature_extractor: nn.Module, path: str, img_sz=224):\n        super().__init__()\n        self.feature_extractor=feature_extractor\n        self.path = Path(path)\n        self.files = list(self.path.rglob(\"*\"))\n        self.tfms = A.Compose([\n            A.SmallestMaxSize(img_sz),\n            A.CenterCrop(img_sz, img_sz)\n        ])\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n    def __len__(self): return len(self.files)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.files[idx]).convert(\"RGB\")\n        img = np.array(img)\n        img = self.tfms(image=img)['image']\n        img = torch.tensor(img, dtype=torch.float32)/255.\n        img = img.permute(2,0,1)\n        return img\n\nNext, we can simply build the dataset, dataloader and store the image embeddings as a PyTorch tensor.\n# Create the Dataset\ndataset = FeatureExtractionDataset(\n    feature_extractor=feature_extractor, \n    path=\"/path/to/data\"\n)\n# Create the DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=batch_size,  \n    shuffle=False,\n    num_workers=16,  \n    pin_memory=True\n)\nFinally we can extract the features from each image and store as a PyTorch Tensor.\nfeatures = []\nfor i,batch in tqdm(enumerate(dataloader), total=(len(dataset)//batch_size)+1):\n    batch = batch.to(device)\n    with torch.no_grad():\n        _f = feature_extractor(batch)\n    _f = _f.to(\"cpu\")\n    features.append(_f)\n    \nfeatures = torch.concat(features)\ntorch.save(features, \"./image_embeddings.pt\")\nAnd that’s it! We have successfully created our image database that we will retrieve similar images from based on a query.\n\n\n2.3.2 Inference with 4M-21model EPFL-VILAB/4M-21_L to get most similar image\nSo now that we have the database, our next step is to actually be able to use inputs such as “caption”, “brightness” and “number of items” to get an embedding that will be used as our “query”.\nWe will closely follow the demo notebook shared by the authors.\n\nimport torch\nfrom fourm.models.fm import FM\nfrom fourm.vq.vqvae import VQVAE\nfrom tokenizers import Tokenizer\nfrom fourm.models.generate import (\n    GenerationSampler,\n    build_chained_generation_schedules,\n    init_empty_target_modality,\n    custom_text,\n)\nfrom fourm.data.modality_info import MODALITY_INFO\nfrom fourm.utils.plotting_utils import decode_dict\nfrom fourm.vq.vqvae import VQVAE\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 224\nTOKENIZER_PATH = \"./fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json\"\nFM_MODEL_PATH = \"EPFL-VILAB/4M-21_L\"\nIMAGE_DATASET_PATH = \"/home/ubuntu/GIT_REPOS/ml-4m/data/custom_data/\"\n\nAll tokenizers have been made available on the hub - EPFL VILAB. For our demo, we only need the text tokenizer, since we are using captions and metadata as inputs (both as text). We will also need to the fourm model to create the sampler that is able to create query embedding using input caption & metadata.\n\ntext_tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\nfm_model = FM.from_pretrained(FM_MODEL_PATH).eval().to(DEVICE)\nsampler = GenerationSampler(fm_model)\n\nBelow, our input conditional domains are caption & metadata. And our target domain is tok_dinov2_global. As discussed in the paper, we want to obtain the global embeddings of DINOv2 using input modalities for retrieval.\n\n\n\n\n\n\nNote\n\n\n\nThe authors shared how to do multimodal retrieval in code here.\n\n\n\n# Generation configurations\ncond_domains = [\"caption\", \"metadata\"]\ntarget_domains = [\"tok_dinov2_global\",]\ntokens_per_target = [16]\ngeneration_config = {\n    \"autoregression_schemes\": [\"roar\"],\n    \"decoding_steps\": [1],\n    \"token_decoding_schedules\": [\"linear\"],\n    \"temps\": [2.0],\n    \"temp_schedules\": [\"onex:0.5:0.5\"],\n    \"cfg_scales\": [1.0],\n    \"cfg_schedules\": [\"constant\"],\n    \"cfg_grow_conditioning\": True,\n}\ntop_p, top_k = 0.8, 0.0\n\nschedule = build_chained_generation_schedules(\n    cond_domains=cond_domains,\n    target_domains=target_domains,\n    tokens_per_target=tokens_per_target,\n    **generation_config\n)\n\nNow that we have a generation schedule to use caption and metadata as inputs to generate target tok_dinov2_global, we can create our dictionary of input and target modalities. let’s initialise the sample.\n\nbatched_sample = {}\nfor target_mod, ntoks in zip(target_domains, tokens_per_target):\n    batched_sample = init_empty_target_modality(\n        batched_sample, MODALITY_INFO, target_mod, 1, ntoks, DEVICE\n    )\n    \n\nLet’s say that we want to retrieve a dark image of a swimming pool. So our input caption would be ‘swimming pool’, and metadata is passed in combination of V1 and V0s.\nV1 represents which metadata to pass in, the encoding for each metadata type is here.\nBrightness is encoded with number 10, and takes in range of values from 0-255. 0 represents a dark image whereas 255 represents a bright image. So to represent a brightness of 50/255, we will write V1=10 V0=50.\n\ncaption = \"Swimming pool\"\nmetadata = \"v1=10 v0=68\" #brightness 68/255 as metadata\n\nLet’s create the required dictionaries by the model as input using custom_text method as in the demo notebook.\n\nbatched_sample = custom_text(\n    batched_sample,\n    input_text=caption,\n    eos_token=\"[EOS]\",\n    key=\"caption\",\n    device=DEVICE,\n    text_tokenizer=text_tokenizer,\n)\nbatched_sample = custom_text(\n    batched_sample,\n    input_text=metadata,\n    eos_token=\"[EOS]\",\n    key=\"metadata\",\n    device=DEVICE,\n    text_tokenizer=text_tokenizer,\n)\n\nNow, we can utilise the sampler that we created before to get the output from our model.\n\nout_dict = sampler.generate(\n    batched_sample,\n    schedule,\n    text_tokenizer=text_tokenizer,\n    verbose=True,\n    seed=0,\n    top_p=top_p,\n    top_k=top_k,\n)\n\n1it [00:00,  1.20it/s]\n\n\nThis output dictionary consists of tok_dinov2_global as key and the tensor represents the token IDs that make up the representation of the DINOv2 global embeddings.\n\nout_dict['tok_dinov2_global']['tensor']\n\ntensor([[5426, 6424, 5294, 5716,  189, 4065, 7631, 8145, 3108, 7638, 4331, 7005,\n         5675, 1472, 3069, 5687]], device='cuda:0')\n\n\nLet’s now use the decoder to get a 768 representation embedding for the image that becomes our “query” for retrieval purposes. To decode the tokens to the respective embedding, we will need to load the necessary VQ-VAE as well that was used during training.\n\nVQVAE_PATH = \"EPFL-VILAB/4M_tokenizers_DINOv2-B14-global_8k_16_224\"\nvqvae = VQVAE.from_pretrained(VQVAE_PATH)\n\nLet’s now get the image embeddings using decode_dict as in the demo notebook.\n\nwith torch.no_grad():\n    dec_dict = decode_dict(\n        out_dict, {\"tok_dinov2_global\": vqvae.to(DEVICE)}, text_tokenizer, \n        image_size=IMG_SIZE, patch_size=16, decoding_steps=1\n    )\n\n\ndec_dict[\"tok_dinov2_global\"].shape\n\ntorch.Size([768])\n\n\nAs can be seen we have an embedding of size 768 which is our query embedding. Using cosine similarity, we can retrieve the most similar embedding from our image database.\n\n\n\n\n\n\nFigure 7: Image retrieval using EPFL-VILAB/4M-21_L for “swimming pool” and 68/255 brightness\n\n\n\nAs can be seen, the model sucessfully returns the image of a swimming pool for low brightness. If we increased the brightness to 255/255 we get the following image.\n\n\n\n\n\n\nFigure 8: Image retrieval using EPFL-VILAB/4M-21_L for “swimming pool” and 255/255 brightness\n\n\n\n\n\n2.3.3 Gradio app with required filters\nNow that we have all the underlying code, we can simply build a Gradio interface for the same! Why? This makes it very easy for all to use and play with the 4M-21 model. Feel free to create your own apps too. If you do, please don’t forget to let me know about it on my Twitter - https://x.com/amaarora.\nThe code for the gradio app is pretty simple, I actually used Claude 3.5 Sonnet to help me build the app.\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Image Retrieval using 4M-21: An Any-to-Any Vision Model\")\n    with gr.Row():\n        with gr.Column(scale=1):\n            caption = gr.Textbox(\n                label=\"Caption Description\", placeholder=\"Enter image description...\"\n            )\n            brightness = gr.Slider(\n                minimum=0, maximum=255, value=5, step=1, \n                label=\"Brightness\", info=\"Adjust image brightness (0-255)\"\n            )\n            num_items = gr.Slider(\n                minimum=0, maximum=50, value=5, step=1, \n                label=\"Number of Items\", info=\"Number of COCO instances in image (0-50)\"\n            )\n        with gr.Column(scale=1):\n            output_images = gr.Gallery(\n                label=\"Retrieved Images\",\n                show_label=True,\n                elem_id=\"gallery\",\n                columns=2,\n                rows=2,\n                height=512,\n            )\n    submit_btn = gr.Button(\"Retrieve Most Similar Image\")\n    submit_btn.click(\n        fn=get_similar_images,\n        inputs=[caption, brightness, num_items],\n        outputs=output_images,\n    )\nUsing above code, allows us to create the Gradio app that was shared in Figure 1.\n\n\n\n\n\n\nFigure 9: caption & metadata retrieval using EPFL-VILAB/4M-21_L\n\n\n\n\n\n2.3.4 Deploy app to HuggingFace hub\nWe have deployed the app sucessfully to huggingface Spaces. We followed the documentation here.\nYou can find the huggingface space here.\nSomem minor changes that we had to do between local and for the app to deployed on huggingface spaces:\n\nAll binary files had to be tracked by git-lfs. Read more about it here\nConvert dataset to a huggingface dataset, as we were not able to upload .jpg, .png or other files\nThe complete source code for the gradio app that works on HF spaces can be found here.\n\nOverall it was pretty straightforward and easy to deploy!"
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#credits",
    "href": "posts/2024-06-28 ml-4M.html#credits",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.5 Credits",
    "text": "2.5 Credits\n\n\n\n\n\n\nNote\n\n\n\nThank you jarvislabs.ai for the compute, this blog post would not have been possible without the credits."
  },
  {
    "objectID": "posts/2024-06-28 ml-4M.html#conclusion",
    "href": "posts/2024-06-28 ml-4M.html#conclusion",
    "title": "Image retrieval app using Apple’s 4M-21 any-to-any vision model",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nAs part of this blog post, we looked into the 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities Bachmann et al. (2024) paper and built an image retriever app on top as a real world application.\n\nBachmann, Roman, Oğuzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, and Amir Zamir. 2024. “4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities.” https://arxiv.org/abs/2406.09406.\nIn Section 2.3, we also looked at the Python code to be able to build such an app on any custom database. We built a gradio app for demo purpose and also deployed it to Huggingface Spaces!\nAll code and corresponding files can be found here.\nFinally in Section 2.4, we looked at ways of extending the demo and adding color palettes and more metadata as input filters for retrieval!\nThank you readers for your time. If you have any feedback, please feel free to share it with me here."
  },
  {
    "objectID": "posts/2024-07-04 SWA.html",
    "href": "posts/2024-07-04 SWA.html",
    "title": "Sliding Window Attention",
    "section": "",
    "text": "As part of this blog post, we will look take a deep dive into Sliding Window Attention (SWA) that was introduced as part of the Longformer architecture (Beltagy, Peters, and Cohan (2020)), and also understand how it’s implemented in PyTorch!\nWhen I first started looking into sliding window attention, below tweet kind of summarises my journey. O thought it’s pretty complicated and hard to implement. But, as is usual with many things, the more time you spend on it, the easier it gets.\nHaving spent some time on digging through the LongerFormer implementation in Huggingface, I have realised that it’s really not that hard. But, first, let’s understand what sliding window attention really is and how it’s different from full-attention."
  },
  {
    "objectID": "posts/2024-07-04 SWA.html#introduction",
    "href": "posts/2024-07-04 SWA.html#introduction",
    "title": "Sliding Window Attention",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n\n\n\n\n\nFigure 1: Comparing the full self-attention pattern and the configuration of attention patterns in Longformer\n\n\n\nThe above image from the Longformer paper (Beltagy, Peters, and Cohan (2020)), summarises the difference between Full \\(n^2\\) attention & Sliding window attention.\nIn the traditional sense, \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nEach token in the Query vector \\(Q\\) can attend to all tokens in the Key vector \\(K\\).\nBut, this leads to a computational complexity of \\(O(n^2)\\). As a result, memory requirements grow by a factor of \\(n^2\\) for a sequence of length \\(n\\).\nThis limits the traditional Transformer architecture from having long context length. The solution is to use Sliding window attention where each token in the Query vector \\(Q\\) only attends to it’s neighbouring tokens with an overlap of window length \\(w\\).\nSo, a token at position \\(i\\) in \\(Q\\), can attend to tokens in range \\((i-w, i+w)\\) in \\(K\\)."
  },
  {
    "objectID": "posts/2024-07-04 SWA.html#sec-matmul",
    "href": "posts/2024-07-04 SWA.html#sec-matmul",
    "title": "Sliding Window Attention",
    "section": "2 Matrix multiplication using torch.einsum",
    "text": "2 Matrix multiplication using torch.einsum\n\n\n\n\n\n\nFigure 2: Visualisation of matrix multiplication from http://matrixmultiplication.xyz/\n\n\n\nBefore we get started with Sliding Window Attention, let’s implement \\(Q.K^T\\) matrix multiplication with the help of torch.einsum.\nFor a refresher/introduction to matrix multiplication and torch.einsum, I recommend the below amazing lecture by Jeremy Howard.\n\nTo implement, \\(Q.K^T\\) using Einstein summation is as easy as doing:\n\nimport torch \nq = torch.arange(1, 9).reshape(4,2)\nk = torch.arange(1, 9).reshape(4,2)\nout = torch.einsum('xd,yd-&gt;xy', q, k)\nout.shape\n\ntorch.Size([4, 4])\n\n\n\n\n\n\n\n\nMore on torch.einsum\n\n\n\nI would recommend the readers to play around with torch.einsum notation, try writing simple matrix multiplications and see the results for yourself to get an intuition.\nx = torch.tensor([7,6,5,4]).unsqueeze(1)\ny = torch.arange(start=1, end=5).reshape(1,4)\ntorch.einsum(\"ij, jk\", x,y)\nAs for why torch.einsum('xd,yd-&gt;xy', q, k) represents \\(Q.K^T\\), here’s a detailed explanation:\n\n“xd, yd -&gt; xy” specifies the operation: x and y represent the outer dimensions (4x4) & d represents the inner dimension for multiplication (2, in this case)\nThe result is a 4x4 tensor where each element is the dot product of a row from \\(q\\) with a column from \\(k\\)\n\n\n\nBefore moving on the next section, I would recommend that the readers make sure that they can correlate below outputs with Figure 2.\n\nq,k.T,out\n\n(tensor([[1, 2],\n         [3, 4],\n         [5, 6],\n         [7, 8]]),\n tensor([[1, 3, 5, 7],\n         [2, 4, 6, 8]]),\n tensor([[  5,  11,  17,  23],\n         [ 11,  25,  39,  53],\n         [ 17,  39,  61,  83],\n         [ 23,  53,  83, 113]]))\n\n\nGreat, now that we know what Sliding Window Attention is, and how to use einstum summation to do matrix multiplication, we are ready to see how Sliding Window Attention can be implemented in PyTorch."
  },
  {
    "objectID": "posts/2024-07-04 SWA.html#sliding-window-attention-in-pytorch",
    "href": "posts/2024-07-04 SWA.html#sliding-window-attention-in-pytorch",
    "title": "Sliding Window Attention",
    "section": "3 Sliding window attention in PyTorch",
    "text": "3 Sliding window attention in PyTorch\nFrom Appendix A of the LongFormer paper (implementation detail, text slightly modified to match implementation):\nLongformer-chunks only supports the nondilated case. It chunks Q and K into overlapping blocks of size \\(2*w\\) and overlap of size \\(w\\), multiplies the blocks, then mask out the diagonals. This is very compute efficient because it uses a single matrix multiplication operation from PyTorch, but it consumes \\(2x\\) the amount of memory a perfectly optimized implementation should consume because it computes some of the zero values. Because of the compute efficiency, this implementation is most suitable for the pretrain/finetune case. We didn’t find the increase in memory to be a problem for this setting.\n\nTo explain further, to achieve the same results as Figure 1 (b), it is possible to divide the Query \\(Q\\) and Key \\(K\\) vectors to chunks of size \\(2*w\\), where \\(w\\) represents the window length or the overlap size. Then, we can perform the attention operation and get scores by doing \\(Q.K^T\\) within the chunks themselves! This way, it’s very efficient as it only involves a single matrix multiplication operation.\n\nLet’s see how the above translates to PyTorch code. Let’s define a query \\(q\\) and a key \\(k\\) vector of batch size 1, sequence length 8 and embedding size 768.\n\nimport torch\n\n\nq = torch.randn(1, 8, 768)\nk = torch.randn(1, 8, 768)\n\nLet’s assume a query and key vector of batch size 1, sequence length 8 and embedding size of 768. These can be converted to overlapping chunks using the _chunk function below.\n\n\n\n\n\n\nFigure 3: Chunking overview\n\n\n\nGiven a reference image above, in PyTorch implementation, we don’t really need to create three separate vectors, but instead we can create one called overlapping_chunks with the right shape and overlap.\n\ndef _chunk(hidden_states, window_overlap):\n    \"\"\"convert into overlapping chunks. Chunk size = 2w, overlap = w\"\"\"\n    chunk_size = [\n        hidden_states.size(0), #bs\n        torch.div(hidden_states.size(1), window_overlap, rounding_mode=\"trunc\") - 1, #n_chunks\n        window_overlap * 2,\n        hidden_states.size(2),\n    ]\n\n    overlapping_chunks = torch.empty(chunk_size, device=hidden_states.device)\n    for chunk in range(chunk_size[1]):\n        overlapping_chunks[:, chunk, :, :] = hidden_states[\n            :, chunk * window_overlap : chunk * window_overlap + 2 * window_overlap, :\n        ]\n    return overlapping_chunks\n\nLet’s check the key & query shapes after chunking. In total we have 3 chunks, where the chunk size is 4.\n\nquery = _chunk(q, window_overlap=2)\nkey   = _chunk(k, window_overlap=2)\nquery.shape, key.shape\n\n(torch.Size([1, 3, 4, 768]), torch.Size([1, 3, 4, 768]))\n\n\nFinally, we can now perform sliding window attention using torch.einsum. This is where the matrix multiplication of between query \\(Q\\) and key (transposed) \\(K^T\\) occurs using torch.einsum.\n\ndiagonal_chunked_attention_scores = torch.einsum(\"bcxd,bcyd-&gt;bcxy\", (query, key)) \ndiagonal_chunked_attention_scores.shape\n\ntorch.Size([1, 3, 4, 4])\n\n\nBy performing matrix multiplication \\(Q.K^T\\) within chunks, we have succesfully replicated Figure 1 (b) in PyTorch. Had we not created any chunks, and done our matmul operation on all of \\(Q\\) and \\(K^T\\), it would have been equivalent to Figure 1 (a).\nAnd that’s really it! This is all the magic behind Sliding Window Attention from the Longformer architecture. (Beltagy, Peters, and Cohan (2020)).\n\nBeltagy, Iz, Matthew E. Peters, and Arman Cohan. 2020. “Longformer: The Long-Document Transformer.” https://arxiv.org/abs/2004.05150."
  },
  {
    "objectID": "posts/2024-07-04 SWA.html#conclusion",
    "href": "posts/2024-07-04 SWA.html#conclusion",
    "title": "Sliding Window Attention",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nAs part of this blog post, we first looked at the difference full-attention with complexity \\(O(n^2)\\) and sliding window attention. Figure 1\nNext, we learnt how to easily perform \\(Q.K^T\\) using torch.einsum. Finally, we saw that by converting Query \\(Q\\) and Key \\(K\\) to chunks, we can easily implement sliding window attention in PyTorch."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html",
    "href": "posts/2023-04-30_LaMini-LM.html",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "",
    "text": "Overview of LaMini-LM\n\n\nAs part of this blog post, we will be reviewing the LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions (Wu et al. (2023)) paper released on Apr 27, 2023.\nThe main objectives of this blog post are:\n\nTo provide a thorough review of LaMini-LM (Wu et al. (2023)) reseasrch paper.\nWe will try to replicate most figures and share commentary on why some of the results might me misleading. Specifically we replicate Figure 5 in Section 3.2.\nRecreate a small sample of the 2.58M shared Instruction Dataset to showcase the complete process - see Section 2.\nCompare similarity on generated instructions using sentence embeddings generated by using Sentence Transformer from HuggingFace.\nShare all code in PyTorch that is used as part of this blog post.\nCompare evaluation results from the paper with our own evaluation on multiple NLP datasets - see Section 6.\nWe do our own human evaluation in Section 6.1. Honestly speaking, for the three results, the results for the 1.5B GPT-2 LaMini model, the results were unsatisfactory. 😔"
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#introduction",
    "href": "posts/2023-04-30_LaMini-LM.html#introduction",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "",
    "text": "Overview of LaMini-LM\n\n\nAs part of this blog post, we will be reviewing the LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions (Wu et al. (2023)) paper released on Apr 27, 2023.\nThe main objectives of this blog post are:\n\nTo provide a thorough review of LaMini-LM (Wu et al. (2023)) reseasrch paper.\nWe will try to replicate most figures and share commentary on why some of the results might me misleading. Specifically we replicate Figure 5 in Section 3.2.\nRecreate a small sample of the 2.58M shared Instruction Dataset to showcase the complete process - see Section 2.\nCompare similarity on generated instructions using sentence embeddings generated by using Sentence Transformer from HuggingFace.\nShare all code in PyTorch that is used as part of this blog post.\nCompare evaluation results from the paper with our own evaluation on multiple NLP datasets - see Section 6.\nWe do our own human evaluation in Section 6.1. Honestly speaking, for the three results, the results for the 1.5B GPT-2 LaMini model, the results were unsatisfactory. 😔"
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-data-gen",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-data-gen",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "2 Dataset Generation",
    "text": "2 Dataset Generation\n\n2.1 Introduction\nThe authors provide a large dataset of 2.58M instructions and responses.\nTwo examples in the LaMini-instruction data:\n{\n    \"instruction\": \"What is the capital of Australia?\\n\\nIs Canberra the only city chosen to serve as a capital due to a compromise between Sydney and Melbourne?\", \n    \"response\": \"The capital of Australia is Canberra. Yes, Canberra was chosen as the capital of Australia in 1908 due to a compromise between Sydney and Melbourne, as neither city wanted the other to become the capital.\",\n    \"instruction_source\": \"generated_flan\"\n}\n\n\n{\n    \"instruction\": \"Create a poem that has the word \\\"ocean\\\" in every other line.\", \n    \"response\": \"The ocean's depth is a mystery,\\nIts waves crashing with energy.\\nSalt air fills the coastal scene,\\nThe ocean's blue hues pure and clean.\\n\\nSeagulls cry out a melancholy song,\\nThe ocean's rhythm steady and strong.\\nSunsets paint the water gold,\\nThe ocean's secrets forever untold.\",\n    \"instruction_source\": \"self_instruct_without_topic\"\n}\nThe dataset is built on top of existing resources and also newer “instructions”+“responses”.\nFrom the paper:\n\nWe collate instructions from various prior datasets such as self-instruct (Wang et al. (2022)), P3 (Sanh et al. (2022)), FLAN (Longpre et al. (2023)) and Alpaca (Taori et al. (2023)).\n\nThe researchers have collated existing resources and also generated a new set “instructions+responses” using gpt-3.5-turbo (ChatGPT) using Self-Instruct approach (Wang et al. (2022)). At the time of writing I believe this is the biggest Instruction dataset available.\nBelow I provide an overview of the existing datasets that are part of the 2.58M LaMini-LM dataset:\n\nSelf-Instruct: Instruction, input, and output samples from a language model. (Wang et al. (2022))\nP3: P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks. Hosted at HuggingFace here.\nFLAN: Instruction dataset on a wide variety of datasets (473 datasets, 146 task categories, and 1,836 total tasks) using various instruction templates. Refer to the GitHub repo for more details.\nAlpaca: 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003. (Taori et al. (2023))\n\nThe authors use two strategies to generate instructions on top of existing ones which they called:\n\nExample-guided\nTopic-guided\n\nLet’s look at them in detail in the following sections.\n\n\n2.2 Example Guided\nExample guided generation follows Wang et al. (2022) & Taori et al. (2023).\nSpecifically, the authors include only few random examples, and some limited constraints as shown in the example prompt in Figure 1.\nNewer instructions are generated by providing these examples from existing datasets - Self-Instruct (\\(X_{SI}\\)), P3 (\\(X_{P3}\\)) & FLAN (\\(X_{FLAN}\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe number of in-context examples used for generation of \\(X_{SI}\\) is 3 whereas for \\(X_{P3}\\) & \\(X_{FLAN}\\) is 2. This is because the instructions in \\(X_{P3}\\) & \\(X_{FLAN}\\) are longer in length compared to \\(X_{SI}\\). This is due to token limits of ChatGPT.\n\n\n\n\n\n\n\n\nFigure 1: An example of instruction generation prompt based on three random examples from self-instruct\n\n\n\nTo generate your own instructions using ChatGPT, either paste the above prompt in ChatGPT, or we can use the openai API like so:\n\n\nCode\nimport openai\nopenai.api_key = \"sk_\" #Your API key goes here \n\nN = 20\nexamples = [\n    'What are some things you can do to de-stress?', \n    'How can individuals and organizations reduce unconscious bias?',\n    'Write a program to compute the sum of integers from k to n.'\n]\n\nprompt=f\"\"\"\n&lt;example&gt;{examples[0]}&lt;/example&gt;\n&lt;example&gt;{examples[1]}&lt;/example&gt;\n&lt;example&gt;{examples[2]}&lt;/example&gt;\n\nGenerate {N} diverse examples that are similar to the provided examples.\nYou do not need to provide a response to the generated examples.\nEach example must include an instruction.\nEach generated instruction can be either an imperative sentence or a question.\nEach example must start with the label \"&lt;example&gt;\" and end with the label \"&lt;/example&gt;\".\n\"\"\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=messages,\n    temperature=0, # not specified in the paper \n)\nresponse.choices[0].message[\"content\"]\n\n\nIn the above code, you can see how we can easily replace the examples list with a function that looks like - get_random_examples(n=3, subset='self-instruct') and based on that get example instructions from the existing datasets. By using different subsets, we can generate different examples.\nThe instructions that are generated by using examples from \\(X_{SI}\\), \\(X_{P3}\\) & \\(X_{FLAN}\\) are referred to as \\(\\hat{X}_{SI}\\), \\(\\hat{X}_{P3}\\) & \\(\\hat{X}_{FLAN}\\). So, the below 20 generated instructions would be part of \\(\\hat{X}_{SI}\\) because the 3 examples are from \\(X_{SI}\\).\nRunning the above returns an output that looks like:\n&lt;example&gt;What are some healthy breakfast options?&lt;/example&gt;\n&lt;example&gt;How can you improve your public speaking skills?&lt;/example&gt;\n&lt;example&gt;Write a recipe for a vegan lasagna.&lt;/example&gt;\n&lt;example&gt;What are some ways to save money on groceries?&lt;/example&gt;\n&lt;example&gt;How can you improve your time management skills?&lt;/example&gt;\n&lt;example&gt;Write a workout plan for beginners.&lt;/example&gt;\n&lt;example&gt;What are some tips for studying effectively?&lt;/example&gt;\n&lt;example&gt;How can you improve your writing skills?&lt;/example&gt;\n&lt;example&gt;Write a program to find the largest number in an array.&lt;/example&gt;\n&lt;example&gt;What are some ways to improve your memory?&lt;/example&gt;\n&lt;example&gt;How can you improve your interpersonal communication skills?&lt;/example&gt;\n&lt;example&gt;Write a step-by-step guide for making a paper airplane.&lt;/example&gt;\n&lt;example&gt;What are some ways to reduce your carbon footprint?&lt;/example&gt;\n&lt;example&gt;How can you improve your problem-solving skills?&lt;/example&gt;\n&lt;example&gt;Write a program to check if a number is prime.&lt;/example&gt;\n&lt;example&gt;What are some ways to improve your creativity?&lt;/example&gt;\n&lt;example&gt;How can you improve your leadership skills?&lt;/example&gt;\n&lt;example&gt;Write a guide for making homemade soap.&lt;/example&gt;\n&lt;example&gt;What are some healthy breakfast options?&lt;/example&gt;\n&lt;example&gt;What are some ways to improve your emotional intelligence?&lt;/example&gt;\n\n\n2.3 Topic Guided\nThe process and prompt for topic guided instruction generation is slightly different from example-guided instruction generation.\nThe overall process for topic-guided generation looks like:\n\nFind a list of common categories from Wikipidea (Total 3.5M)\nFilter out topics based on two rules.\n\nThe category must be less than three words.\nThe category must comprise more than 10 sub-categories and 50 pages.\n\nUse the prompt in Figure 2 and provide random examples from the same dataset and 3 topics obtained after filtering.\n\n\n\n\n\n\n\nNote\n\n\n\nAfter filtering, the authors obtain a list of 3.5K categories that serve as common topics.\n\n\n\n\n\n\n\n\nFigure 2: An example of instruction generation prompt based on three random examples from self-instruct and three random topics.\n\n\n\n\n\nCode\nimport openai\nopenai.api_key = \"sk_\" #Your API key goes here \n\nN = 20\nexamples = [\n    'Try coming up with a creative way to stay motivated during a workout.', \n    'In your opinion, what are the qualities of an effective sports coach?',\n    'Return the SSN number for the person: \"Yann LeCun\"'\n]\ntopics = ['Machine Learning', 'Infantry', 'Design bureaus']\n\nprompt=f\"\"\"\n&lt;example&gt;{examples[0]}&lt;/example&gt;\n&lt;example&gt;{examples[1]}&lt;/example&gt;\n&lt;example&gt;{examples[2]}&lt;/example&gt;\n\nGenerate {N} diverse examples that are similar to the provided examples with the topics {topics[0]}, {topics[1]}, {topics[2]}\".\nYou do not need to provide a response to the generated examples. \nEach example must include an instruction. Each generated instruction can be either an imperative sentence or a question. \nEach example must start with the label \"&lt;example&gt;\" and end with the label \"&lt;/example&gt;\".\".\"\"\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=messages,\n    temperature=0, # not specified in the paper \n)\nprint(response.choices[0].message[\"content\"])\n\n\nAs before, we can easily replace the examples list with a function that looks like - get_random_examples(n=3, subset='self-instruct') & also replace topics with a function that looks like - get_random_topics(n=3, subset='wiki-categories').\nRunning the above code returns an output that looks like:\n&lt;example&gt;What are some common machine learning algorithms and their applications?&lt;/example&gt;\n&lt;example&gt;Design a new weapon for the infantry that is both effective and lightweight.&lt;/example&gt;\n&lt;example&gt;Retrieve the contact information for a design bureau specializing in sustainable architecture.&lt;/example&gt;\n&lt;example&gt;How can machine learning be used to improve healthcare outcomes?&lt;/example&gt;\n&lt;example&gt;Create a workout plan for an infantry soldier to improve their endurance and strength.&lt;/example&gt;\n&lt;example&gt;What are some key considerations when designing a user interface for a mobile app?&lt;/example&gt;\n&lt;example&gt;Find a machine learning library that is compatible with Python.&lt;/example&gt;\n&lt;example&gt;Develop a training program for infantry soldiers to improve their marksmanship skills.&lt;/example&gt;\n&lt;example&gt;What are some ethical concerns surrounding the use of machine learning in decision-making?&lt;/example&gt;\n&lt;example&gt;Design a new vehicle for the infantry that can navigate difficult terrain.&lt;/example&gt;\n&lt;example&gt;Research and compare different design bureaus to find one that aligns with your project goals.&lt;/example&gt;\n&lt;example&gt;How can machine learning be used to improve customer service in the retail industry?&lt;/example&gt;\n&lt;example&gt;Create a nutrition plan for an infantry soldier to optimize their performance in the field.&lt;/example&gt;\n&lt;example&gt;What are some best practices for designing a logo for a new brand?&lt;/example&gt;\n&lt;example&gt;Implement a machine learning algorithm to predict customer churn for a telecommunications company.&lt;/example&gt;\n&lt;example&gt;Develop a training program for infantry soldiers to improve their communication and teamwork skills.&lt;/example&gt;\n&lt;example&gt;What are some challenges that arise when designing for virtual reality?&lt;/example&gt;\n&lt;example&gt;Find a design bureau that specializes in creating interactive exhibits for museums.&lt;/example&gt;\n&lt;example&gt;How can machine learning be used to improve fraud detection in the financial industry?&lt;/example&gt;\n&lt;example&gt;Design a new piece of equipment for the infantry that can be used in urban environments.&lt;/example&gt;\nSome key things to note just from the small sample above, instructions like\n\n“Design a new piece of equipment for the infantry that can be used in urban environments”\n“Research and compare different design bureaus to find one that aligns with your project goals”\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nare noisy. As also mentioned in the paper, ChatGPT has failed to provide enough context for the instructions.\n\n“Design a new piece of equipment for the infantry that can be used in urban environments”\n\nThe above instruction IMHO is very generic.\n\n“Research and compare different design bureaus to find one that aligns with your project goals”\n\nThe model has failed to define project goals or say anything about the “project”\n\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nThe model is asking to generate contact information, and it’s the response as we will see in the next section that’s more vague, not just the instruction.\n\n\n2.4 Response Generation\nLet’s collate the above instructions and generate responses for each one to create the resulting pairs. One could simply copy paste the instructions in ChatGPT or use the openAI API as before.\nLet’s take five instructions as examples to generate a .jsonl type dataset as below which can then be used to finetune models using the openAI API.\n\n\nCode\nimport openai\nfrom collections import defaultdict\n\nopenai.api_key = \"sk_\" #Your API key goes here \n\ndataset = defaultdict(dict)\n\ninstructions = [\n    \"&lt;example&gt;What are some common machine learning algorithms and their applications?&lt;/example&gt;\",\n    \"&lt;example&gt;Design a new weapon for the infantry that is both effective and lightweight.&lt;/example&gt;\",\n    \"&lt;example&gt;Retrieve the contact information for a design bureau specializing in sustainable architecture.&lt;/example&gt;\",\n    \"&lt;example&gt;How can machine learning be used to improve healthcare outcomes?&lt;/example&gt;\",\n    \"&lt;example&gt;Create a workout plan for an infantry soldier to improve their endurance and strength.&lt;/example&gt;\",\n]\n\nfor idx, inst in enumerate(instructions):    \n    prompt = f\"\"\"Given the following instruction separated by `&lt;example&gt;`, generate a response.\n    Response must start with the label \"&lt;response&gt;\" and end with the label \"&lt;/response&gt;\".\n    \n    Instruction: {inst}    \n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model='gpt-3.5-turbo',\n        messages=messages,\n        temperature=0, # not specified in the paper \n    )\n    dataset[idx] = {'prompt': inst, 'completion':response.choices[0].message[\"content\"]}\n\n\nRunning above code will give us a dataset that can be used to finetune the base models using OpenAI. This dataset looks something like:\ndefaultdict(dict,\n            {0: {'prompt': '&lt;example&gt;What are some common machine learning algorithms and their applications?&lt;/example&gt;',\n              'completion': '&lt;response&gt;Some common machine learning algorithms and their applications include: \\n- Linear Regression: used for predicting numerical values\\n- Logistic Regression: used for classification problems\\n- Decision Trees: used for both classification and regression problems\\n- Random Forest: used for classification, regression, and feature selection\\n- Support Vector Machines: used for classification and regression problems\\n- K-Nearest Neighbors: used for classification and regression problems\\n- Neural Networks: used for complex problems such as image recognition and natural language processing&lt;/response&gt;'},\n             1: {'prompt': '&lt;example&gt;Design a new weapon for the infantry that is both effective and lightweight.&lt;/example&gt;',\n              'completion': '&lt;response&gt;A possible solution for a new weapon for the infantry that is both effective and lightweight could be a compact assault rifle that uses advanced materials and technology to reduce weight without sacrificing firepower. The rifle could have a modular design that allows for easy customization and upgrades, and could also incorporate features such as a suppressor and a holographic sight for improved accuracy. Additionally, the rifle could be designed to be easily disassembled and reassembled for maintenance and cleaning in the field.&lt;/response&gt;'},\n             2: {'prompt': '&lt;example&gt;Retrieve the contact information for a design bureau specializing in sustainable architecture.&lt;/example&gt;',\n              'completion': '&lt;response&gt;Here is the contact information for a design bureau specializing in sustainable architecture:&lt;/response&gt;'},\n             3: {'prompt': '&lt;example&gt;How can machine learning be used to improve healthcare outcomes?&lt;/example&gt;',\n              'completion': '&lt;response&gt;Machine learning can be used to improve healthcare outcomes by analyzing large amounts of patient data to identify patterns and predict potential health issues. This can lead to earlier diagnosis and treatment, as well as personalized treatment plans based on individual patient characteristics. Additionally, machine learning can help healthcare providers identify patients who are at risk for readmission or complications, allowing for targeted interventions to improve outcomes and reduce costs.&lt;/response&gt;'},\n             4: {'prompt': '&lt;example&gt;Create a workout plan for an infantry soldier to improve their endurance and strength.&lt;/example&gt;',\n              'completion': '&lt;response&gt;Here is a workout plan for an infantry soldier to improve their endurance and strength:&lt;/response&gt;\\n- Warm up: 5-10 minutes of light cardio (jogging, jumping jacks, etc.)\\n- Circuit 1: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 push-ups\\n  - 20 squats\\n  - 20 lunges (10 per leg)\\n  - 1-minute plank\\n- Circuit 2: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 burpees\\n  - 20 jumping jacks\\n  - 20 mountain climbers (10 per leg)\\n  - 1-minute wall sit\\n- Circuit 3: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 dumbbell rows (10 per arm)\\n  - 20 dumbbell curls (10 per arm)\\n  - 20 dumbbell overhead presses (10 per arm)\\n  - 1-minute rest\\n- Cool down: 5-10 minutes of stretching and foam rolling.'}})\nFrom the smallest of examples, it appears as though:\n{\n    'prompt': '&lt;example&gt;Retrieve the contact information for a design bureau specializing in sustainable architecture.&lt;/example&gt;',\n    'completion': '&lt;response&gt;Here is the contact information for a design bureau specializing in sustainable architecture:&lt;/response&gt;'\n}\nis not of high quality. This small exercise indicates that there might be more noise in the 2.58M “instruction+response” dataset shared by the authors of LaMini-LM."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-data-exploration",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-data-exploration",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "3 Dataset Exploration",
    "text": "3 Dataset Exploration\nIn the last section I shared how the dataset generation looks like for LaMini-LM. In this section we will explore the 2.58M instruction dataset. The dataset has been shared publicly and is available on HuggingFace here.\n\n\n\n\n\n\nFigure 3: Dataset Preview on Huggingface.\n\n\n\n\n3.1 Statistics\nSome statistics about the dataset from the research paper have been shared in Figure 4 below.\n\n\n\n\n\n\nFigure 4: Data statistics of the generated dataset.\n\n\n\nAs can be seen aboce, in total there are 2.58M samples in LaMini-LM. It can be observed that the instructions for \\(D_{P3}\\) & \\(D_{FLAN}\\) are in general longer compared to the rest.\n\n\n\n\n\n\nNote\n\n\n\nThis was also mentioned in Section 2.2, and this is why authors used 2 in-context examples for \\({X_{P3}}\\) and \\(X_{FLAN}\\) compared to 3 in \\(X_{SI}\\).\n\n\n\n\n3.2 Diversity\nAs part of this section we will be looking at the diversity in the generated instructions. We will also try to recreate Figure 5 ourselves using sentence-transformers.\nThe authors took a sample of 50K instructions from \\({\\hat{X}_{SI}}\\), \\({\\hat{X}_{A}}\\), \\({\\hat{X}_{P3}}\\) & \\(X_{P3}\\) and visualised t-SNE of instruction sentence embeddings that were computed using Sentence Transformer.\nThe t-SNE figure has been shared below.\n\n\n\n\n\n\nFigure 5: The t-SNE visualizations of 50k sample of instruction sentence embeddings.\n\n\n\nSome comments about the the t-SNE visualisation directly from the paper:\n\nWe observe that \\(\\hat{X}_{SI}\\) exhibits greater diversity than \\(\\hat{X}_A\\) and \\(\\hat{X}_{P3}\\) is slightly more diverse than \\(X_{P3}\\).\n\nBut in no way does having a wider spread in \\(\\hat{X}_{SI}\\) and \\(\\hat{X}_{P3}\\) signify that the instructions are of higher quality. What if the instructions are meaningless?\nFor example one of the instructions from the small 20 instructions that were generated in Section 2.3 is:\n\nRetrieve the contact information for a design bureau specializing in sustainable architecture.\n\nAnd it’s not just the instruction, but rather the response too:\n{\n    'prompt': '&lt;example&gt;Retrieve the contact information for a design bureau specializing in sustainable architecture.&lt;/example&gt;',\n    'completion': '&lt;response&gt;Here is the contact information for a design bureau specializing in sustainable architecture:&lt;/response&gt;'\n}\nI think by training on such examples that might not be of high quality, we are allowing the model to hallucinate.\n\n\n\n\n\n\nHallucination\n\n\n\nWhen the model tries to answer questions it has no information about, the model is referred to be “hallucinating”.\n\n\n\n\nCode\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom umap import UMAP\nfrom tqdm.notebook import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import load_dataset, load_dataset_builder\n\n\n\n\nCode\nds_builder = load_dataset_builder(\"MBZUAI/LaMini-instruction\")\nds_builder.info.features\n\n\n{'instruction': Value(dtype='string', id=None),\n 'response': Value(dtype='string', id=None),\n 'instruction_source': Value(dtype='string', id=None)}\n\n\n\n\nCode\ndataset = load_dataset(\"MBZUAI/LaMini-instruction\")\ndataset\n\n\nFound cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['instruction', 'response', 'instruction_source'],\n        num_rows: 2585615\n    })\n})\n\n\nTotal of 2582019 samples in the dataset ➡️ 2.58M. Also, we have a column instruction_source that matches Dataset in Figure 4. First, we filter out the datasets based on source. We are trying to replicate Figure 5.\n\n\nCode\nX_alpaca = dataset.filter(lambda example: example[\"instruction_source\"]=='alpaca')\nX_p3     = dataset.filter(lambda example: example[\"instruction_source\"]=='original_p3')\nX_hat_si = dataset.filter(lambda example: example[\"instruction_source\"] in ['self_instruct_with_topic', 'self_instruct_without_topic'])\nX_hat_p3 = dataset.filter(lambda example: example[\"instruction_source\"]=='generated_p3')\nX_alpaca['train'].num_rows, X_p3['train'].num_rows, X_hat_si['train'].num_rows, X_hat_p3['train'].num_rows\n\n\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9195cf0efbc66452.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4d4436fd5c79b44c.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-990830c59dd517ae.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b3523be466a9a289.arrow\n\n\n(51985, 464396, 550137, 297312)\n\n\nNext, let’s keep the 50K sample from each source as per the research paper.\n\n\nCode\nsample_dict = {}\nfor X in tqdm([X_hat_si, X_hat_p3, X_alpaca, X_p3]):\n    np.random.seed(123)\n    idxs = np.random.choice(X['train'].num_rows, 50000)\n    sample_50k = X['train'][idxs]\n    src = np.unique(sample_50k['instruction_source'])[0]\n    if str(src).startswith('self_instruct'): src = 'generated_self_instruct'\n    sample_dict[src] = sample_50k['instruction']\ndf = pd.DataFrame(sample_dict)\ndf.head(2)\n\n\n\n\n\n\n\n\n\n\n\n\ngenerated_self_instruct\ngenerated_p3\nalpaca\noriginal_p3\n\n\n\n\n0\nHow does tobacco use affect the cardiovascular...\nWhat do you read in your free time?\\nRead a be...\nClassify the types of data structures.\nI know that the answer to the question \"What h...\n\n\n1\nWatch a sitcom and write down three humorous s...\nSuppose a survey found that the majority of pa...\nDetermine how this example sentence illustrate...\nThe toddler became cranky. \\n\\nI am hesitating...\n\n\n\n\n\n\n\nNow that we have the 50K sample, we could just use sentence-transformer to create the embeddings. I have already done that using a GPU.\n\n\nCode\n# Convert to Sentence embeddings and then apply `UMAP` to get 2D projections\nif not os.path.exists('../assets/projections_alpaca.npy'):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    for col in tqdm(df.columns):\n        sentence_embeddings = model.encode(df[col], batch_size=256, show_progress_bar=True, device='cuda')\n        umap_2d = UMAP(random_state=0)\n        umap_2d.fit(sentence_embeddings)\n        projections = umap_2d.transform(sentence_embeddings)\n        np.save(f'../assets/projections_{col}.npy', projections)\n\n\nLet’s load the UMAP projections and store in a new DataFrame called df_proj.\n\n\nCode\ndf_proj = pd.DataFrame()\nfor col in df.columns:\n    projections = np.load(f'../assets/projections_{col}.npy')\n    _df = pd.DataFrame(projections, columns=[f'{col}_0', f'{col}_1'])\n    _df[col] = df[col]\n    df_proj = pd.concat([df_proj, _df], axis=1)\n\n\n\n\nCode\nax = sns.scatterplot(data=df_proj, x='generated_self_instruct_0', y='generated_self_instruct_1')\nsns.scatterplot(data=df_proj, x='alpaca_0', y='alpaca_1')\nax.set(xlabel='X', ylabel='Y');\nplt.legend(title='Dataset', loc='upper left', labels=['Self Instruct', 'Alpaca']);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThank you authors!\n\n\n\nPreviously, the dataset shared on Huggingface did not contain instruction_source column, but the authors were really kind enough to add it.\n\n\nHi! Thanks for the feedback. You are correct, we'll update the HF repo data with a new column shortly.\n\n— Alham Fikri Aji ((AlhamFikri?)) April 30, 2023\n\n\n\n\n\n\n3.3 Human Evaluation\nFrom the paper:\nWe follow the human evaluation protocol given by Wang et al. (2022a), which categorizes the quality of the generated text into four levels:\n\nRate-A: The generated text is of high quality;\nRate-B: The generated text is acceptable but has minor errors;\nRate-C: The generated text has significant errors in content.\nRate-D: The generated text is completely unacceptable.\n\nWe randomly sample 20 examples from each subset of \\(D_{ALL}\\) and one of the co-authors scores the generated text.\nIn general, both the generated instructions and the generated responses are of high quality as shown in Figure 6.\n\n\n\n\n\n\nFigure 6: Human evaluation results for the generated instruction dataset.\n\n\n\nAs part of this blog post, let’s look at self_instruct_with_topic and perform human evaluation on 20 samples ourselves.\n\n\nCode\ndataset_si_with_topic = dataset.filter(lambda example: example[\"instruction_source\"] == 'self_instruct_with_topic')\ndataset_si_with_topic = dataset_si_with_topic.shuffle(seed=42)\nsample_20 = dataset_si_with_topic['train'][range(20)]\n\n\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d9007dd1dde13ff9.arrow\nLoading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d54e2b73deab01dc.arrow\n\n\nNow let’s score the 20 samples for self_instruct_with_topic, below, I used a simple IpyWidget that I created using ChatGPT. :)\n\n\nCode\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create widgets to display the current example\ninstruction_widget = widgets.HTML(layout=widgets.Layout(width='50%'))\nresponse_widget = widgets.HTML(layout=widgets.Layout(width='25%'))\nscore_widget = widgets.Dropdown(options=[('', 0), ('1', 1), ('2', 2), ('3', 3), ('4', 4)], layout=widgets.Layout(width='25%'))\n\n# Create a container for the example\nexample_container = widgets.HBox([instruction_widget, response_widget, score_widget])\n\n# Create buttons for navigation\nprevious_button = widgets.Button(description='Previous')\nnext_button = widgets.Button(description='Next')\nsubmit_button = widgets.Button(description='Submit')\nclear_button = widgets.Button(description='Clear')\n\n# Keep track of the current example index\ncurrent_index = 0\n\n# Initialize a list to store the scores\nscores = [0] * len(sample_20['instruction'])\n\ndef update_example(index):\n    instruction_widget.value = sample_20['instruction'][index]\n    response_widget.value = sample_20['response'][index]\n    score_widget.value = scores[index]\n\ndef on_previous(button):\n    global current_index\n    scores[current_index] = score_widget.value\n    current_index = max(0, current_index - 1)\n    update_example(current_index)\n\ndef on_next(button):\n    global current_index\n    scores[current_index] = score_widget.value\n    current_index = min(len(sample_20['instruction']) - 1, current_index + 1)\n    update_example(current_index)\n\ndef on_submit(button):\n    scores[current_index] = score_widget.value\n    print('Scores:', scores)\n\ndef on_clear(button):\n    scores[current_index] = 0\n    score_widget.value = 0\n\n# Set button callbacks\nprevious_button.on_click(on_previous)\nnext_button.on_click(on_next)\nsubmit_button.on_click(on_submit)\nclear_button.on_click(on_clear)\n\n# Display the example container and navigation buttons\ndisplay(example_container)\ndisplay(widgets.HBox([previous_button, next_button]))\ndisplay(widgets.HBox([submit_button, clear_button]))\n\n# Initialize the first example\nupdate_example(current_index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: IpyWidget for scoring instructions & responses\n\n\n\nYou can see how hard it is to score even 20 samples each. Scoring is an intensive task especially when it is about topics that the labeler has no idea about. Above, the instruction is “What are some behavioral patterns exhibited by Zygaenoidea moths?”.\nAs a labeler, I have no idea what “Zygaenoidea moths” are, let alone know their characterstics. I had to search for “Zygaenoidea moths” on google, and that linked me to scholarly articles.\nThrough this simple exercise, I hope I have showcased how difficult it can be to rate responses generated by the LLM."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-dataset-review",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-dataset-review",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "4 Dataset Review",
    "text": "4 Dataset Review\nAs part of Section 2 and Section 3, by calling the OpenaiAPI ourselves, we saw that there might be noise in the dataset.\ngpt-3.5-turbo fails to provide context in some of the instructions that we saw before like:\n\n“Research and compare different design bureaus to find one that aligns with your project goals”\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nThis means that there is possibility there is noise in the dataset. It is harder to look at text and figure out noise and clean datasets, IMHO, this is an open research question and I will try to work on this in my next blog post.\nAlso, from the simple exercise, we saw how hard it can be to label Instruction and Response. There is no direct way, if the labeler doesn’t have knowledge about the topic, then the task becomes even more intensive."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#model-training",
    "href": "posts/2023-04-30_LaMini-LM.html#model-training",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "5 Model Training",
    "text": "5 Model Training\nFrom the paper:\nWe present LaMini-LM, a family of language models instruction-tuned on our 2.58M instructions dataset \\(D_{ALL}\\). We train two types of models, encoder-decoder and decoder-only, for architectural comparison. The size for both categories of models ranges from 61M to 1.5B to facilitate size comparison. The underlying models for initialization are from five sources, including T5 (Raffel et al., 2020), Flan-T5 (Chung et al. (2022)), Cereberas-GPT (Dey et al. (2023)), GPT-2 (Radford et al., 2019), and GPT-Neo (Gao et al. (2020)).\n\n\n\n\n\n\nFigure 8: LaMini-LM collection.\n\n\n\nAlso, from the paper:\nWe finetune all models over 5 epochs and a batch size of 1024. For our encoder-decoder models, we use a learning rate of 5 × 10−4 following Chung et al. (2022). For our decoder-only models, we follow the same configuration as Alpaca (Taori et al., 2023) including the learning rate of 2 × 10−5. We use HuggingFace’s transformers for training. Moreover, we use the same prompt wrapper as Alpaca (Taori et al., 2023), hence we also wrap our instruction similarly during inference. We perform all of our experiments on 8×V100 (32G) and 8×A100 (40G) GPUs.\nAs part of this blog post, we will not be re-training the models, but you can see it is supervised finetuning on the Instruction Dataset using Transformers library."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-model-eval",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-model-eval",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "6 Model Evaluation",
    "text": "6 Model Evaluation\nThe authors have evaluated the performance of their trained models on several NLP tasks using model evaluation harness. (Gao et al. (2021))\nAs part of this blog post we will also be evluating the models using this framework.\nResults of model evaluation provided by the authors are shared in the table below. I have also shared the results from LLAMA.\n\n\n\n\n\n\nFigure 9: LaMini-LM collection.\n\n\n\nAs shared by anton, and as shown above, the results from LaMini-LM, don’t match LLAMA.\n\n\nWhat's even more interesting is the discrepancy of the numbers on LLaMA 7B vs their paper reported numbers… pic.twitter.com/12TBbuArLb\n\n— anton ((abacaj?)) April 28, 2023\n\n\nThe results shared in the original LLAMA paper are better compared to those shared in the LaMini-LM research paper. The most surprising is OpenBookQA, where in the LLAMA paper the reported accuracy is 57.2% compared to 42.4% in LaMini-LM.\nTo further analyse, let’s run evaluation on BoolQ, the results are reported in LLAMA, but not present in LaMini-LM.\nTo do this, let’s first install the library:\ngit clone https://github.com/EleutherAI/lm-evaluation-harness\ncd lm-evaluation-harness\npip install -e .\nNext, we could just simply run\npython main.py --model hf-causal --model_args pretrained=MBZUAI/LaMini-GPT-1.5B --tasks openbookqa,boolq,piqa --device cuda:0\nto evaluate the 1.5 GPT-2 on OpenBookQA (Mihaylov et al. (2018)), BoolQ (Clark et al. (2019)), PIQA (Bisk et al. (2019)).\n\n\n\nTable 1: Evaluation Results on BoolQ, PIQA, OpenBookQA\n\n\n\n\n\nTask\nVersion\nMetric\nValue\n\nStderr\n\n\n\n\nboolq\n1\nacc\n0.7725\n±\n0.0073\n\n\npiqa\n0\nacc\n0.7127\n±\n0.0106\n\n\n\n\nacc_norm\n0.7214\n±\n0.0105\n\n\nopenbookqa\n0\nacc\n0.2680\n±\n0.0198\n\n\n\n\nacc_norm\n0.3440\n±\n0.0213\n\n\n\n\n\n\nIt appears as though the results for LaMini-GPT are better than LLAMA. LLAMA’s 7B model is at 76.5% accuracy whereas LaMini-GPT is at 77.25% accuracy.\nAlso, our results on OpenBookQA don’t match those provided in the paper. The authors reported on acc_norm for OpenBookQA using a wrapper for decoder models. We get 34.4% compared to 39.8% reported in the paper.\nThis is because the the authors used a wrapper during inference, which I didn’t. The authors were really kind enough to respond to my query and also share the updated wrapper code.\n\n\nHere is our adapted lm-eval-harness code: https://t.co/fP2y0IcboQ\n\n— Chiyu Zhang ((ChiyuZhang0851?)) May 1, 2023\n\n\nBased on the above table, let’s re-run for other evaluation datasets too and see if our results match those from the paper.\npython main.py --model hf-causal --model_args pretrained=MBZUAI/LaMini-GPT-1.5B --tasks openbookqa,sciq,race,record,sst,mrpc,rte,wsc,winogrande --device cuda:0\n\n\n\nTable 2: Evaluation Results on MRPC, WinoGrande, WSC, RACE, SST, RTE, Record, SciQ\n\n\n\n\n\nTask\nVersion\nMetric\nValue\n\nStderr\n\n\n\n\nmrpc\n0\nacc\n0.7475\n±\n0.0215\n\n\n\n\nf1\n0.8352\n±\n0.0161\n\n\nwinogrande\n0\nacc\n0.5777\n±\n0.0139\n\n\nwsc\n0\nacc\n0.6635\n±\n0.0466\n\n\nrace\n1\nacc\n0.3742\n±\n0.0150\n\n\nsst\n0\nacc\n0.8933\n±\n0.0105\n\n\nrte\n0\nacc\n0.6354\n±\n0.0290\n\n\nrecord\n0\nf1\n0.8244\n±\n0.0038\n\n\n\n\nem\n0.8177\n±\n0.0039\n\n\nsciq\n0\nacc\n0.9100\n±\n0.0091\n\n\n\n\nacc_norm\n0.8790\n±\n0.0103\n\n\n\n\n\n\nAfter running more evaluation on these benchmarks shared in Table 2, looks like the results are different compared to the paper. This maybe due to the same reason as before.\n\n\n\n\n\n\nThank you again authors!\n\n\n\nThe authors have responded regarding the difference between their and original LLaMA benchmarking results. It might be due to difference in prompting. We will probably need to run our own benchmarking using 7B LLaMA model & lm-evaluation-harness.\n\n\n(abacaj?) (amaarora?) The LLaMA results use a different method, so a higher number there doesn’t necessarily mean better than ours. Therefore, the tables shouldn’t be compared. The differences may come from the different prompts they used. Here is the description in the LLaMa paper. pic.twitter.com/B8stCQwyPl\n\n— Chiyu Zhang ((ChiyuZhang0851?)) May 1, 2023\n\n\n\n\n\n6.1 Human Evaluation\nLastly, let’s look at the human evaluation bit. From the paper:\nTo complete the evaluation, we additionally evaluate the practicality of both our LaMini-LM and our baseline models by utilizing the user-oriented instructions from Wang et al. (2022a), which consists of 252 instructions covering 71 commonly used apps use-cases.\n\n\n\n\n\n\nImportant\n\n\n\nThe training set consists of 0.27M instructions+responses that have been generated using “example-guided” approach from Self-Instruction, and 0.28M instructions+responses that have been generated using “topic-guided” approach from Self-Instruction. Doesn’t that mean that the evaluation set is very similar to the training set here?\n\n\nAlso, would have been nice to know what these 252 Instructions look like. The authors have kindly provided the human evaluation results table which I share below in Figure 10, but not the evaluation instructions.\n\n\n\n\n\n\nFigure 10: LaMini-LM collection.\n\n\n\n\n\nCode\n# pip install -q transformers\nfrom transformers import pipeline\ncheckpoint = \"MBZUAI/LaMini-GPT-1.5B\" \nmodel = pipeline('text-generation', model = checkpoint, device='cuda:0')\n\ninstruction = 'Two large and 1 small pumps can fill a swimming pool in 4 hours. One large and 3 small pumps can also fill the same swimming pool in 4 hours. How many hours will it take 4 large and 4 small pumps to fill the swimming pool?'\ninput_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\n\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nResponse Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nTwo large and 1 small pumps can fill a swimming pool in 4 hours. One large and 3 small pumps can also fill the same swimming pool in 4 hours. How many hours will it take 4 large and 4 small pumps to fill the swimming pool?\n\n### Response:It will take 4 large and 4 small pumps (6 pumps total) 4 hours to fill the swimming pool.\n\n\n\nBy the way, ChatGPT nails it and returns the right answer “1 hour & 36 minutes” but it would be unfair to compare a 1.5B model with ChatGPT.\n\n\n\nCode\ninstruction = 'Today is 30 Apr, 2023. I want to participate in a marathon on July 30, 2023. Please create a training program for me. I can run 5kms easily as of now.'\ninput_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\n\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nResponse Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nToday is 30 Apr, 2023. I want to participate in a marathon on July 30, 2023. Please create a training program for me. I can run 5kms easily as of now.\n\n### Response:Understood. Training program created for participant. Training will be divided into four phases: \n1. Endurance training\n2. Strength and flexibility training \n3. Low-impact exercise (stretching, yoga, etc.) \n4. Functional training (running drills, pace training, etc.)\n\n\nThis is not a statisfactory and I would rate it Rate-C, “The response is relevant and responds to the instruction, but it has significant errors in the content.”\n\n\nCode\ninstruction = 'Write a product description for a sustainable, eco-friendly backpack made from recycled materials.'\ninput_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\n\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nResponse Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWrite a product description for a sustainable, eco-friendly backpack made from recycled materials.\n\n### Response:Sustainable, eco-friendly backpack made from recycled materials.\n\n\nThe result above looks unsatisfactory too."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#conclusion",
    "href": "posts/2023-04-30_LaMini-LM.html#conclusion",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nTo summarise,\n\nWe recreated a small sample of the dataset using example-guided and topic-guided approach as mentioned in the paper.\nWe used OpenaiAPI to also generate responses for the paper.\nWe replicated Figure 5 which showcases that diversity in Self-Instruct guided \\(\\hat{X}_{SI}\\) is actually more compared to Alpaca \\(\\hat{X}_A\\)\nThe authors were very kind enought to update the HF dataset and add a new column called instruction_source to match Figure 4.\nWe used ChatGPT to create a simple IpyWidget to rate the scores. Through this simple exercise we realised how hard it can be to score text based responses.\nWe ran our own evaluation for LaMini-LM (1.5B GPT) using lm-evaluation-harness by EleutherAI on several NLP datasets. The results were bit different compared to the table as in Table 2. As mentioned before, this is due to difference in prompting, the authors used a specific prompt for inference.\nWe also saw that LLaMA results from the original paper didn’t match those reported in LaMini-LM. The authors were kind enough to run LLaMA benchmarking again, and it led to the same results as in Figure 9. See this tweet for clarification."
  },
  {
    "objectID": "nbs/2024-07-02 Gemma.html",
    "href": "nbs/2024-07-02 Gemma.html",
    "title": "Gemma 2",
    "section": "",
    "text": "If you haven’t seen Gemma 2’s announcement yet or played around with the model, you are missing out. On my end, I am already experimenting with Gemma 2 for a product use case. And the the preliminary results seem to be better than Llama-3.\nWhy is Gemma 2 such a big deal? Let me answer it for you in 3 short bullet points:\n\nThe model comes in practical sizes 3B, 9B & 27B that can fit on a single GPU (at the time of writing this blog post, the 3B version is yet to be released)\nPerformance of the Gemma 2 models is on par with models twice or more it’s size! We talk more about this in the blog post.\nModel weights are open-source - thank you Google Deepmind!\n\nThis blog post is more like an annotated version of the paper. We go deep into all the tips and tricks that have been shared such as knowledge distillation, interleaved global & local attention layers, grouped query attention (GQA), sliding window size in attention layers and we do all of this in understable Pytorch code!\nSo, let’s get started.\nsubscribe.html"
  },
  {
    "objectID": "nbs/2024-07-02 Gemma.html#introduction",
    "href": "nbs/2024-07-02 Gemma.html#introduction",
    "title": "Gemma 2",
    "section": "",
    "text": "If you haven’t seen Gemma 2’s announcement yet or played around with the model, you are missing out. On my end, I am already experimenting with Gemma 2 for a product use case. And the the preliminary results seem to be better than Llama-3.\nWhy is Gemma 2 such a big deal? Let me answer it for you in 3 short bullet points:\n\nThe model comes in practical sizes 3B, 9B & 27B that can fit on a single GPU (at the time of writing this blog post, the 3B version is yet to be released)\nPerformance of the Gemma 2 models is on par with models twice or more it’s size! We talk more about this in the blog post.\nModel weights are open-source - thank you Google Deepmind!\n\nThis blog post is more like an annotated version of the paper. We go deep into all the tips and tricks that have been shared such as knowledge distillation, interleaved global & local attention layers, grouped query attention (GQA), sliding window size in attention layers and we do all of this in understable Pytorch code!\nSo, let’s get started."
  },
  {
    "objectID": "nbs/2024-07-02 Gemma.html#gemma-2-performance",
    "href": "nbs/2024-07-02 Gemma.html#gemma-2-performance",
    "title": "Gemma 2",
    "section": "2 Gemma 2 performance",
    "text": "2 Gemma 2 performance"
  },
  {
    "objectID": "nbs/2024-07-02 Gemma.html#group-query-attention",
    "href": "nbs/2024-07-02 Gemma.html#group-query-attention",
    "title": "Gemma 2",
    "section": "3 Group Query Attention",
    "text": "3 Group Query Attention\nThe authors used Grouped Query Attention instead of multi-head attention (MHA) or multi-query attention. In this part of the blog post, let me take you through the key difference between multi-head attention and grouped query attention, and let’s implement GQA in PyTorch code.\nFor an introduction and in-depth understand to multi-head attention, I would like to refer the reader to my prior blog on Vision Transformer where we implement attention from scratch in PyTorch in Section 8.\n\n\n\n\n\n\nFigure 1: Group Query Attention Overview\n\n\n\nRemember Multi Head Attention from the Attention is all you need paper? In every head we calculate attention using the formula:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nIn Grouped Query Attention, we reduce the number of keys and values (thus grouping heads together as shown in Figure 1). If the number of keys & value heads is reduced to 1, it is Multi-Query Attention. Thus Group Query Attention is somewhere in the middle between MHA & MQA. Let’s implement it in PyTorch.\n\n\n\n\n\n\nGQA implementation in PyTorch\n\n\n\nWe modify the implementation from Meta’s Llama-3 repo here. Basically, we removed rotary embeddings, KV caching, and model parallelization to keep the implementation simple and straightforward.\n\n\n\nimport torch\nimport torch.nn as nn\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport math\nimport torch.nn.functional as F\n\nHaving made the imports, let’s define the model arguments. We assume that the input and output dimensions inside the Decoder layer are \\(4096\\).\nBelow, the n_kv_heads defines the number of key & value heads. If the number is equal to 1, the below Attention implementation follows Multi-Query Attention. When the number is greater than 1 and less than n_heads, then we follow Group Query Attention as in Figure 1.\n\n@dataclass\nclass ModelArgs:\n    dim: int = 4096\n    n_layers: int = 32\n    n_heads: int = 32\n    n_kv_heads: Optional[int] = 8\n    vocab_size: int = -1  # defined later by tokenizer\n    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n    ffn_dim_multiplier: Optional[float] = None\n    norm_eps: float = 1e-5\n    max_batch_size: int = 32\n    max_seq_len: int = 2048\n\nFor our implementation, we assume 8 key & value heads whereas 32 query heads.\n\nargs = ModelArgs()\nargs\n\nModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=2048)\n\n\nBelow, follows a standard implementation of Attention (without positional embeddings to keep it simple). We have our weight matrices for q,k & v layers defined as Linear layers. These weight matrices transform an input tensor \\(X\\) to query \\(q\\), key \\(k\\) & value \\(v\\) respectively.\nTaking in an input of shape \\((2, 32, 4096)\\) which represents a batch of 2 sequences of length 32, each represented by a 4096 long vector.\nUpon taking the transform, given the weight matrices self.wq, self.wk & self.wv, the dimensions for our \\(q\\), \\(k\\) & \\(v\\) matrices will be:\n\\(q\\) \\(-&gt;\\) \\((2, 32, 4096)\\)\n\\(k\\) \\(-&gt;\\) \\((2, 32, 1024)\\)\n\\(v\\) \\(-&gt;\\) \\((2, 32, 1024)\\)\n\n\n\n\n\n\nTime to take a break and “think” 💭\n\n\n\nThis would be a great time for you to take a break and think about the dimensions. Can you reason in your head as to why \\(k\\) and \\(v\\) are of dimensions \\((2,32,1024)\\)?\nHint: We have fewer number of k,v heads by an order of magnitude of “4”.\n\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n        self.n_local_heads = args.n_heads\n        self.n_local_kv_heads = self.n_kv_heads\n        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n        self.head_dim = args.dim // args.n_heads\n        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False,)\n        self.wk = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False,)\n        self.wv = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False,)            \n        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False,)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n    ):\n        bsz, seqlen, _ = x.shape\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n\n        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n\n        # repeat k/v heads if n_kv_heads &lt; n_heads\n        xk = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n        xv = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n        xk = xk.transpose(1, 2)\n        xv = xv.transpose(1, 2)\n        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n        output = torch.matmul(scores, xv)  # (bs, n_local_heads, seqlen, head_dim)\n        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n        return self.wo(output)\n\nThe above implementation really follows Figure 1 very closely. First, we get the dimension per head self.head_dim, by simply doing args.dim // args.n_heads. Given the values, in this case, each head has a dimension of \\(128\\).\nNow, after the matrix multiplication with weight matrices, we do a reshape to get our \\(xq\\), \\(xk\\) & \\(xv\\) values.\nCan you think what their dimensions would be?\n\n\n\n\n\n\nTime to take a break and “think” 💭\n\n\n\nThe dimensions for \\(xq\\), \\(xk\\) & \\(xv\\) are \\([2, 32, 32, 128]\\), \\([2, 32, 8, 128]\\) & \\([2, 32, 32, 128]\\) respectively. Thereby, we are doing a “grouped” attention, because 4 queries get grouped to work a single key & value pair.\n\n\nIn practice, we just repeat the \\(k\\) & \\(v\\) values, in this case n_rep is 4 to get \\(k\\) and \\(v\\) to have tensors of shape \\([2, 32, 32, 128]\\). We do this using the repeat_kv function below.\n\ndef repeat_kv(x: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n    bs, slen, n_kv_heads, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    return (\n        x[:, :, :, None, :]\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n    )\n\nFor a detailed explanation of this repeat_kv function, refer here.\nAnd that’s really it. After that, we calculate our attention scores as usual, using the attention formula:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\n\nX = torch.randn(2, 32, 4096)\nattn = Attention(args)\nattn(X).shape\n\ntorch.Size([2, 32, 4096])\n\n\nAnd that’s really all the magic that there is behind Group Query Attention (GQA)! You have just succesfully implemented it from scratch using PyTorch yourself!"
  },
  {
    "objectID": "nbs/2024-07-02 Gemma.html#sliding-window-attention",
    "href": "nbs/2024-07-02 Gemma.html#sliding-window-attention",
    "title": "Gemma 2",
    "section": "4 Sliding Window Attention",
    "text": "4 Sliding Window Attention\nSliding Window Attention was introduced as part of the LongFormer architecture. It is a well known fact that transformers have a complexity of \\(O(n^2)\\).\n\n\n\n\n\n\nTime to take a break and “think” 💭\n\n\n\nCan you think of a reason why the complexity is \\(O(n^2)\\)? In simple language it means that computation grows by amount \\(n^2\\) for a sequence length of \\(n\\).\nThe reason for this is the multiplication of \\(Q\\) and \\(K^T\\), where both query \\(Q\\), and key \\(K\\) have length of \\(n\\).\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nTherefore, the multiplication at the top causes matrix multiplication complexity to grow by \\(n^2\\).\n\n\nNow, what if we want to work with long sequences? With the traditional transformer architecture this would lead to a high requirement of GPU memory (since tensor multiplication occurs on GPU) and thereby, limited sequence length.\nThe solution? 💡\nSliding window attention! The idea is to get Query tokens to not attend to all Key tokens but only neighbouring tokens. Sounds complex? It isn’t really. Let me explain with the help of PyTorch code first and then some diagrams.\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport torch\nfrom transformers import LongformerModel, AutoTokenizer\n\nLet’s first initialize a LongFormer architecture and assume a very long text as our input document.\n\nmodel = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\ntext = \" \".join([\"Hello world! \"] * 1000)  # long input document\ninput_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  #batch size of 1\ninput_ids.shape\n\ntorch.Size([1, 4002])\n\n\nAs per the documentation on HuggingFace:\nThe user can define which tokens attend “locally” and which tokens attend “globally” by setting the tensor global_attention_mask at run-time appropriately. All Longformer models employ the following logic for global_attention_mask:\n0: the token attends “locally”; 1: the token attends “globally”.\n\nglobal_attention_mask = torch.zeros(\n    input_ids.shape, dtype=torch.long, device=input_ids.device\n) \noutputs = model(input_ids, global_attention_mask=global_attention_mask)\noutputs.last_hidden_state.shape\n\nInput ids are automatically padded to be a multiple of `config.attention_window`: 512\n\n\ntorch.Size([1, 4002, 768])\n\n\nAnd that’s it. This is all it takes to implement the LongFormer architecture. Thank you HuggingFace for making is so easy for us. But, hey, the devil is mostly in the details. So is the fun. Let’s get our hands dirty and dig into code and also reference it with the LongFormer architecture to understand it’s implementation.\n\n\n\n\n\n\nNote\n\n\n\nFrom the documentation again:\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually represents the memory and time bottleneck, can be reduced from \\(O(n^2)\\) to \\(O(nxw)\\) with \\(n\\) being the sequence length and \\(𝑤\\) being the average window size.\n\nAA: Don’t worry if this doesn’t make much sense yet, continue reading and come back to this note again in a bit.\n\n\n\nFirst things first, the only thing we care about right now is LongFormerSelfAttention. That’s where all the magic happens. So, let’s unpack it. We assume input tensor of shape \\((1, 4096, 768)\\).\n\nfrom transformers.models.longformer import LongformerSelfAttention, LongformerConfig\n\nLet’s define the config.\n\ncfg = LongformerConfig()\ncfg.attention_window = [4] \ncfg.num_attention_heads=1\ncfg.hidden_size=1\ncfg\n\nLongformerConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"attention_window\": [\n    4\n  ],\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"longformer\",\n  \"num_attention_heads\": 1,\n  \"num_hidden_layers\": 12,\n  \"onnx_export\": false,\n  \"pad_token_id\": 1,\n  \"sep_token_id\": 2,\n  \"transformers_version\": \"4.43.0.dev0\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}\n\n\nAbove, we can see that attention_window is 512. That means, each token will attend to 256 tokens to its left and 256 tokens to its right. This is the “window length” \\(w\\) that we have referenced previously.\n\nx = torch.arange(8).unsqueeze(0).unsqueeze(-1)\nx = x.float()\nattn_mask = torch.zeros(1, 8)\nis_index_masked = attn_mask &lt; 0\nis_index_masked.shape\n\ntorch.Size([1, 8])\n\n\n\nattn = LongformerSelfAttention(cfg, layer_id=0)\nattn(x, attention_mask=attn_mask, is_index_masked=is_index_masked)[0].shape\n\n&gt; /home/ubuntu/GIT_REPOS/transformers/src/transformers/models/longformer/modeling_longformer.py(828)_sliding_chunks_query_key_matmul()\n    826 \n    827         import pdb; pdb.set_trace()\n--&gt; 828         query = self._chunk(query, window_overlap, getattr(self.config, \"onnx_export\", False))\n    829         key = self._chunk(key, window_overlap, getattr(self.config, \"onnx_export\", False))\n    830 \n\nipdb&gt; query\ntensor([[[0.0743],\n         [0.2809],\n         [0.4876],\n         [0.6942],\n         [0.9008],\n         [1.1075],\n         [1.3141],\n         [1.5207]]], grad_fn=&lt;ViewBackward0&gt;)\nipdb&gt; n\n&gt; /home/ubuntu/GIT_REPOS/transformers/src/transformers/models/longformer/modeling_longformer.py(829)_sliding_chunks_query_key_matmul()\n    827         import pdb; pdb.set_trace()\n    828         query = self._chunk(query, window_overlap, getattr(self.config, \"onnx_export\", False))\n--&gt; 829         key = self._chunk(key, window_overlap, getattr(self.config, \"onnx_export\", False))\n    830 \n    831         # matrix multiplication\n\nipdb&gt; query\ntensor([[[[0.0743],\n          [0.2809],\n          [0.4876],\n          [0.6942]],\n\n         [[0.4876],\n          [0.6942],\n          [0.9008],\n          [1.1075]],\n\n         [[0.9008],\n          [1.1075],\n          [1.3141],\n          [1.5207]]]], grad_fn=&lt;AsStridedBackward0&gt;)\nipdb&gt; query.shape\ntorch.Size([1, 3, 4, 1])\nipdb&gt; n\n&gt; /home/ubuntu/GIT_REPOS/transformers/src/transformers/models/longformer/modeling_longformer.py(835)_sliding_chunks_query_key_matmul()\n    833         # bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\n    834         # bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\n--&gt; 835         diagonal_chunked_attention_scores = torch.einsum(\"bcxd,bcyd-&gt;bcxy\", (query, key))  # multiply\n    836 \n    837         # convert diagonals into columns\n\nipdb&gt; key\ntensor([[[[0.1567],\n          [0.9540],\n          [1.7513],\n          [2.5485]],\n\n         [[1.7513],\n          [2.5485],\n          [3.3458],\n          [4.1431]],\n\n         [[3.3458],\n          [4.1431],\n          [4.9404],\n          [5.7376]]]], grad_fn=&lt;AsStridedBackward0&gt;)\nipdb&gt; key.shape\ntorch.Size([1, 3, 4, 1])\nipdb&gt; torch.einsum(\"bcxd,bcyd-&gt;bcxy\", (query, key))\ntensor([[[[0.0116, 0.0709, 0.1301, 0.1894],\n          [0.0440, 0.2680, 0.4920, 0.7160],\n          [0.0764, 0.4651, 0.8539, 1.2426],\n          [0.1088, 0.6623, 1.2158, 1.7692]],\n\n         [[0.8539, 1.2426, 1.6314, 2.0201],\n          [1.2158, 1.7692, 2.3227, 2.8762],\n          [1.5776, 2.2958, 3.0141, 3.7323],\n          [1.9395, 2.8225, 3.7054, 4.5884]],\n\n         [[3.0141, 3.7323, 4.4505, 5.1687],\n          [3.7054, 4.5884, 5.4714, 6.3543],\n          [4.3968, 5.4445, 6.4922, 7.5399],\n          [5.0881, 6.3006, 7.5131, 8.7255]]]], grad_fn=&lt;ViewBackward0&gt;)\nipdb&gt; query\ntensor([[[[0.0743],\n          [0.2809],\n          [0.4876],\n          [0.6942]],\n\n         [[0.4876],\n          [0.6942],\n          [0.9008],\n          [1.1075]],\n\n         [[0.9008],\n          [1.1075],\n          [1.3141],\n          [1.5207]]]], grad_fn=&lt;AsStridedBackward0&gt;)\nipdb&gt; query.shape\ntorch.Size([1, 3, 4, 1])\nipdb&gt; key.shape\ntorch.Size([1, 3, 4, 1])\nipdb&gt; torch.einsum(\"bcxd,bcyd-&gt;bcxy\", (query, key)).shape\ntorch.Size([1, 3, 4, 4])\nipdb&gt; torch.einsum(\"bcxd,bcyd-&gt;bcxy\", (query, key))\ntensor([[[[0.0116, 0.0709, 0.1301, 0.1894],\n          [0.0440, 0.2680, 0.4920, 0.7160],\n          [0.0764, 0.4651, 0.8539, 1.2426],\n          [0.1088, 0.6623, 1.2158, 1.7692]],\n\n         [[0.8539, 1.2426, 1.6314, 2.0201],\n          [1.2158, 1.7692, 2.3227, 2.8762],\n          [1.5776, 2.2958, 3.0141, 3.7323],\n          [1.9395, 2.8225, 3.7054, 4.5884]],\n\n         [[3.0141, 3.7323, 4.4505, 5.1687],\n          [3.7054, 4.5884, 5.4714, 6.3543],\n          [4.3968, 5.4445, 6.4922, 7.5399],\n          [5.0881, 6.3006, 7.5131, 8.7255]]]], grad_fn=&lt;ViewBackward0&gt;)\nipdb&gt; query\ntensor([[[[0.0743],\n          [0.2809],\n          [0.4876],\n          [0.6942]],\n\n         [[0.4876],\n          [0.6942],\n          [0.9008],\n          [1.1075]],\n\n         [[0.9008],\n          [1.1075],\n          [1.3141],\n          [1.5207]]]], grad_fn=&lt;AsStridedBackward0&gt;)\nipdb&gt; key\ntensor([[[[0.1567],\n          [0.9540],\n          [1.7513],\n          [2.5485]],\n\n         [[1.7513],\n          [2.5485],\n          [3.3458],\n          [4.1431]],\n\n         [[3.3458],\n          [4.1431],\n          [4.9404],\n          [5.7376]]]], grad_fn=&lt;AsStridedBackward0&gt;)"
  },
  {
    "objectID": "nbs/2024-07-02 Gemma.html#creditskudos",
    "href": "nbs/2024-07-02 Gemma.html#creditskudos",
    "title": "Gemma 2",
    "section": "5 Credits/Kudos",
    "text": "5 Credits/Kudos\n\nimport torch\nfrom transformers import LongformerModel, AutoTokenizer\n\nmodel = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\ntext = \" \".join([\"Hello world! \"] * 1000)  # long input document\ninput_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  #batch size of 1\ninput_ids.shape\n\nglobal_attention_mask = torch.zeros(\n    input_ids.shape, dtype=torch.long, device=input_ids.device\n) \noutputs = model(input_ids, global_attention_mask=global_attention_mask)\noutputs.last_hidden_state.shape\n\n\nThanks to bylidiz on GitHub for posting this great explanation on Group Query Attention."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html",
    "href": "posts/2024-07-07 Gemma.html",
    "title": "Gemma 2",
    "section": "",
    "text": "I am sure by now you would have seen Gemma 2’s announcement or played around with the model. If you haven’t yet, I highly recommend that you do.\nGoing by the benchmarks shared in the official Gemma 2 report, the model is extremely competitive and outperforming other models relative to it’s size.\nAlso, as can be seen in Figure 2, the results from Chatbot arena for Gemma 2 models look pretty impressive, given the model sizes.\nBelow, I try to summarise “Why is Gemma 2 such a big deal?”:\nAs part of this blog post, we will be going deeper into some of the architectural components of Gemma 2 along with their implementation in PyTorch. Specifically we will be looking into:"
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#introduction",
    "href": "posts/2024-07-07 Gemma.html#introduction",
    "title": "Gemma 2",
    "section": "",
    "text": "I am sure by now you would have seen Gemma 2’s announcement or played around with the model. If you haven’t yet, I highly recommend that you do.\n\n\n\n\n\n\nFigure 1: Comparison of Gemma 2 models on a variety of benchmarks\n\n\n\nGoing by the benchmarks shared in the official Gemma 2 report, the model is extremely competitive and outperforming other models relative to it’s size.\n\n\n\n\n\n\nFigure 2: Evaluation of Gemma 2 9B and 27B Instruction Tuned models on the Chatbot Arena\n\n\n\nAlso, as can be seen in Figure 2, the results from Chatbot arena for Gemma 2 models look pretty impressive, given the model sizes.\nBelow, I try to summarise “Why is Gemma 2 such a big deal?”:\n\nThe model comes in practical sizes 3B, 9B & 27B that can fit on a single GPU (at the time of writing this blog post, the 3B version is yet to be released)\nPerformance of the Gemma 2 models is on par with models twice or more it’s size!\nModel weights are open-source - thank you Google Deepmind!\n\nAs part of this blog post, we will be going deeper into some of the architectural components of Gemma 2 along with their implementation in PyTorch. Specifically we will be looking into:\n\nGrouped Query Attention (Section 3)\nSliding Window Attention (Section 4)\nRotary Position Embeddings (RoPE) (Section 5)\nLogit soft-capping (Section 6)\nModel merging (Section 7)\n\n\n\n\n\n\n\nGentle introduction to Gemma 2\n\n\n\nFor a more gentle introduction, I would like to refer the readers to Welcome Gemma 2 - Google’s new open LLM by Huggingface."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#gemma-2",
    "href": "posts/2024-07-07 Gemma.html#gemma-2",
    "title": "Gemma 2",
    "section": "2 Gemma 2",
    "text": "2 Gemma 2\nIn this section, we look into the architecture details as shared in Gemma 2: Improving Open Language Models at a Practical Size.\nFrom the report:\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. The 9 billion and 27 billion parameter models are available today, with a 2 billion parameter model to be released shortly. In this new version, we provide several technical modifications to our architecture, such as interleaving local-global attentions (Beltagy, Peters, and Cohan (2020)) and group-query attention (Ainslie et al. (2023)). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger.\n\n\n\n\n\n\nNote\n\n\n\nI guess the key point that me hooked to Gemma 2 was the last line shared in the abstract:\n\nThe resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger\n\nThis is pretty big news, and very important for the projects that I have been recently working on. Smaller models in productions means - lower latency, lower memory requirements, faster runtime, thus, an overall reduction in computing costs.\n\n\nThe recent large language models AI@Meta (2024), have been known to have dataset sizes as big as 15T tokens! It is the longer training on bigger datasets that has been key towards LLMs having continued improvements in performance. The models are trained to predict the next tokens in a left-to-right manner.\n\nAI@Meta. 2024. “Llama 3 Model Card.” https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.\nIn Gemma 2, the authors trained the smaller 2.6B and 9B models using knowledge distillation. This alongside other architecture details, has allowed Gemma 2 to have the best in class performance given it’s size.\nAs per the Gemma 2 report, the authors utilised Group query attention, Sliding window attention, RoPE, knowledge distillation & logit soft capping!\nAll of these concepts have been introduced in the amazing Welcome Gemma 2 - Google’s new open LLM by Huggingface. I would like to refer the readers to this blog post for a gentle introduction.\nAs part of this blog post we will go into the details of Grouped Query Attention in Section 3, Sliding Window Attention in ?@sec-swa & also RoPE embeddings in ?@sec-rope, along with their implementations in PyTorch! So, let’s get started."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#sec-gqa",
    "href": "posts/2024-07-07 Gemma.html#sec-gqa",
    "title": "Gemma 2",
    "section": "2 Group Query Attention",
    "text": "2 Group Query Attention\nGrouped query attention was introduced by Ainslie et al. (2023) in 2023. The key difference as compared to the standard Multi-headed attention has been highlighted in Figure 3.\n\nAinslie, Joshua, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.” https://arxiv.org/abs/2305.13245.\nIn this part of the blog post, we understand more about Group Query Attention and implement in in PyTorch code from scratch.\nFor an introduction and in-depth understand to multi-head attention, I would like to refer the reader to my previous blog post on Vision Transformer where we implement attention from scratch in PyTorch in Section 8.\n\n\n\n\n\n\nFigure 3: Group Query Attention Overview\n\n\n\nFrom the Attention is all you need paper, attention mechanism was introduced using the formula:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nIn Grouped Query Attention, we reduce the number of key and value heads (thus, in a way, grouping heads together as shown in Figure 3). If the number of keys & value heads is reduced to 1, it is equivalent to Multi-Query Attention Shazeer (2019).\n\nShazeer, Noam. 2019. “Fast Transformer Decoding: One Write-Head Is All You Need.” https://arxiv.org/abs/1911.02150.\nThus, Group Query Attention (GQA) is somewhere in the middle between MHA & MQA. Let’s now implement it in PyTorch.\n\n\n\n\n\n\nGQA implementation in PyTorch\n\n\n\nWe modify the implementation from Meta’s Llama-3 repo here. Basically, we removed rotary embeddings, KV caching, and model parallelization to keep the implementation to a bare minimum.\n\n\n\nimport torch\nimport torch.nn as nn\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport math\nimport torch.nn.functional as F\n\nHaving made the imports, let’s define the model arguments. We assume that the input and output dimensions inside the Decoder layer are \\(4096\\).\nBelow, the n_kv_heads defines the number of key & value heads. If the number is equal to 1, the below Attention implementation follows Multi-Query Attention. When the number is greater than 1 and less than n_heads, then we follow Group Query Attention as in Figure 3.\n\n@dataclass\nclass ModelArgs:\n    dim: int = 4096\n    n_layers: int = 32\n    n_heads: int = 32\n    n_kv_heads: Optional[int] = 8\n    vocab_size: int = -1  # defined later by tokenizer\n    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n    ffn_dim_multiplier: Optional[float] = None\n    norm_eps: float = 1e-5\n    max_batch_size: int = 32\n    max_seq_len: int = 2048\n\nFor our implementation, we assume 8 key & value heads whereas 32 query heads.\n\nargs = ModelArgs()\nargs\n\nModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=2048)\n\n\nBelow, follows a standard implementation of Attention (without positional embeddings to keep it simple). We have our weight matrices for q,k & v layers defined as Linear layers. These weight matrices transform an input tensor \\(X\\) to query \\(q\\), key \\(k\\) & value \\(v\\) respectively.\nTaking in an input of shape \\((2, 32, 4096)\\) which represents a batch of 2 sequences of length 32, each represented by a 4096 long vector.\nUpon taking the transform, given the weight matrices self.wq, self.wk & self.wv, the dimensions for our \\(q\\), \\(k\\) & \\(v\\) matrices will be:\n\\(q\\) \\(-&gt;\\) \\((2, 32, 4096)\\)\n\\(k\\) \\(-&gt;\\) \\((2, 32, 1024)\\)\n\\(v\\) \\(-&gt;\\) \\((2, 32, 1024)\\)\n\n\n\n\n\n\nTime to take a break and “think” 💭\n\n\n\nThis would be a great time for you to take a break and think about the dimensions. Can you reason in your head as to why \\(k\\) and \\(v\\) are of dimensions \\((2,32,1024)\\)?\nHint: We have fewer number of k,v heads by an order of magnitude of “4”.\n\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n        self.n_local_heads = args.n_heads\n        self.n_local_kv_heads = self.n_kv_heads\n        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n        self.head_dim = args.dim // args.n_heads\n        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False,)\n        self.wk = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False,)\n        self.wv = nn.Linear(args.dim, args.n_kv_heads * self.head_dim, bias=False,)            \n        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False,)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n    ):\n        bsz, seqlen, _ = x.shape\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n\n        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n\n        # repeat k/v heads if n_kv_heads &lt; n_heads\n        xk = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n        xv = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n        xk = xk.transpose(1, 2)\n        xv = xv.transpose(1, 2)\n        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n        output = torch.matmul(scores, xv)  # (bs, n_local_heads, seqlen, head_dim)\n        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n        return self.wo(output)\n\nThe above implementation really follows Figure 3 very closely. First, we get the dimension per head self.head_dim, by simply doing args.dim // args.n_heads. Given the values, in this case, each head has a dimension of \\(128\\).\nNow, after the matrix multiplication with weight matrices, we do a reshape to get our \\(xq\\), \\(xk\\) & \\(xv\\) values.\nCan you think what their dimensions would be?\n\n\n\n\n\n\nTime to take a break and “think” 💭\n\n\n\nThe dimensions for \\(xq\\), \\(xk\\) & \\(xv\\) are \\([2, 32, 32, 128]\\), \\([2, 32, 8, 128]\\) & \\([2, 32, 32, 128]\\) respectively. Thereby, we are doing a “grouped” attention, because 4 queries get grouped to work a single key & value pair.\n\n\nIn practice, we just repeat the \\(k\\) & \\(v\\) values, in this case n_rep is 4 to get \\(k\\) and \\(v\\) to have tensors of shape \\([2, 32, 32, 128]\\). We do this using the repeat_kv function below.\n\ndef repeat_kv(x: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n    bs, slen, n_kv_heads, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    return (\n        x[:, :, :, None, :]\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n    )\n\nFor a detailed explanation of this repeat_kv function, refer here.\nAnd that’s really it. After that, we calculate our attention scores as usual, using the attention formula:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\n\nX = torch.randn(2, 32, 4096)\nattn = Attention(args)\nattn(X).shape\n\ntorch.Size([2, 32, 4096])\n\n\nAnd that’s really all the magic that there is behind Group Query Attention (GQA)! You have just succesfully implemented it from scratch using PyTorch yourself!"
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#sliding-window-attention",
    "href": "posts/2024-07-07 Gemma.html#sliding-window-attention",
    "title": "Gemma 2",
    "section": "4 Sliding Window Attention",
    "text": "4 Sliding Window Attention\nFor a detailed explanation & implementation in PyTorch of Sliding Window Attention Beltagy, Peters, and Cohan (2020), I would like to refer the readers to my previous blog post.\n\nBeltagy, Iz, Matthew E. Peters, and Arman Cohan. 2020. “Longformer: The Long-Document Transformer.” https://arxiv.org/abs/2004.05150."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#rotary-positional-embeddings-rope",
    "href": "posts/2024-07-07 Gemma.html#rotary-positional-embeddings-rope",
    "title": "Gemma 2",
    "section": "5 Rotary Positional Embeddings (RoPE)",
    "text": "5 Rotary Positional Embeddings (RoPE)\nRoPE were introduced as part of the RoFormer architecture Su et al. (2023). From the paper itself:\n\nSu, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” https://arxiv.org/abs/2104.09864.\nThe proposed Rotary Position Embedding (RoPE) encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. We evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives.\nThe Roformer has been integrated in the transformers library and can be used like so:\nfrom transformers import AutoTokenizer, RoFormerModel\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\nmodel = RoFormerModel.from_pretrained(\"junnyu/roformer_chinese_base\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nmodel(**inputs)\n\nimport torch.nn as nn \nimport torch\nfrom typing import Optional\nimport numpy as np\n\nThe below are the Positional Encodings from the Attention Is All You Need by Vaswani et al. (2017) paper:\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” CoRR abs/1706.03762. http://arxiv.org/abs/1706.03762.\n\\[\nPE_{(pos, 2i)} = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n\\]\n\\[\nPE_{(pos, 2i+1)} = \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n\\]\nThey work with absolute positions, but, not with relative positions. From Huggingface’s implementation of the RoFormer architecture, this is how one could implement them in PyTorch code:\n\nclass RoFormerSinusoidalPositionalEmbedding(nn.Embedding):\n    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n\n    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -&gt; None:\n        super().__init__(num_positions, embedding_dim)\n        self.weight = self._init_weight(self.weight)\n\n    @staticmethod\n    def _init_weight(out: nn.Parameter) -&gt; nn.Parameter:\n        \"\"\"\n        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n        the 2nd half of the vector. [dim // 2:]\n        \"\"\"\n        n_pos, dim = out.shape\n        position_enc = np.array(\n            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n        )\n        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n        out.detach_()\n        return out\n\n    @torch.no_grad()\n    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -&gt; torch.Tensor:\n        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_ids_shape[:2]\n        positions = torch.arange(\n            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n        )\n        return super().forward(positions)\n\nI don’t go much into the detail of the implementation of RoFormerSinusoidalPositionalEmbedding, since it is pretty self-explanatory when we compare the implementation with the formula.\n\nI would like to refer the readers to The Annotated Transformer for another resource on positional encodings.\n\nThere are some challenges to using absolute position encodings as above that I highlight below:\n\nThe self-attention architecture has shown to be position agnostic. Thus, by adding positional information to the context representation, it renders them unsuitable for the linear self-attention architecture. Yun et al. (2020)\nThese encodings do-not follow the intuition that tokens close to each other should have more importance compared to tokens further away from each other.\nThe sequences at test-time might be of different length to trainining-time, thus, leading to train-test discrepency.\n\n\nYun, Chulhee, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. 2020. “Are Transformers Universal Approximators of Sequence-to-Sequence Functions?” https://arxiv.org/abs/1912.10077.\nThus, there is a need for positional encodings that overcome the above two challenges. From the RoPE paper:\nWe introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. Specifically, RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Note that the proposed RoPE is prioritized over the existing methods through valuable properties, including the sequence length flexibility, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding.\nBy utilising a derived rotation matrix, through RoPE, the authors were able to overcome the challenges and come up with a solution that not only solves the problem in theory but these embeddings are also easy to implement in practice! Thus, the widespread use of RoPE throughout multiple LLMs.\n\nIn this blog post, we do not go into the derivation of RoPE. I would like the readers to refer to another wonderful blog post by Eleuther AI that goes into the mathematical details - Rotary Embeddings: A Relative Revolution.\n\nRotary Position Embeddings can be implemented easily using the following matrix multiplication, where\n\\(x_{i}\\): contextual representation of token \\(x\\) at position \\(i\\). (nn.Embedding)\n\\[\nR_{\\Theta,m}^d x = \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\n\\vdots \\\\\nx_{d-1} \\\\\nx_d\n\\end{pmatrix} \\otimes \\begin{pmatrix}\n\\cos m\\theta_1 \\\\\n\\cos m\\theta_1 \\\\\n\\cos m\\theta_2 \\\\\n\\cos m\\theta_2 \\\\\n\\vdots \\\\\n\\cos m\\theta_{d/2} \\\\\n\\cos m\\theta_{d/2}\n\\end{pmatrix} + \\begin{pmatrix}\n-x_2 \\\\\nx_1 \\\\\n-x_4 \\\\\nx_3 \\\\\n\\vdots \\\\\n-x_d \\\\\nx_{d-1}\n\\end{pmatrix} \\otimes \\begin{pmatrix}\n\\sin m\\theta_1 \\\\\n\\sin m\\theta_1 \\\\\n\\sin m\\theta_2 \\\\\n\\sin m\\theta_2 \\\\\n\\vdots \\\\\n\\sin m\\theta_{d/2} \\\\\n\\sin m\\theta_{d/2}\n\\end{pmatrix}\n\\tag{1}\\]\nWe can get the sinusoidal and cosine values of the matrix multiplication from RoFormerSinusoidalPositionalEmbedding.\n\nembedding_layer = RoFormerSinusoidalPositionalEmbedding(100, 64)\nsinusoidal_pos = embedding_layer([1, 9])[None, None, :, :]\nsinusoidal_pos.shape\n\ntorch.Size([1, 1, 9, 64])\n\n\nAssuming 12 attention heads, each with a dimension of 64, we can randomly initialise our query and key layer like so:\n\nquery_layer = torch.randn(1, 12, 9, 64)\nkey_layer   = torch.randn(1, 12, 9, 64)\nquery_layer.shape, key_layer.shape\n\n(torch.Size([1, 12, 9, 64]), torch.Size([1, 12, 9, 64]))\n\n\n\ndef apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer=None):\n    # https://kexue.fm/archives/8265\n    # sin [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n    # cos [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n    sin, cos = sinusoidal_pos.chunk(2, dim=-1)\n    # sin [θ0,θ1,θ2......θd/2-1] -&gt; sin_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]\n    sin_pos = torch.stack([sin, sin], dim=-1).reshape_as(sinusoidal_pos)\n    # cos [θ0,θ1,θ2......θd/2-1] -&gt; cos_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]\n    cos_pos = torch.stack([cos, cos], dim=-1).reshape_as(sinusoidal_pos)\n    # rotate_half_query_layer [-q1,q0,-q3,q2......,-qd-1,qd-2]\n    rotate_half_query_layer = torch.stack([-query_layer[..., 1::2], query_layer[..., ::2]], dim=-1).reshape_as(\n        query_layer\n    )\n    query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos\n    # rotate_half_key_layer [-k1,k0,-k3,k2......,-kd-1,kd-2]\n    rotate_half_key_layer = torch.stack([-key_layer[..., 1::2], key_layer[..., ::2]], dim=-1).reshape_as(key_layer)\n    key_layer = key_layer * cos_pos + rotate_half_key_layer * sin_pos\n    if value_layer is not None:\n        # rotate_half_value_layer [-v1,v0,-v3,v2......,-vd-1,vd-2]\n        rotate_half_value_layer = torch.stack([-value_layer[..., 1::2], value_layer[..., ::2]], dim=-1).reshape_as(\n            value_layer\n        )\n        value_layer = value_layer * cos_pos + rotate_half_value_layer * sin_pos\n        return query_layer, key_layer, value_layer\n    return query_layer, key_layer\n\nNow, one could simply apply the rotary position embeddings using the above function apply_rotary_position_embeddings. Note that rotate_half_query_layer is just the following matrix:\n\\[\\begin{pmatrix}\n-x_2 \\\\\nx_1 \\\\\n-x_4 \\\\\nx_3 \\\\\n\\vdots \\\\\n-x_d \\\\\nx_{d-1}\n\\end{pmatrix}\\]\nFinally, by doing query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos, we are replicating the matrix multiplication as in Equation 1.\n\nquery_layer, key_layer = apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer)\nquery_layer.shape, key_layer.shape\n\n(torch.Size([1, 12, 9, 64]), torch.Size([1, 12, 9, 64]))\n\n\nAnd that is all that there is to Rotary Position Embeddings. We have successfully re-implemented RoPE in PyTorch."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#gemma-2-report",
    "href": "posts/2024-07-07 Gemma.html#gemma-2-report",
    "title": "Gemma 2",
    "section": "1 Gemma 2 Report",
    "text": "1 Gemma 2 Report\nIn this section, we look into the architecture details as shared in the report - Gemma 2: Improving Open Language Models at a Practical Size.\nFrom the report:\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. The 9 billion and 27 billion parameter models are available today, with a 2 billion parameter model to be released shortly. In this new version, we provide several technical modifications to our architecture, such as interleaving local-global attentions (Beltagy, Peters, and Cohan (2020)) and group-query attention (Ainslie et al. (2023)). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger.\n\n\n\n\n\n\nNote\n\n\n\nI guess the key point that me hooked to Gemma 2 was the last line shared in the abstract:\nThe resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger.\nThis is pretty big news, and very important for the projects that I have been recently working on. Smaller models in productions means - lower latency, lower memory requirements, faster runtime, thus, an overall reduction in computing costs.\n\n\nThe recent large language models AI@Meta (2024), have been known to have dataset sizes as big as 15T tokens! It is the longer training on bigger datasets that has been key towards LLMs having continued improvements in performance. The models are trained to predict the next tokens in a left-to-right manner.\n\nAI@Meta. 2024. “Llama 3 Model Card.” https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.\nIn Gemma 2, the authors trained the smaller 2.6B and 9B models using knowledge distillation. This, alongside other architecture details, has allowed Gemma 2 to have the best in class performance given it’s size. Let’s look into each one of the components in the following sections."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#conclusion",
    "href": "posts/2024-07-07 Gemma.html#conclusion",
    "title": "Gemma 2",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nAs part of the following blog post we took a deep dive into grouped query qttention, sliding window attention, RoPE embeddings, logits soft-capping & also model-merging.\nWe did it all with the motivation from Gemma 2. The idea was to dig deeper into the Gemma 2 architecture. I hope that through this blog post, the reader is able to understand more about the Gemma 2 architecture in detail.\nThank you for your time!"
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#sec-swa",
    "href": "posts/2024-07-07 Gemma.html#sec-swa",
    "title": "Gemma 2",
    "section": "3 Sliding Window Attention",
    "text": "3 Sliding Window Attention\nFor a detailed explanation & implementation in PyTorch of Sliding Window Attention Beltagy, Peters, and Cohan (2020), I would like to refer the readers to my previous blog post.\n\nBeltagy, Iz, Matthew E. Peters, and Arman Cohan. 2020. “Longformer: The Long-Document Transformer.” https://arxiv.org/abs/2004.05150.\nThe authors interleaved local and global attentions in alternating layers, which helped reduce number of parameters (for compact model-size) while mantaining performance. This is pretty unique! From the paper:\nThe sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#sec-rope",
    "href": "posts/2024-07-07 Gemma.html#sec-rope",
    "title": "Gemma 2",
    "section": "4 Rotary Positional Embeddings (RoPE)",
    "text": "4 Rotary Positional Embeddings (RoPE)\nRoPE were introduced as part of the RoFormer architecture Su et al. (2023). From the paper itself:\n\nSu, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” https://arxiv.org/abs/2104.09864.\nThe proposed Rotary Position Embedding (RoPE) encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. We evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives.\nThe Roformer has been integrated in the transformers library and can be used like so:\nfrom transformers import AutoTokenizer, RoFormerModel\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\nmodel = RoFormerModel.from_pretrained(\"junnyu/roformer_chinese_base\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nmodel(**inputs)\n\nimport torch.nn as nn \nimport torch\nfrom typing import Optional\nimport numpy as np\n\nThe below are the Positional Encodings from the Attention Is All You Need by Vaswani et al. (2017) paper:\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” CoRR abs/1706.03762. http://arxiv.org/abs/1706.03762.\n\\[\nPE_{(pos, 2i)} = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n\\]\n\\[\nPE_{(pos, 2i+1)} = \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n\\]\nThey work with absolute positions, but, not with relative positions. From Huggingface’s implementation of the RoFormer architecture, this is how one could implement them in PyTorch code:\n\nclass RoFormerSinusoidalPositionalEmbedding(nn.Embedding):\n    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n\n    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -&gt; None:\n        super().__init__(num_positions, embedding_dim)\n        self.weight = self._init_weight(self.weight)\n\n    @staticmethod\n    def _init_weight(out: nn.Parameter) -&gt; nn.Parameter:\n        \"\"\"\n        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n        the 2nd half of the vector. [dim // 2:]\n        \"\"\"\n        n_pos, dim = out.shape\n        position_enc = np.array(\n            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n        )\n        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n        out.detach_()\n        return out\n\n    @torch.no_grad()\n    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -&gt; torch.Tensor:\n        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_ids_shape[:2]\n        positions = torch.arange(\n            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n        )\n        return super().forward(positions)\n\nI don’t go much into the detail of the implementation of RoFormerSinusoidalPositionalEmbedding, since it is pretty self-explanatory when we compare the implementation with the formula.\n\nI would like to refer the readers to The Annotated Transformer for another resource on positional encodings.\n\nThere are some challenges to using absolute position encodings as above that I highlight below:\n\nThe self-attention architecture has shown to be position agnostic. Thus, by adding positional information to the context representation, it renders them unsuitable for the linear self-attention architecture. Yun et al. (2020)\nThese encodings do-not follow the intuition that tokens close to each other should have more importance compared to tokens further away from each other.\nThe sequences at test-time might be of different length to trainining-time, thus, leading to train-test discrepency.\n\n\nYun, Chulhee, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. 2020. “Are Transformers Universal Approximators of Sequence-to-Sequence Functions?” https://arxiv.org/abs/1912.10077.\nThus, there is a need for positional encodings that overcome the above two challenges. From the RoPE paper:\nWe introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. Specifically, RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Note that the proposed RoPE is prioritized over the existing methods through valuable properties, including the sequence length flexibility, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding.\nBy utilising a derived rotation matrix, through RoPE, the authors were able to overcome the challenges and come up with a solution that not only solves the problem in theory but these embeddings are also easy to implement in practice! Thus, the widespread use of RoPE throughout multiple LLMs.\n\nIn this blog post, we do not go into the derivation of RoPE. I would like the readers to refer to another wonderful blog post by Eleuther AI that goes into the mathematical details - Rotary Embeddings: A Relative Revolution.\n\nRotary Position Embeddings can be implemented easily using the following matrix multiplication, where\n\\(x_{i}\\): contextual representation of token \\(x\\) at position \\(i\\). (nn.Embedding)\n\\[\nR_{\\Theta,m}^d x = \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\n\\vdots \\\\\nx_{d-1} \\\\\nx_d\n\\end{pmatrix} \\otimes \\begin{pmatrix}\n\\cos m\\theta_1 \\\\\n\\cos m\\theta_1 \\\\\n\\cos m\\theta_2 \\\\\n\\cos m\\theta_2 \\\\\n\\vdots \\\\\n\\cos m\\theta_{d/2} \\\\\n\\cos m\\theta_{d/2}\n\\end{pmatrix} + \\begin{pmatrix}\n-x_2 \\\\\nx_1 \\\\\n-x_4 \\\\\nx_3 \\\\\n\\vdots \\\\\n-x_d \\\\\nx_{d-1}\n\\end{pmatrix} \\otimes \\begin{pmatrix}\n\\sin m\\theta_1 \\\\\n\\sin m\\theta_1 \\\\\n\\sin m\\theta_2 \\\\\n\\sin m\\theta_2 \\\\\n\\vdots \\\\\n\\sin m\\theta_{d/2} \\\\\n\\sin m\\theta_{d/2}\n\\end{pmatrix}\n\\tag{1}\\]\nWe can get the sinusoidal and cosine values of the matrix multiplication from RoFormerSinusoidalPositionalEmbedding.\n\nembedding_layer = RoFormerSinusoidalPositionalEmbedding(100, 64)\nsinusoidal_pos = embedding_layer([1, 9])[None, None, :, :]\nsinusoidal_pos.shape\n\ntorch.Size([1, 1, 9, 64])\n\n\nAssuming 12 attention heads, each with a dimension of 64, we can randomly initialise our query and key layer like so:\n\nquery_layer = torch.randn(1, 12, 9, 64)\nkey_layer   = torch.randn(1, 12, 9, 64)\nquery_layer.shape, key_layer.shape\n\n(torch.Size([1, 12, 9, 64]), torch.Size([1, 12, 9, 64]))\n\n\n\ndef apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer=None):\n    # https://kexue.fm/archives/8265\n    # sin [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n    # cos [batch_size, num_heads, sequence_length, embed_size_per_head//2]\n    sin, cos = sinusoidal_pos.chunk(2, dim=-1)\n    # sin [θ0,θ1,θ2......θd/2-1] -&gt; sin_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]\n    sin_pos = torch.stack([sin, sin], dim=-1).reshape_as(sinusoidal_pos)\n    # cos [θ0,θ1,θ2......θd/2-1] -&gt; cos_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]\n    cos_pos = torch.stack([cos, cos], dim=-1).reshape_as(sinusoidal_pos)\n    # rotate_half_query_layer [-q1,q0,-q3,q2......,-qd-1,qd-2]\n    rotate_half_query_layer = torch.stack([-query_layer[..., 1::2], query_layer[..., ::2]], dim=-1).reshape_as(\n        query_layer\n    )\n    query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos\n    # rotate_half_key_layer [-k1,k0,-k3,k2......,-kd-1,kd-2]\n    rotate_half_key_layer = torch.stack([-key_layer[..., 1::2], key_layer[..., ::2]], dim=-1).reshape_as(key_layer)\n    key_layer = key_layer * cos_pos + rotate_half_key_layer * sin_pos\n    if value_layer is not None:\n        # rotate_half_value_layer [-v1,v0,-v3,v2......,-vd-1,vd-2]\n        rotate_half_value_layer = torch.stack([-value_layer[..., 1::2], value_layer[..., ::2]], dim=-1).reshape_as(\n            value_layer\n        )\n        value_layer = value_layer * cos_pos + rotate_half_value_layer * sin_pos\n        return query_layer, key_layer, value_layer\n    return query_layer, key_layer\n\nNow, one could simply apply the rotary position embeddings using the above function apply_rotary_position_embeddings. Note that rotate_half_query_layer is just the following matrix:\n\\[\\begin{pmatrix}\n-x_2 \\\\\nx_1 \\\\\n-x_4 \\\\\nx_3 \\\\\n\\vdots \\\\\n-x_d \\\\\nx_{d-1}\n\\end{pmatrix}\\]\nFinally, by doing query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos, we are replicating the matrix multiplication as in Equation 1.\n\nquery_layer, key_layer = apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer)\nquery_layer.shape, key_layer.shape\n\n(torch.Size([1, 12, 9, 64]), torch.Size([1, 12, 9, 64]))\n\n\nAnd that is all that there is to Rotary Position Embeddings. We have successfully re-implemented RoPE in PyTorch."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#sec-logits",
    "href": "posts/2024-07-07 Gemma.html#sec-logits",
    "title": "Gemma 2",
    "section": "5 Logit soft-capping",
    "text": "5 Logit soft-capping\nAnother trick that was used by the authors of Gemma 2 was logit soft capping. Generally we use torch.clip or torch.clamp which is more like hard clipping. Instead the authors utilised soft-capping which can be formulated as:\n\\[\\text{logits} \\leftarrow \\text{soft\\_cap} * \\tanh\\left(\\frac{\\text{logits}}{\\text{soft\\_cap}}\\right)\\]\nLet’s have a look at the tanh function and plot it using matplotlib.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfig,ax = plt.subplots(figsize=(6,3))\n\ndef plot_tanh():\n    x = np.linspace(-5, 5, 400)\n    y = np.tanh(x)\n    plt.plot(x, y)\n    plt.title('Tanh Function')\n    plt.xlabel('x')\n    plt.ylabel('tanh(x)')\n    plt.grid(True)\n    plt.show()\n\nplot_tanh()\n\n\n\n\n\n\n\n\nLooking at the tanh function, one can notice that it limits the upper and lower bounds between -1 & 1, with \\(+∞\\) approaching 1, and \\(-∞\\) approaching -1. It’s pretty easy to implement logit soft capping in PyTorch.\nimport torch\n\ndef soft_cap_logits(logits, soft_cap):\n    scaled_logits = logits / soft_cap\n    t = torch.tanh(scaled_logits)\n    return soft_cap * t\n\n\nlogits = torch.tensor([0.5, 1.0, 2.0, 3.0])\nsoft_cap = 2.0\ncapped_logits = soft_cap_logits(logits, soft_cap)\nThe authors capped the attention logits at 50.0 and final logits at 30.0.\n\n\n\n\n\n\nWarning\n\n\n\nFrom the paper:\nNote that attention logit soft-capping is, at the time of publication, incompatible with common FlashAttention implementations, and we have removed this feature from libraries that use FlashAttention, namely, the HuggingFace transformers library and the vLLM implementation."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#new-heading",
    "href": "posts/2024-07-07 Gemma.html#new-heading",
    "title": "Gemma 2",
    "section": "7 New heading",
    "text": "7 New heading"
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#sec-merge",
    "href": "posts/2024-07-07 Gemma.html#sec-merge",
    "title": "Gemma 2",
    "section": "6 Model merging",
    "text": "6 Model merging\nFrom the paper:\nModel merging. We average models from experiments run with different hyperparameters Ramé et al. (2024) .\nRamé, Alexandre, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, and Olivier Bachem. 2024. “WARP: On the Benefits of Weight Averaged Rewarded Policies.” https://arxiv.org/abs/2406.16768.\n\nI would like to refer the readers to mergekit (Goddard et al. (2024)), which is an open-source library for merging pre-trained Large Language Models.\n\nGoddard, Charles, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. 2024. “Arcee’s MergeKit: A Toolkit for Merging Large Language Models.” arXiv Preprint arXiv:2403.13257.\nFrom the Gemma 2 introduction blog by Huggingface:\n*According to the Technical Report, Gemma 2 used Warp, a new merging technique that merges models in three distinct stages:\n\nExponential Moving Average (EMA): This is applied during the reinforcement learning (RL) fine-tuning process.\nSpherical Linear intERPolation (SLERP): This is applied after the RL fine-tuning of multiple policies.\nLinear Interpolation Towards Initialization (LITI): This stage is applied after the SLERP stage.\n\nPlease refer to one of my previous blogs for an in-depth explanation and implementation in PyTorch on Exponential Moving Average.\nGoing by the mergekit repository, merging models is as simple as running this one line of code:\nmergekit-yaml path/to/your/config.yml ./output-model-directory [--cuda] [--lazy-unpickle] [--allow-crimes] [... other options]\n\n\n\n\n\n\nNote\n\n\n\nI haven’t personally tried model-merging yet, but will share results shortly in a future blog post. Intuitively it feels very similar to model ensembling."
  },
  {
    "objectID": "posts/2024-07-07 Gemma.html#gemma-2-architectural-details",
    "href": "posts/2024-07-07 Gemma.html#gemma-2-architectural-details",
    "title": "Gemma 2",
    "section": "1 Gemma 2 architectural details",
    "text": "1 Gemma 2 architectural details\nIn this section, we look into the architecture details as shared in the report - Gemma 2: Improving Open Language Models at a Practical Size.\nFrom the report:\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. The 9 billion and 27 billion parameter models are available today, with a 2 billion parameter model to be released shortly. In this new version, we provide several technical modifications to our architecture, such as interleaving local-global attentions (Beltagy, Peters, and Cohan (2020)) and group-query attention (Ainslie et al. (2023)). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger.\n\n\n\n\n\n\nNote\n\n\n\nI guess the key point that me hooked to Gemma 2 was the last line shared in the abstract:\nThe resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger.\nThis is pretty big news, and very important for the projects that I have been recently working on. Smaller models in productions means - lower latency, lower memory requirements, faster runtime, thus, an overall reduction in computing costs.\n\n\nThe recent large language models AI@Meta (2024), have been known to have dataset sizes as big as 15T tokens! It is the longer training on bigger datasets that has been key towards LLMs having continued improvements in performance. The models are trained to predict the next tokens in a left-to-right manner.\n\nAI@Meta. 2024. “Llama 3 Model Card.” https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.\nIn Gemma 2, the authors trained the smaller 2.6B and 9B models using knowledge distillation. This, alongside other architecture details, has allowed Gemma 2 to have the best in class performance given it’s size. Let’s look into each one of the components in the following sections."
  }
]