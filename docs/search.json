[
  {
    "objectID": "posts/2021-07-26-annotateddetr.html",
    "href": "posts/2021-07-26-annotateddetr.html",
    "title": "The Annotated DETR",
    "section": "",
    "text": "Welcome to “The Annotated DETR”.\nOne of the most brilliant and well-explained articles I have read is The Annotated Transformer. It introduced Attention like no other post. The simple idea was to present an “annotated” version of the paper Attention is all you need along with code.\nSomething I have always believed in is that when you write things in code, the implementation and secrets become clearer. Nothing is hidden anymore. Reading the research paper with it’s code in front of me, is the best way for me to understand the paper.\n\nThere is nothing magic about magic. The magician merely understands something simple which doesn’t appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can “do magic.”\n– Jeffrey Friedl in the book Mastering Regular Expressions\n\nThe DETR Architecture might seem like magic at first with all it’s glitter and beauty too, but hopefully I would have uncovered that magic for you and revealed all the tricks by the time you finish reading this post. That is my goal -\n\nTo make it as simple as possible for the readers to understand how the DETR model works underneath.\n\nIn this post, I am not trying to reinvent the wheel, but merely bringing together a list of prexisting excellent resources to make it easier for the reader to grasp DETR. I leave it up to the reader to further build upon these foundations in any area they choose.\n\nYou can’t build a great building on a weak foundation. You must have a solid foundation if you’re going to have a strong superstructure.\n– Gordon B. Hinckley\n\nNOTE: All code referenced below has been copied from the official DETR implementation. Also, text directly quoted from the research paper is in Italics."
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#introduction",
    "href": "posts/2021-07-26-annotateddetr.html#introduction",
    "title": "The Annotated DETR",
    "section": "2 Introduction",
    "text": "2 Introduction\nWe present a new method that views object detection as a direct set prediction problem. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bi-partite matching, and a transformer encoder-decoder architecture. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Training code and pretrained models are available at https://github.com/facebookresearch/detr.\nThe goal of object detection is to predict a set of bounding boxes and category labels for each object of interest. Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals, anchors, or window centers. Their performances are significantly influenced by postprocessing steps to collapse near-duplicate predictions, by the design of the anchor sets and by the heuristics that assign target boxes to anchors. To simplify these pipelines, we propose a direct set prediction approach to bypass the surrogate tasks. This end-to-end philosophy has led to significant advances in complex structured prediction tasks such as machine translation or speech recognition, but not yet in object detection: previous attempts either add other forms of prior knowledge, or have not proven to be competitive with strong baselines on challenging benchmarks. This paper aims to bridge this gap.\nOur DEtection TRansformer (DETR, see Figure-1) predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. DETR simplifies the detection pipeline by dropping multiple hand-designed components that encode prior knowledge, like spatial anchors or non-maximal suppression. Unlike most existing detection methods, DETR doesn’t require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes.\n\nDETR, however, obtains lower performances on small objects. Also, training settings for DETR differ from standard object detectors in multiple ways."
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#data-preparation",
    "href": "posts/2021-07-26-annotateddetr.html#data-preparation",
    "title": "The Annotated DETR",
    "section": "3 Data Preparation",
    "text": "3 Data Preparation\nThe input images are batched together, applying \\(0\\)-padding adequately to ensure they all have the same dimensions \\((H_0,W_0)\\) as the largest image of the batch.\n\nIf you haven’t worked with COCO before, the annotations are in a JSON format and must be converted to tensors before they can be fed to the model as labels. Refer to the COCO website here for more information on data format.\n\n\n3.1 Coco Detection Dataset\nThe CocoDetection class below inherits from torchvision.datasets.CocoDetection, and adds custom _transforms on top. There’s also ConvertCocoPolysToMask class that is able to prepare the dataset for both object detection and panoptic segmentation.\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, ann_file, transforms, return_masks):\n        super(CocoDetection, self).__init__(img_folder, ann_file)\n        self._transforms = transforms\n        self.prepare = ConvertCocoPolysToMask(return_masks)\n\n    def __getitem__(self, idx):\n        img, target = super(CocoDetection, self).__getitem__(idx)\n        image_id = self.ids[idx]\n        target = {'image_id': image_id, 'annotations': target}\n        img, target = self.prepare(img, target)\n        if self._transforms is not None:\n            img, target = self._transforms(img, target)\n        return img, target\nNote that in __getitem__, we first use torchvision.datasets.CocoDetection.__getitem__ to get img, target. The img here is returned as a PIL.Image instance and target is a list of Dicts for each annotation. Here, the img, target look like below:\nimg \n>> <PIL.Image.Image image mode=RGB size=640x427 at 0x7F841F918520>\n\ntarget\n>> [{'segmentation': [[573.81, 93.88, 630.42, 11.35, 637.14, 423.0, 569.01, 422.04, 568.05, 421.08, 569.97, 270.43, 560.38, 217.66, 567.09, 190.79, 576.69, 189.83, 567.09, 173.52, 561.34, 162.0, 570.93, 107.31, 572.85, 89.08]], 'area': 24373.2536, 'iscrowd': 0, 'image_id': 463309, 'bbox': [560.38, 11.35, 76.76, 411.65], 'category_id': 82, 'id': 331291}, {'segmentation': [[19.19, 206.3, 188.07, 204.38, 194.79, 249.48, 265.8, 260.04, 278.27, 420.28, 78.68, 421.24, 77.72, 311.85, 95.0, 297.46, 13.43, 267.71, 21.11, 212.06]], 'area': 42141.60884999999, 'iscrowd': 0, 'image_id': 463309, 'bbox': [13.43, 204.38, 264.84, 216.86], 'category_id': 79, 'id': 1122176}]\nNext, we pass the img and target through self.prepare method which is an instance of ConvertCocoPolysToMask class, and perform pre-processing on the outputs from torchvision.datasets.CocoDetection.__getitem__.\nimg\n>> <PIL.Image.Image image mode=RGB size=640x427 at 0x7F841F918520>\n\ntarget\n>> {'boxes': tensor([[560.3800,  11.3500, 637.1400, 423.0000], [ 13.4300, 204.3800, 278.2700, 421.2400]]), 'labels': tensor([82, 79]), 'image_id': tensor([463309]), 'area': tensor([24373.2539, 42141.6094]),  'iscrowd': tensor([0, 0]), 'orig_size': tensor([427, 640]), 'size': tensor([427, 640])}\nThis is what happened inside the self.prepare method: 1. Converted boxes, labels, image_id, area.. to a tensor. 2. We no longer return segmentation masks since we are just working with Object Detection. 3. Filter out objects if iscrowd=1. 4. Convert annotation from \\([X, Y, W, H]\\) to \\([X_1, Y_1, X_2, Y_2]\\) format. 5. Filter out objects if \\(X_2 < X_1\\) or \\(Y_2 < Y_1\\).\n\nI am going to skip over the source code of ConvertCocoPolysToMask but you can find it here if interested. I am also going to skip over the transforms for brevity, but you can find them here if interested.\n\nFrom the paper, transforms/augmentations that get applied are:\nWe use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333. To help learning global relationships through the self-attention of the encoder, we also apply random crop augmentations during training, improving the performance by approximately 1 AP. Specifically, a train image is cropped with probability 0.5 to a random rectangular patch which is then resized again to 800-1333.\nThe overall process of creating this dataset looks like below:\nfrom detr.datasets.coco import make_coco_transforms, CocoDetection\n\ncoco_img_folder = '../../kaggle/mscoco/train2017/'\ncoco_ann_file   = '../../kaggle/mscoco/annotations/instances_train2017.json'\n# create train transforms as in paper\ncoco_train_tfms = make_coco_transforms('train')\ncoco_dset       = CocoDetection(img_folder=coco_img_folder, ann_file=coco_ann_file, transforms=coco_train_tfms, return_masks=False)\n\n# first item in dataset\ncoco_dset[0][0].shape\n>> torch.Size([3, 800, 1066])\n\ncoco_dset[0][1]\n>> \n{'boxes': tensor([[0.5205, 0.6888, 0.9556, 0.5955],\n         [0.2635, 0.2472, 0.4989, 0.4764],\n         [0.3629, 0.7329, 0.4941, 0.5106],\n         [0.6606, 0.4189, 0.6789, 0.7815],\n         [0.3532, 0.1326, 0.1180, 0.0969],\n         [0.2269, 0.1298, 0.0907, 0.0972],\n         [0.3317, 0.2269, 0.1313, 0.1469],\n         [0.3571, 0.0792, 0.1481, 0.1481]]),\n 'labels': tensor([51, 51, 56, 51, 55, 55, 55, 55]),\n 'image_id': tensor([9]),\n 'area': tensor([258072.8281,  95516.1953, 106571.9219,  52219.3594,   4813.5459,\n           3565.9253,   7758.4976,   6395.6035]),\n 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n 'orig_size': tensor([480, 640]),\n 'size': tensor([704, 938])}\nAs can be seen, the coco_dset returns the image as a tensor and also returns the target which is of type Dict. But, right now, the images are of different shape, and we need to make them all to be of the same shape before they can be batched together and passed to the DETR model as input. Let’s see how that looks like next.\n\n\n3.2 Tensor and Mask\nFrom the paper - the input images are batched together, applying \\(0\\)-padding adequately to ensure they all have the same dimensions \\((H_0,W_0)\\) as the largest image of the batch.\nLet’s suppose our batch_size is \\(2\\), where the first image is of shape [3, 765, 512] in blue and the second image is of shape [3, 608, 911] in orange. This has been shown in Figure-2 below:\n\nThis is how the collate function looks like:\ndef collate_fn(batch):\n    batch = list(zip(*batch))\n    batch[0] = nested_tensor_from_tensor_list(batch[0])\n    return tuple(batch)\nnested_tensor_from_tensor_list is responsible for zero padding the original images, to ensure they all have the same dimensions \\((H_0,W_0)\\) as the largest image of the batch. Let’s look at its source code:\ndef nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    # TODO make this more general\n    if tensor_list[0].ndim == 3:\n        if torchvision._is_tracing():\n            # nested_tensor_from_tensor_list() does not export well to ONNX\n            # call _onnx_nested_tensor_from_tensor_list() instead\n            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n\n        # TODO make it support different-sized images\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n        batch_shape = [len(tensor_list)] + max_size\n        b, c, h, w = batch_shape\n        dtype = tensor_list[0].dtype\n        device = tensor_list[0].device\n        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n        for img, pad_img, m in zip(tensor_list, tensor, mask):\n            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n            m[: img.shape[1], :img.shape[2]] = False\n    else:\n        raise ValueError('not supported')\n    return NestedTensor(tensor, mask)\nThis nested_tensor_from_tensor_list receives a tensor_list, which is a list of img tensors of varying shape - [[3, 608, 911], [3, 765, 512]]. We calculate the max_size in max_size = _max_by_axis([list(img.shape) for img in tensor_list]). This _max_by_axis function returns the maximum value for each axis. Therefore, the value of returned max_size is [3, 765, 911].\nNext, our batch_shape is [len(tensor_list)] + max_size, which equals [2, 3, 765, 911] in our example so far.\nNext, we define tensor and mask which are of shapes [2, 3, 765, 911] and [2, 765, 911] respectively.\ntensor = torch.zeros(batch_shape, dtype=dtype, device=device)\nmask = torch.ones((b, h, w), dtype=torch.bool, device=device)\nFinally, we fill the tensor and mask values with the img values which were of shapes [[3, 608, 911], [3, 765, 512]], and also set the mask values to be False inside the actual image shape.\nfor img, pad_img, m in zip(tensor_list, tensor, mask):\n    pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n    m[: img.shape[1], :img.shape[2]] = False\nThis has been illustrated in Figure-3 below:\n\nHere, the blue region and orange region in both respective resized images represent the filled values. For these blue and orange regions, the mask values are set to False, whereas in the grey region outside the mask values are set to True. In code, this looks like:\n\nCan you guess the tensor and mask shapes? tensor is of shape [2, 3, 765, 911], where both images are zero-padded. The first image (Blue) is zero-padded in height, whereas the second image is zero-padded in width (Orange). Similarly, the mask has shape [2, 765, 911] where the blue and orange regions represent value False, and the gray region in Figure-3, represents value True.\n\nFinally, the nested_tensor_from_tensor_list returns a NestedTensor passing in tensor and mask - NestedTensor(tensor, mask). So what is this NestedTensor? Let’s look at that next.\n\n\n3.3 NestedTensor\nNestedTensor is a simple tensor class that puts tensors and masks together as below:\nclass NestedTensor(object):\n    def __init__(self, tensors, mask: Optional[Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n\n    def to(self, device):\n        # type: (Device) -> NestedTensor # noqa\n        cast_tensor = self.tensors.to(device)\n        mask = self.mask\n        if mask is not None:\n            assert mask is not None\n            cast_mask = mask.to(device)\n        else:\n            cast_mask = None\n        return NestedTensor(cast_tensor, cast_mask)\n\n    def decompose(self):\n        return self.tensors, self.mask\n\n    def __repr__(self):\n        return str(self.tensors)\nAs can be seen from the NestedTensor source code, it combines tensors and mask and stores them as self.tensors and self.mask attributes.\nThis NestedTensor class is really simple - it has two main methods: 1. to: casts both tensors and mask to device (typically \"cuda\") and returns a new NestedTensor containing cast_tensor and cast_mask. 2. decompose: returns tensors and mask as a tuple, thus decomposing the “nested” tensor.\nimport torch\nfrom detr.util.misc import NestedTensor\n\n# represents outputs from custom collate function that we looked at before\ntensor = torch.randn(2, 3, 765, 911)\nmask   = torch.randn(2, 765, 911)\n\nnested_tensor = NestedTensor(tensor, mask)\n\nnested_tensor.tensors.shape\n>> torch.Size([2, 3, 765, 911])\n\nnested_tensor.mask.shape\n>> torch.Size([2, 765, 911])\nAnd that’s it! This NestedTensor is what get’s fed as input to the DETR Backbone CNN in Figure-1."
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#the-detr-architecture",
    "href": "posts/2021-07-26-annotateddetr.html#the-detr-architecture",
    "title": "The Annotated DETR",
    "section": "4 The DETR Architecture",
    "text": "4 The DETR Architecture\nThe overall DETR architecture is surprisingly simple and depicted in Figure-1 below. It contains three main components: a CNN backbone to extract a compact feature representation, an encoder-decoder transformer, and a simple feed forward network (FFN) that makes the final detection prediction.\n\nThe overall implementation of the DETR architecture has been shown below:\nclass DETR(nn.Module):\n    \"\"\" This is the DETR module that performs object detection \"\"\"\n    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n        super().__init__()\n        self.num_queries = num_queries\n        self.transformer = transformer\n        hidden_dim = transformer.d_model\n        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n        self.backbone = backbone\n        self.aux_loss = aux_loss\n\n    def forward(self, samples: NestedTensor):\n        features, pos = self.backbone(samples)\n\n        src, mask = features[-1].decompose()\n        assert mask is not None\n        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n\n        outputs_class = self.class_embed(hs)\n        outputs_coord = self.bbox_embed(hs).sigmoid()\n        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n        if self.aux_loss:\n            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n        return out\nAll the magic happens inside backbone and transformer, which we will look at next."
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#backbone",
    "href": "posts/2021-07-26-annotateddetr.html#backbone",
    "title": "The Annotated DETR",
    "section": "5 Backbone",
    "text": "5 Backbone\nStarting from the initial image \\(x_{img} ∈ R^3×H_0×W_0\\) (with 3 color channels), a conventional CNN backbone generates a lower-resolution activation map \\(f ∈ R^{C×H×W}\\). Typical values we use are C = 2048 and H,W = \\(\\frac{H0}{32} , \\frac{W0}{32}\\).\nclass Backbone(BackboneBase):\n    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n    def __init__(self, name: str,\n                 train_backbone: bool,\n                 return_interm_layers: bool,\n                 dilation: bool):\n        backbone = getattr(torchvision.models, name)(\n            replace_stride_with_dilation=[False, False, dilation],\n            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)\nAbove we create a simple backbone that inherits from BackboneBase. The backbone is created using torchvision.models and supports all models implemented in torchvision. For a complete list of supported models, refer here.\nAs also mentioned above, the typical value for number of channels in the output feature map is 2048, therefore, for all models except resnet18 & resnet34, the num_channels variable is set to 2048. This Backbone accepts a three channel input image tensor of shape \\(3×H_0×W_0\\), where \\(H_0\\) refers to the input image height, and \\(W_0\\) refers to the input image width.\nSince all the fun happens inside the BackboneBase class including the forward method, let’s look at that next.\n\n5.1 BackboneBase\nclass BackboneBase(nn.Module):\n\n    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n        super().__init__()\n        for name, parameter in backbone.named_parameters():\n            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad_(False)\n        if return_interm_layers:\n            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n        else:\n            return_layers = {'layer4': \"0\"}\n        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n        self.num_channels = num_channels\n\n    def forward(self, tensor_list: NestedTensor):\n        xs = self.body(tensor_list.tensors)\n        out: Dict[str, NestedTensor] = {}\n        for name, x in xs.items():\n            m = tensor_list.mask\n            assert m is not None\n            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n            out[name] = NestedTensor(x, mask)\n        return out\nThe forward method of BackboneBase accepts an instance of NestedTensor class that contains tensors and mask as we saw in secion Data preparation. BackboneBase then takes the tensors and passes that through self.body in xs = self.body(tensor_list.tensors), which is responsible for getting the output feature map of shape \\(f ∈ R^{C×H×W}\\), where \\(C\\) is typically set to 2048.\nNote: The self.body either returns the output from the last layer of the backbone model, or from all intermediate layers and the final layer depending on the value of return_layers. For an introduction to IntermediateLayerGetter, please refer to torchvision docs.\nThe output of self.body is a Dict that looks something like {\"0\": <torch.Tensor>} or {\"0\": <torch.Tensor>, \"1\": <torch.Tensor>, \"2\": <torch.Tensor>...} depending on whether return_interm_layers is True or False. Finally, we iterate through this Dict output of self.body which we call xs, interpolate the mask to have the same \\(H\\) and \\(W\\) as the lower-resolution activation map \\(f ∈ R^{C×H×W}\\) output from Backbone and store both x and mask in NestedTensor as shown below:\n        for name, x in xs.items():\n            m = tensor_list.mask\n            assert m is not None\n            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n            out[name] = NestedTensor(x, mask)\nIn summary, the Backbone is responsible for accepting an input NestedTensor that consists of the input image as tensors and a mask corresponding to the image. The backbone merely extracts the features from this input image, interpolates the mask to match the feature map size and returns them as a NestedTensor in a Dict. Let’s see the Backbone in action in code:\nimport torch\nfrom detr.util.misc import NestedTensor \nfrom detr.models.backbone import Backbone \n\n# resnet-50 backbone\nbackbone = Backbone('resnet50', train_backbone=True, return_iterm_layers=False, dilation=False)\n\n# create nested tensor, mimic output from dataset preparation as in last section \ntensor   = torch.randn(2, 3, 765, 911)\nmask     = torch.randn(2, 765, 911)\nbb_input = NestedTensor(tensor, mask)\n\n# get output from backbone \nout = backbone(bb_input)\nout.keys()\n>> dict_keys(['0'])\n\nout['0'].__class__\n>> detr.util.misc.NestedTensor\n\nout['0'].tensors.shape\n>> torch.Size([2, 2048, 24, 29])\n\nout['0'].mask.shape\n>> torch.Size([2, 24, 29])\nTherefore, starting from the initial image \\(x_{img} ∈ {3 x 765 x 911}\\) (with 3 color channels), a conventional CNN backbone generates a lower-resolution activation map \\(f ∈ R^{2048 x 24 x 29}\\).\n\n\n5.2 Positional Encoding\n\nGoing back to Figure-4, it can be seen that Positional Encodings are added to the output lower-resolution activation map from the Backbone CNN. Also, in Figure-5, it can be seen that Positional Encodings are also added to the Attention layer’s input at every Decoder layer. Thus, there are two types of Positional Encodings in DETR.\n\nSince the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings that are added to the input of each attention layer. We defer to the supplementary material the detailed definition of the architecture, which follows the one described in Attention Is All You Need.\nThere are two kinds of positional encodings in our model: spatial positional encodings and output positional encodings (object queries).\n\nIMPORTANT : The spatial positional encodings are the ones that refer to the spatial positions \\(H\\) & \\(W\\) in the lower resolution feature map \\(f ∈ R^{C x H x W}\\). The output positional encodings are the ones that refer to the positions of the various objects in the image and these are always learnt.\n\nThere are two options to define the spatial positional encodings: 1. Fixed positional encodings (PositionEmbeddingSine) 2. Learned positional encodings (PositionEmbeddingLearned)\nBelow, we only look at PositionEmbeddingSine as an example:\nclass PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n    def forward(self, tensor_list: NestedTensor):\n        x = tensor_list.tensors\n        mask = tensor_list.mask\n        assert mask is not None\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n        if self.normalize:\n            eps = 1e-6\n            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        return pos\nAs we already know from the Attention is all you need paper, the fixed positional encodings can be mathematically defined as:\n\\[\nPE(pos, 2i) = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\tag{1}\n\\]\n\\[\nPE(pos, 2i) = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\tag{2}\n\\]\nwhere \\(pos\\) is the position and \\(i\\) is the dimension.\nWe defer the reader to Transformer Architecture: The Positional Encoding by Amirhossein Kazemnejad for more information on Positional Encodings.\nAs for the PositionalEncoding class, remember, the Backbone first converts input image tensor of shape \\(3×H_0×W_0\\) to a lower-resolution activation map of size \\(f ∈ R^{C×H×W}\\). Positional Encodings are added to this lower-resolution feature map. Since, we need to be able to define positions both along the x-axis and y-axis, therefore, we have y_embed and x_embed variables that increase in value by 1 every time boolean True is present in not_mask.\nNext, we convert both pos_x and pos_y to be of dimension dim_t and finally take the alternate .sin() and .cos() to define pos_x and pos_y. In the end, pos becomes a concatenated tensor of pos_x and pos_y.\n\nWe do not look at Positional Encodings in detail as many a good resource exist on this topic. Also, the positional encodings have been utilized from the Attention Is All You Need paper as is.\n\n\n\n5.3 Joiner\nSince the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings that are added to the input of each attention layer.\nThe Joiner class below, is merely a convenience class, that accepts a backbone and position_embedding as defined previously, and a NestedTensor as input. It then generates a lower-resolution activation map of size \\(f ∈ R^{C×H×W}\\) and positional embeddings of the same-size corresponding to the \\(X\\) and \\(Y\\) positions on the grid.\nclass Joiner(nn.Sequential):\n    def __init__(self, backbone, position_embedding):\n        super().__init__(backbone, position_embedding)\n\n    def forward(self, tensor_list: NestedTensor):\n        xs = self[0](tensor_list)\n        out: List[NestedTensor] = []\n        pos = []\n        for name, x in xs.items():\n            out.append(x)\n            # position encoding\n            pos.append(self[1](x).to(x.tensors.dtype))\n        return out, pos\nLet’s see it in action below:\nimport torch \nfrom detr.util.misc import NestedTensor\nfrom detr.models.backbone import *\nfrom detr.models.position_encoding import PositionEmbeddingSine\n\n# create backbone, `Joiner` is what get's stored in DETR backbone, not `backbone`\nbackbone  = Backbone('resnet50', True, False, False)   \npos_embed = PositionEmbeddingSine(num_pos_feats=128, normalize=True)\njoiner    = Joiner(backbone, pos_embed)\n\n# mimic input \ntensor   = torch.randn(2, 3, 765, 911)\nmask     = torch.randn(2, 765, 911)\nbb_input = NestedTensor(tensor, mask)\n\n# get output \nout, pos = joiner(bb_input)\n\nout[-1].__class__\n>> detr.util.misc.NestedTensor\n\n# reduced lower-resolution feature map output from backbone\nout[-1].tensors.shape\n>>  torch.Size([2, 2048, 24, 29])\n\n# positional encodings with same shape as lower-resolution feature map \npos[-1].shape\n>>  torch.Size([2, 256, 24, 29])\n\n\n5.4 Summary\nTherefore, in the offical code implementation of DETR, the Backbone, is actually an instance on Joiner class (how confusing!), that accepts an input, which is of type NestedTensor (how confusing again!). The tensor and mask of this NestedTensor input have been zero-padded to ensure that all have the same dimensions \\((H_0,W_0)\\) as the largest image of the input batch. See Figure-3 for reference.\nFinally, this NestedTensor instance, is passed on the Backbone to get outputs out & pos. Here, out is a list of NestedTensor, the length of the list depends on the value return_interm_layers. out[-1].tensors represents lower-resolution activation map \\(f ∈ R^{C×H×W}\\), where \\(C\\) has value 2048. And pos[-1] represents the spatial positional encodings \\(pos ∈ R^{256×H×W}\\) for each position on the grid."
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#detr-transformer",
    "href": "posts/2021-07-26-annotateddetr.html#detr-transformer",
    "title": "The Annotated DETR",
    "section": "6 DETR Transformer",
    "text": "6 DETR Transformer\n\nNow, before we can move on and look at the DETR Transformer in code, it’s really crucial to first understand the bigger picture. So far we know that the backbone upon accepting an input returns out and pos, where out[-1].tensors.shape is \\(f ∈ R^{2048×H×W}\\) and pos[-1].shape is \\(pos ∈ R^{256×H×W}\\).\nBefore the out is passed to the transformer, the number of channels are reduced as mentioned in the paper.\nFirst, a 1x1 convolution reduces the channel dimension of the high-level activation map \\(f\\) from \\(C\\) to a smaller dimension \\(d\\). creating a new feature map \\(z_0 ∈ R^{d×H×W}\\).\nHere, \\(d\\) is set to 256, therefore, the new feature map size becomes \\(f ∈ R^{256×H×W}\\) which matches that of the positional encodings.\nAs also mentioned in the paper,\nThe encoder expects a sequence as input, hence we collapse the spatial dimensions of \\(z_0\\) into one dimension, resulting in a \\(d×HW\\) feature map.\nTherefore, we can reshape both feature map and spatial positional encodings to shape \\(696 x 1 x 256\\) as shown in Figure-5 above. The overall Transformer can be implemented as below:\nclass Transformer(nn.Module):\n\n    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False,\n                 return_intermediate_dec=False):\n        super().__init__()\n\n        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n                                                dropout, activation, normalize_before)\n        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n\n        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n                                                dropout, activation, normalize_before)\n        decoder_norm = nn.LayerNorm(d_model)\n        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n                                          return_intermediate=return_intermediate_dec)\n\n        self._reset_parameters()\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n    def _reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, src, mask, query_embed, pos_embed):\n        # flatten NxCxHxW to HWxNxC\n        bs, c, h, w = src.shape\n        src = src.flatten(2).permute(2, 0, 1)\n        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n        mask = mask.flatten(1)\n\n        tgt = torch.zeros_like(query_embed)\n        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n                          pos=pos_embed, query_pos=query_embed)\n        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\nBelow, let’s look at the TransformerEncoder, TransformerEncoderLayer, TransformerDecoder and TransformerDecoderLayer in detail."
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#transformer-encoder",
    "href": "posts/2021-07-26-annotateddetr.html#transformer-encoder",
    "title": "The Annotated DETR",
    "section": "7 Transformer Encoder",
    "text": "7 Transformer Encoder\nEach encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network (FFN).\nIn terms of implementation, from this point on the Transformer architecture is implemented very similarly to the implementation as explained in The Annotated Transformer. But, for completeness, I will also share the implementations below.\nThe Transformer Encoder consists of multiple Transformer Encoder layers. Thus, it can be easily implemented as below:\nclass TransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(self, src,\n                mask: Optional[Tensor] = None,\n                src_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):\n        output = src\n        for layer in self.layers:\n            output = layer(output, src_mask=mask,\n                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n        if self.norm is not None:\n            output = self.norm(output)\n        return output\n_get_clones simply clones the Transformer Encoder layer (explained below), num_layers number of times and returns nn.ModuleList.\nBelow, the TransformerEncoderLayer has been implemented.\nclass TransformerEncoderLayer(nn.Module):\n\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n\n    def forward(self, src,\n                src_mask: Optional[Tensor] = None,\n                src_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):\n        src2 = self.norm1(src)\n        q = k = self.with_pos_embed(src2, pos)\n        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask)[0]\n        src = src + self.dropout1(src2)\n        src2 = self.norm2(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n        src = src + self.dropout2(src2)\n        return src\nIt’s really straightforward, we accept an input which is called src, it is normalized using nn.LayerNorm, and finally we set the query and key matrices \\(q\\) and \\(k\\) as the same by adding positional encoding to the normalized input. Finally, self-attention operation is performed to get src2 as the output. In this case, since we can attend to anywhere in the sequence - both forwards and backwards, attn_mask is set to None. Whereas, the key_padding_mask are the elements in the key that are ignored by attention.\nAs per the PyTorch Docs for key_padding_mask, when given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. We already from Data Preparation section that mask is set to True for the gray area in Figure-3, thus, it makes sense to not attend to these positions.\nNext, let’s look at the TransformerDecoder!"
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#transformer-decoder",
    "href": "posts/2021-07-26-annotateddetr.html#transformer-decoder",
    "title": "The Annotated DETR",
    "section": "8 Transformer Decoder",
    "text": "8 Transformer Decoder\nThe decoder follows the standard architecture of the transformer, transforming \\(N\\) embeddings of size \\(d\\) using multi-headed self-attention and encoder-decoder attention mechanisms. The difference with the original transformer is that our model decodes the \\(N\\) objects in parallel at each decoder layer, while Vaswani et al. use an autoregressive model that predicts the output sequence one element at a time. We refer the reader unfamiliar with the concepts to the supplementary material. Since the decoder is also permutation-invariant, the \\(N\\) input embeddings must be different to produce different results. These input embeddings are learnt positional encodings that we refer to as object queries, and similarly to the encoder, we add them to the input of each attention layer. The N object queries are transformed into an output embedding by the decoder. They are then independently decoded into box coordinates and class labels by a feed forward network (described in the next subsection), resulting \\(N\\) final predictions. Using self-attention and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context.\nSimilar to the Transformer Encoder, the Transformer Decoder consists of repeated Transformer Decoder layers and can be easily implemented as below:\nclass TransformerDecoder(nn.Module):\n\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n        self.return_intermediate = return_intermediate\n\n    def forward(self, tgt, memory,\n                tgt_mask: Optional[Tensor] = None,\n                memory_mask: Optional[Tensor] = None,\n                tgt_key_padding_mask: Optional[Tensor] = None,\n                memory_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None,\n                query_pos: Optional[Tensor] = None):\n        output = tgt\n        intermediate = []\n        for layer in self.layers:\n            output = layer(output, memory, tgt_mask=tgt_mask,\n                           memory_mask=memory_mask,\n                           tgt_key_padding_mask=tgt_key_padding_mask,\n                           memory_key_padding_mask=memory_key_padding_mask,\n                           pos=pos, query_pos=query_pos)\n            if self.return_intermediate:\n                intermediate.append(self.norm(output))\n\n        if self.norm is not None:\n            output = self.norm(output)\n            if self.return_intermediate:\n                intermediate.pop()\n                intermediate.append(output)\n\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n\n        return output.unsqueeze(0)\nThe main difference is the option to return intermediate outputs for Auxilary losses (we’ll look at losses later in the blog post). If self.return_intermediate is set to True, stacked output from every decoder layer is returned otherwise, output from the last decoder layer is returned.\n\n8.1 Transformer Decoder Layer\nAs for the Transformer Decoder layer, its implementation is also very similar to the Transformer Encoder layer.\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n\n    def forward(self, tgt, memory,\n                tgt_mask: Optional[Tensor] = None,\n                memory_mask: Optional[Tensor] = None,\n                tgt_key_padding_mask: Optional[Tensor] = None,\n                memory_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None,\n                query_pos: Optional[Tensor] = None):\n        tgt2 = self.norm1(tgt)\n        q = k = self.with_pos_embed(tgt2, query_pos)\n        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt2 = self.norm2(tgt)\n        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n                                   key=self.with_pos_embed(memory, pos),\n                                   value=memory, attn_mask=memory_mask,\n                                   key_padding_mask=memory_key_padding_mask)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt2 = self.norm3(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n        tgt = tgt + self.dropout3(tgt2)\n        return tgt\nThe TransformerDecoderLayer requires a bit more attention than TransformerEncoderLayer (pun intended). First, the tgt variable in this case is just a bunch of zeroes of shape \\(100 x 1 x 256\\) which represents Object Queries as in Figure-5.\n\nThe difference with the original transformer is that the DETR model decodes the \\(N\\) objects in parallel at each decoder layer, while Vaswani et al. used an autoregressive model that predicts the output sequence one element at a time. Since the decoder is also permutation-invariant, the \\(N\\) input embeddings must be different to produce different results. These learnt positional encodings are called object queries and as can be seen in Figure-5, similar to the Encoder, these object queries are also added to the input at each attention layer in the Decoder.\n\nThe Decoder layer can be divided into three parts: 1. Multi-Head self-attention (on object queries, output positional encodings) 2. Multi-Head Attention (Output from 1st part + output positional encodings, Output from encoder + Spatial Positional encodings) 3. Feed Forward Neural Network\nIMO, to make things crystal clear, it is important to note what’s going on in each of the Attention layers inside the Decoder.\nFor the first Multi-Head Self-Attention layer, the Q, K & V values are: 1. Query: Object queries (tgt, initialized as zeroes) + output positional encodings (qpos, learned positional encodings, initialized as nn.Embedding in PyTorch) 2. Key: Object queries (tgt, initialized as zeroes) + output positional encodings (qpos, learned positional encodings, initialized as nn.Embedding in PyTorch) 3. Value: Object queries (tgt)\n\nNote that the Query and Key values are the same in the first attention layer, hence, self-attention.\n\nFor the second Multi-Head Attention layer, the Q, K & V values are: 1. Query: Output from first Multi-Head Self-Attention layer (tgt) + ouput positional encodings (qpos) 2. Key: Output from encoder (memory) + Spatial Positional Encodings (pos) 3. Value: Output from encoder (memory)\nTherefore, it’s in the second attention layer that the Decoder interacts with/attends to the outputs from the Encoder and starts updating the input Object queries to possible object location information.\nAs can be seen from Figure-5, the output from the Decoder is of shape \\(6x1x100x256\\), because it contains output from all Decoder layers, which is finally fed to Class FFN and Box FFN for bounding box prediction and classification.\nNext, we calculate the loss on the outputs of the Decoder, and continue this process over and over again, until the model learns to classify and locate the various objects in the input images. I am going to skip over the loss function for now, but that will be introduced in a separate blog post in the next few days."
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#conclusion",
    "href": "posts/2021-07-26-annotateddetr.html#conclusion",
    "title": "The Annotated DETR",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nI hope that as part of this blog post, I’ve been able to explain the DETR architecture, and IMHO, this blog post is feature complete, barring the loss function, that will be added later.\nAs usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html",
    "href": "posts/2020-07-12-oganized-pytorch.html",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "",
    "text": "Have you tried PytorchLightning already? If so, then you know why it’s so cool. If you haven’t, hopefully by the time you finish reading this post, you will find it pretty cool (the word ‘it’ could refer to this blogpost or the wonderful PytorchLightning library - I leave this decision to the reader).\nNote: From here on, we refer to PytorchLightning as PL, cause it’s a long name to type and I left my favourite keyboard at work.\nFor a while now, I was jealous of Tensorflow solely because it’s possible to use the same script to train a model on CPU, GPU or TPU without really changing much! For example, take this notebook from my one of my favourite kagglers and - at the time of writing this blogpost - a researcher at NVIDIA - Chris Deotte and also, since yesterday, Kaggle 4x Grandmaster! Just by using an appropriate strategy in Tensorflow, it is possible to run the same experiments on your choice of hardware without changing anything else really. That is the same script could run in TPU, GPU or CPU.\nIf you’ve already worked on multi-GPU machines or used torch XLA to run things on TPU using PyTorch, then you know my rant. Changing hardware choices in PyTorch is not as convenient when it comes to this. I love PyTorch - I do, but just this one thing would make me really frustrated.\nWelcome PL! I wish I tried this library sooner.\nIn this blogpost, we will be going through an introduction to PL and implement all the cool tricks like - Gradient Accumulation, 16-bit precision training, and also add TPU/multi-gpu support - all in a few lines of code. We use PL to work on SIIM-ISIC Melanoma Classification challenge on Kaggle. In this blogpost, our focus will be on introducing PL and we use the ISIC competition as an example.\nWe also draw comparisons to the typical workflows in PyTorch and compare how PL is different and the value it adds in a researcher’s life.\nThe first part of this post, is mostly about getting the data, creating our train and validation datasets and dataloaders and the interesting stuff about PL comes in The Lightning Module section of this post. If this stuff bores you because you’ve done this so many times already, feel free to skip forward to the model implemention."
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#whats-isic-melanoma-classification-challenge",
    "href": "posts/2020-07-12-oganized-pytorch.html#whats-isic-melanoma-classification-challenge",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "1 What’s ISIC Melanoma Classification challenge?",
    "text": "1 What’s ISIC Melanoma Classification challenge?\nFrom the description on Kaggle, > Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It’s also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective. Currently, dermatologists evaluate every one of a patient’s moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma.\nIn this competition, the participants are asked to build a Melonama classifier that classifies to identify melonama in images of skin lesions. Typical lesion images look like the ones below:\n\nIn this blogpost, we will use PL to build a solution that can tell the malign melonama images apart from the rest. The model should take only a few hours to train and have 0.92 AUC score!\n\nA side note: Deep learning has come a far way. Compare this to 2012 where AlexNet was trained on multiple e GTX 580 GPU which has only 3GB of memory. To train on 1.2 million examples of Imagenet, the authors had to split the model (with just 8 layers) to 2 GPUs. It took 5-6 days to train this network. Today, it’s possible to train in a few hours or even minutes. For ISIC, each epoch for size 256x256 is around 2mins including validation on a P100 GPU."
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#getting-the-data",
    "href": "posts/2020-07-12-oganized-pytorch.html#getting-the-data",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "2 Getting the data",
    "text": "2 Getting the data\nYou can download the 256x256 version of the Jpeg images here with all the required metadata to follow along."
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#melonama-dataset",
    "href": "posts/2020-07-12-oganized-pytorch.html#melonama-dataset",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "3 Melonama Dataset",
    "text": "3 Melonama Dataset\nGetting our data ready for ingestion into the model is one of the basic things that we need to do for every project.\nclass MelonamaDataset:\n    def __init__(self, image_paths, targets, augmentations=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.augmentations = augmentations\n\n    def __len__(self): return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = np.array(Image.open(image_path))\n        target = self.targets[idx]\n\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image, torch.tensor(target, dtype=torch.long)\nThe above dataset is a pretty simple class that is instantiated by passing in a list of image_paths, targets and augmentations if any. To get an item, it reads an image using Image module from PIL, converts to np.array performs augmentations if any and returns target and image.\nWe can use glob to get train_image_paths and val_image_paths and create train and val datasets respectively.\n# psuedo code\ntrain_image_paths = glob.glob(\"<path_to_train_folder>\")\nval_image_paths = glob.glob(\"<path_to_val_folder>\")\n\nsz = 256 #go bigger for better AUC score but slower train time\n\ntrain_aug = train_aug = albumentations.Compose([\n    RandomCrop(sz,sz),\n    ..., #your choice of augmentations\n    albumentations.Normalize(always_apply=True), \n    ToTensorV2()\n])\n\nval_aug = albumentations.Compose([\n    albumentations.CenterCrop(sz, sz),\n    albumentations.Normalize(always_apply=True),\n    ToTensorV2()\n])\n\ntrain_dataset = MelonamaDataset(train_image_paths, train_targets, train_aug)\nval_dataset = MelonamaDataset(val_image_paths, val_targets, val_aug)\nOnce we have our datasets ready, we can now create our dataloaders and let’s inspect the train images as a sanity check.\n# Dataloaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=64, shuffle=True, num_workers=4)\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=64, shuffle=False, num_workers=4)\n# visualize images\nimport torchvision.utils as vutils\n\ndef matplotlib_imshow(img, one_channel=False):\n    fig,ax = plt.subplots(figsize=(16,8))\n    ax.imshow(img.permute(1,2,0).numpy())\n\nimages= next(iter(train_loader))[0][:16]\nimg_grid = torchvision.utils.make_grid(images, nrow=8, normalize=True)\nmatplotlib_imshow(img_grid)\n\nNow that our dataloaders are done, and looking good, we are ready for some lightning for our Melonama classifier!"
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#lightning-module",
    "href": "posts/2020-07-12-oganized-pytorch.html#lightning-module",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "4 Lightning Module",
    "text": "4 Lightning Module\nPL takes away much of the boilerplate code. By taking away the Engineering Code and the Non-essential code, it helps us focus on the Research code!\nThe Quick Start and Introduction Guide on PL’s official documentation are great resources to start learning about PL! I started there too.\n\n4.1 Model and Training\nOur model in PL looks something like:\nclass Model(LightningModule):\n    def __init__(self, arch='efficientnet-b0'):\n        super().__init__()\n        self.base = EfficientNet.from_pretrained(arch)\n        self.base._fc = nn.Linear(self.base._fc.in_features, 1)\n\n    def forward(self, x):\n        return self.base(x)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=5e-4)\n\n    def step(self, batch):\n        x, y  = batch\n        y_hat = self(x)\n        loss  = WeightedFocalLoss()(y_hat, y.view(-1,1).type_as(y_hat))\n        return loss, y, y_hat.sigmoid()\n\n    def training_step(self, batch, batch_nb):\n        loss, y, y_hat = self.step(batch)\n        return {'loss': loss}\n\n    def validation_step(self, batch, batch_nb):\n        loss, y, y_hat = self.step(batch)\n        return {'loss': loss, 'y': y.detach(), 'y_hat': y_hat.detach()}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        auc = self.get_auc(outputs)\n        print(f\"Epoch {self.current_epoch} | AUC:{auc}\")\n        return {'loss': avg_loss}\n    \n    def get_auc(self, outputs):\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        # shift tensors to cpu\n        auc = roc_auc_score(y.cpu().numpy(), \n                            y_hat.cpu().numpy()) \n        return auc\nWe are using WeightedFocalLoss from my previous blogpost, because this is an imbalanced dataset with only around 1.77% positive classes.\n\n\n4.2 Model implementation compared to PyTorch\nWe add the __init__ and forward method just like you would in pure PyTorch. The LightningModule just adds some extra functionalities on top.\nIn pure pytorch, the main loop with training and validation would look something like:\ntrain_dataset, valid_dataset = MelonamaDataset(...), MelonamaDatasaet(...)\ntrain_loader, valid_loader = DataLoader(train_dataset, ...), DataLoader(valid_dataset, ...)\noptimizer = ...\nscheduler = ...\ntrain_augmentations = albumentations.Compose([...])\nval_aug = albumentations.Compose([...])\nearly_stopping = EarlyStopping(...)\nmodel = PyTorchModel(...)\ntrain_loss = train_one_epoch(model, optimizer, scheduler)\npreds, valid_loss = evaluate(args, valid_loader, model)\nreport_metrics()\nif early_stopping.early_stop:\n    save_model_checkpoint()\n    stop_training()\nAnd ofcourse, then we define our train_one_epoch and evaluate functions where the training loop looks typically like:\nmodel.train()\nfor b_idx, data in enumerate(train_loader):\n    loss = model(**data)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nAnd very similar for evaluate. As you can see, we have to write a lot of code to make things work in PyTorch. While this is great for flexibility, typically we have to reuse the same code over and over again in various projects. The training and evaluate loops hardly change much.\nWhat PL does, is that it automates this process for us. No longer do we need to write the boilerplate code.\nThe training loop, goes directly inside the training_step method and the validation loop inside the validation_step method. The typical reporting of metrics happens inside the validation_epoch_end method. Inside the Model class, both the training_step and validation_step call the step method which get’s the xs and ys from the batch, calls forward to make a forward pass and returns the loss. When we are finished training, our validation loop get’s called and at the end of an epoch validation_epoch_end get’s called which accumulates the results for us and calculates AUC score. We use roc_auc_score because AUC score is used as a metric on the Kaggle competition itself.\nAnd that’s really it. This is all it takes in PL to create, train and validate a deep learning model. There are some other nice functionalities like logging - Wandb and also tensorboard support which you can read more about here.\nShifting from PyTorch to PL is super easy. It took me around a few hours to read up the introduction docs and reimplement the ISIC model in PL. I find PL code is much more organized and compact compared to PyTorch and still very flexible to run experiments. Also, when sharing solutions with others, everybody knows exactly where to look - for example, the training loop is always in the training_step method, validation loop is inside the validation_step and so on.\nIn some ways, I was able to draw comparisons to the wonderful fastai library in the sense that both the libraries make our lives easier.\nSimilar to fastai, to train the model in PL, we can now simply create a Trainer and call .fit().\ndebug = False\ngpus = torch.cuda.device_count()\ntrainer = Trainer(gpus=gpus, max_epochs=2, \n                  num_sanity_val_steps=1 if debug else 0)\n\ntrainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n\n## outputs\n>>  Epoch 0 | AUC:0.8667878706561116\n    Epoch 1 | AUC:0.8867006574533746\nAnd that’s really it. This is all it takes to create a baseline model in PL."
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#gradient-accumulation",
    "href": "posts/2020-07-12-oganized-pytorch.html#gradient-accumulation",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "5 Gradient Accumulation",
    "text": "5 Gradient Accumulation\nSo now that our baseline model is ready, let’s add gradient accumulation!\ntrainer = Trainer(gpus=1, max_epochs=2, \n                  num_sanity_val_steps=1 if debug else 0, \n                  accumulate_grad_batches=2)\nIt’s as simple as adding a single parameter in PL!\nA typical workflow in PyTorch would look like:\naccumulate_grad_batches=2\noptimizer.zero_grad()\nfor b_idx, data in enumerate(train_loader):\n    loss = model(**data, args=args, weights=weights)\n    loss.backward()\n        if (b_idx + 1) % accumulate_grad_batches == 0:\n                # take optimizer every `accumulate_grad_batches` number of times\n                optimizer.step()\n                optimizer.zero_grad()\nPL nicely takes this boilerplate code away from us and provides easy access to researchers to implement gradient accumulation. It is very helpful to have larger batch sizes on a single GPU. To read more about it, refer to this great article by Hugging Face!"
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#bit-precision-training",
    "href": "posts/2020-07-12-oganized-pytorch.html#bit-precision-training",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "6 16-bit precision training",
    "text": "6 16-bit precision training\n16 bit precision can cut the memory usage by half and also speed up training dramatically. Here is a research paper which provides comprehensive analysis on 16-bit precision training.\nFor a more gentler introduction refer to the fastai docs here which has some great resources and explains mixed precision very nicely.\nTo add 16-bit precision training, we first need to make sure that we PyTorch 1.6+. PyTorch only recently added native support for Mixed Precision Training.\nTo download the latest version of PyTorch simply run\n!pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\nAfter this, adding 16-bit training is as simple as:\ntrainer = Trainer(gpus=1, max_epochs=2, \n                  num_sanity_val_steps=1 if debug else 0, \n                  accumulate_grad_batches=2, precision=16)\nIf you want to continue to use an older version of PyTorch, refer here.\nIn a typical workflow in PyTorch, we would be using amp fron NVIDIA to directly manipulate the training loop to support 16-bit precision training which can be very cumbersome and time consuming. With PyTorch now adding support for mixed precision and with PL, this is really easy to implement."
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#tpu-support",
    "href": "posts/2020-07-12-oganized-pytorch.html#tpu-support",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "7 TPU Support",
    "text": "7 TPU Support\nFinally, we are down to my last promise of adding TPU support and being able to run this script on TPUs!\nHere’s a post by Google introducing TPUs and here is an excellent blogpost comparing various pieces of hardware. TPUs are typically 5 times faster than a V100 and reduce training times significantly.\nTo use a TPU, switch to Google Colab or Kaggle notebooks with free TPU availability. For more information on TPUs, watch this video by Google again.\nTo train your models on TPU on PL is again very simple, download the required libraries and add a parameter to the trainer. :)\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n\ntrainer = Trainer(gpus=1, max_epochs=2, \n                  num_sanity_val_steps=1 if debug else 0, \n                  accumulate_grad_batches=2, precision=16, \n                  tpu_cores=8)\nHere is a notebook by Abhishek Thakur for ISIC using TPUs with pure PyTorch. If you compare, you’d realise how easy it is now with PL to train on TPUs."
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#conclusion",
    "href": "posts/2020-07-12-oganized-pytorch.html#conclusion",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nSo I hope by now, you were able to compare the differences between PyTorch and PL and that I have convinced you enough to at least try out PL. [Here] is an excellent Kaggle competition to practice those skills and use PL! In the first few experiments with PL, I have found my work to be more streamlined and also I have noticed a reduction in bugs. I find it easier to experiment with different batch sizes, mixed precision, loss functions, optimizers and also schedulers. PL is definitely worth a try."
  },
  {
    "objectID": "posts/2020-07-12-oganized-pytorch.html#credits",
    "href": "posts/2020-07-12-oganized-pytorch.html#credits",
    "title": "An introduction to PyTorch Lightning with comparisons to PyTorch",
    "section": "9 Credits",
    "text": "9 Credits\nThanks for reading! And please feel free to let me know via twitter if you did end up trying PyTorch Lightning and the impact this has had on your experimentation workflows. Constructive feedback is always welcome.\n\nThe implementation of Model was adapted and modified from this wonderful notebook on Kaggle."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html",
    "href": "posts/2020-07-24-SeNet.html",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "",
    "text": "In this blog post, we will be looking at the Squeeze-and-Excitation networks. We will refer to the research paper by Hu et al and first understand what Squeeze-and-Excitation networks are before implementing the novel architecture in PyTorch with very few modifications to the popular ResNet architecture.\nFirst, we develop an intuition for what SE-Nets are and the novel idea behind their success. Next, we will look at the Squeeze and Excitation operations in a little more detail. Finally, we implement the Squeeze-and-Excitation networks in PyTorch with very minor updates to ResNet implementation in torchvision."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#intuition-behind-squeeze-and-excitation-networks",
    "href": "posts/2020-07-24-SeNet.html#intuition-behind-squeeze-and-excitation-networks",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "2 Intuition behind Squeeze-and-Excitation Networks",
    "text": "2 Intuition behind Squeeze-and-Excitation Networks\nSo, what’s new in the Squeeze-and-Excitation networks? How are they different from the ResNet architecture?\nLet’s consider an RGB image as an input. Then the convolution operation with a 3x3 kernel on the input image can be visualised as below:\n\nA feature map is generated per-channel (RGB) and then summed together to form one channel or the final output of the convolution operation as below:\n\nImplicitly, the convolution kernel would have different weights for different channels and these are learned weights through backpropagation. If it’s an RGB image, then generally the kernels are also cubic to map channel dependencies.\n\n2.1 Main Idea behind Se-Nets:\n\nWe expect the learning of convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able to increase its sensitivity to informative features which can be exploited by subsequent transformations. Consequently, we would like to provide it with access to global information and recalibrate filter responses in two steps, squeeze and excitation, before they are fed into the next transformation.\n\nIn other words, with the squeeze-and-excitation block, the neural nets are better able to map the channel dependency along with access to global information. Therefore, they are better able to recalibrate the filter outputs and thus, this leads to performance gains.\n\nThis main idea can be further explained using the Squeeze-and-Excitation block image above from the paper. First, a feature transformation (such as a convolution operation) is performed on the input image X to get features U. Next, we perform a squeeze operation to get a single value for each channel of output U. After, we perform an excitation operation on the output of the squeeze operation to get per-channel weights.\nFinally, once we have the per-channel weights, the final output of the block is obtained by rescaling the feature map U with these activations.\nFrom the paper: > The role this operation performs at different depths differs throughout the network. In earlier layers, it excites informative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to different inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network.\nNext, we will look at the Squeeze and Excitation operations in a little more detail."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#squeeze-global-information-embedding",
    "href": "posts/2020-07-24-SeNet.html#squeeze-global-information-embedding",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "3 Squeeze: Global Information Embedding",
    "text": "3 Squeeze: Global Information Embedding\nThe main purpose of the Squeeze operation is to extract global information from each of the channels of an image. Since, convolution is a local operation (that is, at a particular time, it is only able to see a part of the image), it might be beneficial to also extract information outside the receptive field of the convolution filter.\nThe Squeeze operation is meant to do exactly that and the authors keep it as simple as possible.\nThe authors perform a Global Average Pooling operation to reduce the C x H x W image to C x 1 x 1 to get a global statistic for each channel.\nFormally, the Global Average Pooling or the Squeeze operation can be formally represented as:\n\nIn other words, all we do is that we take the mean of each channel across H x W spatial dimensions."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#excitation-adaptive-recalibration",
    "href": "posts/2020-07-24-SeNet.html#excitation-adaptive-recalibration",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "4 Excitation: Adaptive Recalibration",
    "text": "4 Excitation: Adaptive Recalibration\nNow that we have a vector of length C from the Squeeze operation, the next step is to generate a set of weights for each channel. This is done with the help of Excitation operation explained in this section.\nFormally, the excitation operation can be represented by:\n\nwhere:\n- δ refers to ReLU operation\n- σ refers to Sigmoid operation\n- W1 and W2 are two fully-connected layers\n- z is the output from the Squeeze block\nThe two FC layers form a bottleneck architecture, that is, the first W1 layer is used for dimensionality reduction by a ratio r and the second W2 layer is a dimensionality increasing layer returning to the channel dimension of U.\nSince, the Sigmoid layer would return numbers between 0 and 1, these are the channel weights and the final output of the block can be respresented as:\n\nFrom the paper: > The excitation operator maps the input-specific descriptor z to a set of channel weights. In this regard, SE blocks intrinsically introduce dynamics conditioned on the input, which can be regarded as a self-attention function on channels whose relationships are not confined to the local receptive field the convolutional filter are responsive to.\nThe authors in the paper also mentioned the thinking and reasoning behind coming up with this excitation function. They write: > To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfil this objective, the function must meet two criteria: first, it must be flexible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised.\nThat is why using a Sigmoid layer makes so much sense rather than Softmax(which would generally impose importance on only one of the channels). A Sigmoid function (which is also used in multi-label classification) allows multiple channels to have higher importance."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#squeeze-and-excitation-block-in-pytorch",
    "href": "posts/2020-07-24-SeNet.html#squeeze-and-excitation-block-in-pytorch",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "5 Squeeze and Excitation Block in PyTorch",
    "text": "5 Squeeze and Excitation Block in PyTorch\nclass SE_Block(nn.Module):\n    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n    def __init__(self, c, r=16):\n        super().__init__()\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(c, c // r, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(c // r, c, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        bs, c, _, _ = x.shape\n        y = self.squeeze(x).view(bs, c)\n        y = self.excitation(y).view(bs, c, 1, 1)\n        return x * y.expand_as(x)\nAs mentioned the Squeeze operation is a global Average Pooling operation and in PyTorch this can be represented as nn.AdaptiveAvgPool2d(1) where 1, represents the output size.\nNext, the Excitation network is a bottle neck architecture with two FC layers, first to reduce the dimensions and second to increase the dimensions back to original. We reduce the dimensions by a reduction ratio r=16. This is as simple as creating a nn.Sequential with two FC layers, with a nn.ReLU() in between and followed by a nn.Sigmoid().\nThe outputs of the Excitation operation are the channel weights which are then multiplied element-wise to input feature X to get the final output of the SE_Block."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#se-block-with-existing-sota-architectures",
    "href": "posts/2020-07-24-SeNet.html#se-block-with-existing-sota-architectures",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "6 SE Block with Existing SOTA Architectures",
    "text": "6 SE Block with Existing SOTA Architectures\nFrom the paper: > The structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced. SE blocks are also computationally lightweight and impose only a slight increase in model complexity and computational burden.\nIn other words, it is really simple to integrate SE blocks with existing state-of-art architectures. The authors provide two examples for SE-ResNet and SE-Inception as below:\n\nIn this blogpost we will implement the SE-ResNet architecture and the SE-Inception architecture is left as an exercise to the reader.\nThe authors experimented by inserting SE block in various positions and found that the performance improvements produced by SE units are fairly robust to their location, provided that they are applied prior to branch aggregation. In this post, we will construct a SE-ResNet architecture using the Standard SE block integration as below:\n\nKey point to note: > As can be seen from the Standard SE block (b) integration, the SE block is preceded directly by the Residual operation in ResNet before summation with the identity branch."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#se-resnet-in-pytorch",
    "href": "posts/2020-07-24-SeNet.html#se-resnet-in-pytorch",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "7 SE-ResNet in PyTorch",
    "text": "7 SE-ResNet in PyTorch\nAs we saw from fig-6, we simply insert the SE block after the Residual operation. To create SE-Resnet34 and below, we simply copy the BasicBlock from torchvision from here.\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\nNext, we update the forward to insert the SE block operation as in fig-6:\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, r=16):\n        super(SEBasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n        # add SE block\n        self.se = SE_Block(planes, r)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        # add SE operation\n        out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\nAnd that’s it! As easy as it may sound, we have just implemented the SEBasicBlock in PyTorch.\nNext, for ResNet-50 and above, we perform the same steps for Bottleneck architecture. First, we copy the Bottleneck class from torchvision.\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\nNext, we add the SE_Block operation similar to what we did for the BasicBlock as below:\nclass SEBottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, r=16):\n        super(SEBottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        # Add SE block\n        self.se = SE_Block(planes, r)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        # Add SE operation\n        out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\nThat’s it! Now that we have implemented SEBasicBlock and SEBottleneck in PyTorch, we are ready to construct SE-ResNet architectures. As was mentioned in the paper,\n\nThe structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced.\n\nLet’s do exactly this. We simply replace the components of ResNet architecture with the SE counterparts.\nBut, one last step is to copy some helper functions from torchvision. These functions are present here:\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\ndef _resnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    return model\n\n7.1 SEResNet-18\nFrom torchvision here, we update the implementation of resnet18 to get se_resnet18:\ndef se_resnet18(pretrained=False, progress=True, **kwargs):\n    return _resnet('resnet18', SEBasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\n\n\n7.2 SEResNet-34\nFrom torchvision here, we update the implementation of resnet34 to get se_resnet34:\ndef se_resnet34(pretrained=False, progress=True, **kwargs):\n    return _resnet('resnet34', SEBasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\n7.3 SEResNet-50\nFrom torchvision here, we update the implementation of resnet50 to get se_resnet50:\ndef se_resnet50(pretrained=False, progress=True, **kwargs):\n    return _resnet('resnet50', SEBottleneck, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\n7.4 SEResNet-101\nFrom torchvision here, we update the implementation of resnet101 to get se_resnet101:\ndef se_resnet101(pretrained=False, progress=True, **kwargs):\n    return _resnet('resnet101', SEBottleneck, [3, 4, 23, 3], pretrained, progress,\n                   **kwargs)"
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#conclusion",
    "href": "posts/2020-07-24-SeNet.html#conclusion",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nIn this blogpost, first, we looked at what SE blocks are and the novel idea that they introduce. Next, we looked at the Squeeze and Excitation operation that is used to generate per-channel weights which are then used to return the final output of the SE block.\nFinally, we looked at integration of SE block in the ResNet architecture to construct se_resnet18, se_resnet34, se_resnet50 and se_resnet101.\nI hope that my explaination of SE Blocks was clear and as always - constructive feedback is always welcome at @amaarora.\nAlso, feel free to subscribe to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#credits",
    "href": "posts/2020-07-24-SeNet.html#credits",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "9 Credits",
    "text": "9 Credits\nThe implementation of SE_Block has been adapted from senet.pytorch repo here.\nAlso, thanks to my friends - Atmadeep Banerjee and Akash Palrecha - both research interns at Harvard - for proof reading the draft version of this blog post and providing me with an honest and constructive feedback.\nBoth Akash and Atmadeep found a technical error in my blog in the main idea behind SENet section where I had skipped the part that channel-dependencies are implicitly present when performing a convolution operation. Also, they helped me make this section better by helping me further improve my understanding of SeNets.\nAkash, was also very kind to point out a grammatical error and also it was his suggestion to add the two reasons mentioned in the paper for choosing the excitation function.\nBoth Akash and Atmadeep helped me in making this blog post better and more accurate. Thanks guys!"
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html",
    "href": "posts/2021-05-15-jh-meetup.html",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "",
    "text": "I am based in Sydney but flew to Brisbane to be a part of - Open Q&A with Jeremy Howard (of Fast.ai, Masks4All, Enlitic, Kaggle, FastMail), which was meant to be a live Q&A session with Jeremy. But, to my surprise, it rather turned out to be a talk by Jeremy where he not only shared his experience of founding/co-founding multiple tech startups - Fastmail, Optimal Decisions Group, Enlitic, Kaggle & fast.ai - and that of having lived in the SF bay area for around 10 years, but also shared some alarming data points about the state of AI in Australia today.\nHere are some key points that were discussed: - As per Stanford’s Artificial Intelligence Index Report 2021 - Australia invested around 0.29% of the total global AI investment in 2020. - Seed & angel investment per capita in Australia is an order of magnitude behind the US and has fallen for the past three years. [reference] - There is a lack of investment in research in Australia - it was less than the OECD average for the years 2005-2017. [reference] - Australian ICT share of global value added to the economy has fallen for the last 25 years and today, it is near the very bottom of the OECD behind Chile, Turkey, and other countries. [reference]\nAnd yet, despite the obvious issues, Jeremy thinks that Australia is a great place to start a tech startup - we’ll see in this blog how he comes to that conclusion.\nThroughout this blog, you will also find text that starts with \"AA:\" - these are my learnings and experiences that I share with you. The rest of this blog is the summary of the talk by Jeremy."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#australia-invested-a-meager-0.29-of-the-total-global-investment-in-ai-in-2020",
    "href": "posts/2021-05-15-jh-meetup.html#australia-invested-a-meager-0.29-of-the-total-global-investment-in-ai-in-2020",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "1 Australia invested a meager 0.29% of the total global investment in AI in 2020",
    "text": "1 Australia invested a meager 0.29% of the total global investment in AI in 2020\nThe talk began by stating a very scary statistic from Stanford’s Artificial Intelligence Index Report 2021 - “Australia invested a meager 0.29% of the total global AI investment in 2020”.\n\nBut, even after this, Jeremy starts by making an argument - “Australia is a great place to start a tech startup or do AI research despite the obvious issues”.\nInteresting, right?"
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#dont-listen-to-old-people",
    "href": "posts/2021-05-15-jh-meetup.html#dont-listen-to-old-people",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "2 Don’t listen to old people",
    "text": "2 Don’t listen to old people\nJeremy started his journey in consulting at Mckinsey & Company to learn about business and he also worked at other consulting companies along the way. This helped him learn some valuable skills around - “how to influence people” & “how to influence organizations” but the actual feedbacks on his initial ideas weren’t very positive.\nWhen he first presented an idea about building dynamic websites using collaborative filtering to one of the directors at a consulting firm, the feedback he received was this - > You have no idea how companies work. This isn’t a company. Companies are about competition, about market forces. This is nerdy technology.\nHe had similar feedback the second time around too for a different idea on using algorithms to find interesting websites based on user’s history.\nSo, based on his overall experience of learning business, he had his first piece of advice for potential people planning on doing tech startups -\n\nDon’t listen to old people. Unless, it’s explicitly about the thing that “you” want to do and they have years of experience doing that thing in the way “you” want to do it. Otherwise, “you” get these biases about business as usual without the status quo."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#in-australia-dont-try-to-be-an-australian-company",
    "href": "posts/2021-05-15-jh-meetup.html#in-australia-dont-try-to-be-an-australian-company",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "3 In Australia, don’t try to be an Australian company",
    "text": "3 In Australia, don’t try to be an Australian company\nAfter his initial gig at consulting which lasted around 10 years, Jeremy founded not one but two startups within 1 month of each other: 1. Fastmail 2. Optimal Decisions Group\nAs someone with a background in consulting, he didn’t know much about the Australian startup scene - and thus, didn’t have any government funding or grants. These were companies weren’t Australian startups but rather startups that happened to be in Australia!\nHe mentioned that since he had more access to global customers than Australian customers, it never occurred to him that these companies were bounded by geographical boundaries. These were two companies that were competing on a global stage!\nAA: I found this bit to be really interesting, as I also work for a similar company that I believe is also competing on a global stage - Weights & Biases. Even though the company was founded in San Francisco, it has clients worldwide. It’s solving problems that benefit the world and not just the US in general. Could this be the secret for founding successful startups?"
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#to-be-a-tech-founder-requires-this-weird-combination-of-great-humility-and-arrogance",
    "href": "posts/2021-05-15-jh-meetup.html#to-be-a-tech-founder-requires-this-weird-combination-of-great-humility-and-arrogance",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "4 To be a tech founder requires this weird combination of great humility and arrogance",
    "text": "4 To be a tech founder requires this weird combination of great humility and arrogance\nFor the next startup that Jeremy co-founded alongside Anthony Goldbloom - Kaggle, they went to San Francisco to try and get funding.\nJeremy stated: > “With Kaggle we decided to try a different approach which was to get VC funding. I said to Anthony who we’re doing this with, let’s not even try to get funding in Australia because Australia doesn’t fund tech startups. The amount of funding of startups in Australia in a year is less than the amount of funding of startups in the US in a day. So we went to San Francisco to try and get funding.”\nIn San Francisco, Jeremy had a very different experience than that in Australia. He shared examples of how Mark Andreson and Vinod Khosla were ready to invest five million dollars when in fact, he had initially thought to seek five hundred thousand dollars of funding. He observed a very different theme in the bay area, which was, quote:\n\nEvery time we’d say we want to do X, people would say “Woah, okay, that’s great. What if you could make an even bigger X”?\n\nThis was a very different experience from what Jeremy had found doing his startups in Australia: > Oh, I am trying to create an email company that does like synchronized email and I am trying to sell it on the internet and almost everybody would say like “Why”? Microsoft & Yahoo already have an email service. Obviously, there is no chance you can beat them, so why are you doing this? Is there something smaller you could do?\nJeremy said: > “To be a tech founder requires a whole lot of arrogance. You need the arrogance to believe that you can build something that other people are going to want to buy and the other people that will come along and compete with you won’t do as good as you, and you can win! But you also need the humility to recognize that other people come along and they can have some better ideas than you so sometimes you should borrow those ideas, or sometimes you should try and find ways to do it better. So it requires this weird combination of great humility and arrogance.”\nJeremy also compared Australia to the bay area, below, I summarize:\n\n\n\n\n\n\n\n\n\nUS\nAustralia\n\n\n\n\nTheme\nWhat if you could do an even bigger or better “X”?\nIs there something smaller that you could do?\n\n\nCultural difference\nOh, it’s really cool that you’re trying to do something better!\nWhy aren’t you OK with what’s already there? Why build something new?\n\n\nVCs - Academics\nVCs would call academics and ask for their opinion on new ideas.\nYou don’t see this strong connection between investors and academics."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#moving-to-san-francisco-was-interesting",
    "href": "posts/2021-05-15-jh-meetup.html#moving-to-san-francisco-was-interesting",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "5 Moving to San Francisco was interesting",
    "text": "5 Moving to San Francisco was interesting\nAfter raising money for Kaggle, Jeremy moved to San Francisco.\nIn the US, Jeremy was starstruck - he would notice that meetups are hosted at Google or Facebook. He would be one day talking to a Google product manager and would feel very starstruck. But the other thing he noticed was, that while he was talking to these legends, they’re actually “pretty normal”! Jeremy said:\n\nI kind of expected them to be at another level. I felt that as a little Australian nobody, I would just be dominated by these people. But, no! When I compared them to my mates back in Australia, I kind of realized that the Australian talent pool is just fantastic. But, there’s this huge difference in opportunity and belief.\n\nAA: At this point, I have to say that after hearing this, I got very excited! It’s because I remember saying the same thing a few months ago when I was looking for a job in deep learning in Australia! I would compare myself to my friends from university who moved to the US and are now working at giants like Facebook, Google, Microsoft, and because I knew them, I was like they’re no different than me! Then, why in Australia do I “need to have a Ph.D.” to work at these companies? Why can’t I at least be given a chance to interview like my peers in the US? Also, why are there like 5 job vacancies for Amazon in tech here in Australia whereas in India or the US there are 500 if not 5,000? In this process of finding a job, I was only able to find a small handful of companies that were actually using deep learning in their products. I decided to join one of them, and it wasn’t a very pleasant experience. Most if not all of my time was spent on doing software engineering (and not research) until I decided to quit. Finally, like my peers, I also ended up working for a US company.\nThis experience in the US gave Jeremy more self-confidence as an Australian as he realized that “Australians are not way behind”.\nJeremy also found that folks in the U.S were on the whole bolder and even though they were in the center of the world’s biggest marketplace, they were still actually more global! He stressed that “We need that attitude in Australia”."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#around-2012-i-started-noticing-deep-learning-starting-to-win-things",
    "href": "posts/2021-05-15-jh-meetup.html#around-2012-i-started-noticing-deep-learning-starting-to-win-things",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "6 Around 2012, I started noticing deep learning starting to win things",
    "text": "6 Around 2012, I started noticing deep learning starting to win things\nIn this part of the talk, Jeremy shared about the way he felt and still feels about deep learning.\nSince Jeremy was a chief scientist and president at Kaggle, he got to validate the winning solutions. So, he was always really seeing what are the actual best ways to do things.\nAnd around 2012, Jeremy started noticing deep learning to win things or at least do pretty well! But, one of Jeremy’s real concerns was, quote: > People using these neural nets were all the same person. They were from 1 of 5 universities that were all very exclusive. They were all white, they were all male and they were all solving stupid problems like trying to find their cats in their photos. I mean look okay it’s nice to find cats in your photos, people make a lot of money from that. But, where were the people trying to deal with global water shortages or access to education or dealing with huge economic inequity - it wasn’t on the radar.\nJeremy said that he knew that this was because it wasn’t a diverse set of people solving the problems. He felt that perhaps there is something he could do about this.\nHe shared that he feels about deep learning to be a fundamental technology like “electricity” and also shared examples to showcase the power of what deep learning can do today by showing DALL-E from OpenAI and AlphaFold from Deepmind.\nBelow, I also share DALL-E from OpenAI which is a deep learning model, that can generate images given some text input:\n\nTo someone who doesn’t know what’s going on, this feels like “magic”. Jeremy said that there are people who feel like deep learning is some technology on another level that only a select people can do. And he completely disagrees with this.\nJeremy also mentioned that deep learning is just kind of one algorithm with little changes that go between one model and another. When Jeremy looked at the source code for the AlphaGo model, he found that the model was almost identical to the computer vision object recognition models that Jeremy had previously used."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#no-one-was-doing-deep-learning-in-medicine",
    "href": "posts/2021-05-15-jh-meetup.html#no-one-was-doing-deep-learning-in-medicine",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "7 No one was doing deep learning in medicine",
    "text": "7 No one was doing deep learning in medicine\nJeremy realized that there actually was some low-hanging fruit at that time with deep learning and it was actually “medicine”. He said: > No one was doing deep learning in medicine.\nHe found that, globally, there is such a shortage of medical specialists and doctors, that, according to the world economic forum, it’s going to take 300 years to fill in the gap to allow the developing world to have access to the same, medical expertise as a developed world.\nStunned by this statistic, Jeremy thought maybe there is something he could do to help the doctors be more productive by adding deep learning to their workflow. He, along with three others, spent 4 weeks training a deep learning model on lung CT scans which, to their surprise, turned out to have much lower false positives and false negatives when compared to a panel of 4 radiologists.\nThis turned out to be his next startup called Enlitic. Jeremy mentioned that this turned out to be both great and disappointing. It turned out great because as he had envisioned, it helped put deep learning on the map for medicine and it is now everywhere in radiology. But, he still felt he was doing so little when there were so many other problems to be solved."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#to-do-deep-learning-you-dont-need-a-p.h.d",
    "href": "posts/2021-05-15-jh-meetup.html#to-do-deep-learning-you-dont-need-a-p.h.d",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "8 To do deep learning, you “don’t” need a P.h.D",
    "text": "8 To do deep learning, you “don’t” need a P.h.D\nJeremy felt that there are all these wonderful people in the world solving important problems like disaster resilience, or access to food and they don’t have a way to tap into this extremely powerful tool - “deep learning”.\nJeremy said: > So between this and this kind concern about inequality, and the kind of exclusivity and the kind of homogenous group of people working on deep learning, Rachel and I decided to start something new - fast.ai.\nJeremy also said: > fast.ai is all about helping everybody do what Enlitic is doing but not having a bunch of deep learning people do it, but to have disaster resilience built by disaster resilience people and have oncology stuff built by oncology people.\nAA: And I am so glad for the fast.ai course! Had it not been for fast.ai, I believe I would have paid a fortune to learn deep learning and it would possibly have taken me a lot longer to understand the technology. I recently also wrote a tweet about “Why I blog” which included how fast.ai was at the center of all my learnings. So, thanks Jeremy and Rachel for doing this! For those of you that don’t know about fast.ai, it’s most notably known for the fastai library and the fast.ai course, which is based on a top-down approach where you build image classification models first without actually understanding how they work. Then, you slowly go into the details until you know everything that there is to know. This approach just works!\nJeremy also mentioned that through fast.ai, they’ve till date trained hundreds of thousands of people, and every few days people share how fast.ai has helped them publish new papers in a top journal, get a job or bring deep learning to their respective startups.\nAA: This is wonderful to hear as I am also definitely one of those people who have greatly benefited from the fast.ai course."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#suprised-at-how-little-had-changed-in-australia",
    "href": "posts/2021-05-15-jh-meetup.html#suprised-at-how-little-had-changed-in-australia",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "9 Suprised at how little had changed in Australia",
    "text": "9 Suprised at how little had changed in Australia\nA few months ago, Jeremy came back to Australia and mentioned that coming back to Australia has been “amazing”!\nFor those of you that don’t know, Jeremy grew up in Australia. And he mentioned that having lived in Australia his whole life, he kind of always had this vague sense that Australia had a great working culture, and after spending 10 years in America, ” coming back to Australia was like this huge breath of fresh air”.\nBut, Jeremy was also surprised at how little had changed since he had left. He was trying to find people doing world-class deep learning research or building startups having a huge global impact and he wasn’t able to find it. He also presented some alarming data and charts related to this that were shared with Jeremy by Michael Evans:\n\nAs can be seen from figure-3 above, from an investing point of view, the seed of angel investment in Australia per capita is an order of magnitude behind the US. This basically means that tech founders and entrepreneurs in Australia have 10 times less money per person compared to their peers in the US in terms of early seed investment! This makes it really hard for entrepreneurs in Australia to get things going.\nSimilarly, he also shared some more data on investment activity and lack of investment in research. In Australia, the average investment activity in AI averages around $20M a year.\nContrast this with Japan’s 2018 AI budget totalled which totalled $1 billion, Singapore’s five-year funding at $151 million, Taiwan’s annual budget for the Taiwan AI action plan at $448 million, France’s $2.4 billion allocation toward AI. Also, while the US strategy does not provide a federal number on investment, the nation had $8.3 billion in AI VC investment in 2017, unclassified AI R&D by the federal government reached $2.19 billion in the same year (up 40% from 2015), MIT’s $1.38 billion AI centre, DARPA invested $2.8 billion in AI, and the National Science Foundation provided over $146 million each year in support of AI research. [reference]\nAs shown in figure-4 below, not only is Australia’s investment in research compared to OECD average worse, but it is getting worse.\n\nJeremy also shared that Australia’s ICT share of global value added to the economy has fallen over the last 25 years! As shown in figure-5 below, this ICT share is plumetting and Australia is near the very bottom of the OECD countries, behind Chile, Turkey, and other countries.\n\nJeremy mentioned that these were data points that reflect what he was seeing - and I have to agree - being in Australia, you do feel that there is this very limited number of people and organizations investing in technology.\nFinally, he also presented a Deloitte study as in figure-6 below:\n\nAs per this study, on being asked “why are you interested in AI?”, 50% of Australian enterprises said it’s because they want to catchup or keep up and only 22% said that they want to get ahead. This is worse than every other country that Deloitte spoke to.\nJeremy also said: > Australian customers are so conservative, that if you want to sell something to enterprises in Australia, you have to tell them that their competitors have already bought it. If you say “you could use this to power ahead of your field and become a global success story” - they don’t care. I don’t know exactly why this is, but it’s true in the data and it’s absolutely true from all of my experience."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#australia-ranks-right-at-the-top-in-use-of-tech",
    "href": "posts/2021-05-15-jh-meetup.html#australia-ranks-right-at-the-top-in-use-of-tech",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "10 Australia ranks right at the top in “use of tech”",
    "text": "10 Australia ranks right at the top in “use of tech”\n\nHaving said that, as shown in figure-7 above, in OECD, Australia ranks at the top in terms of “use of tech”!. And yet, Australia ranks right at the bottom when in terms of investment in technology.\nAnd this is actually why Jeremy thinks Australia is a great place to build a startup. Jeremy said: > If you can get past all this stuff pulling you down, in Australia, you’re in a place where you’re surrounded by brilliant people. They don’t have other cool tech startups on the whole. It’s not that there’s none, but relatively very few.\nJeremy said that he believes there are some great opportunities here in Australia and also shared some advice for building a tech/AI startup: > Do it in as un-Australian a way as possible: “you” don’t have to have Australian investors or customers. Just believe that “you” can put something on the internet that people are going to buy.\nJeremy feels that this is the secret to having some great startups in Australia and also ended the talk by sharing an example of how things are starting to change in Adelaide.\nLastly, Jeremy said: > The most important thing is the kind of raw foundation that we have which I think is kind of the best in the world. We want to help in any way we can to change Australia from a diamond in the rough to a glowing diamond that everybody around the world knows about."
  },
  {
    "objectID": "posts/2021-05-15-jh-meetup.html#the-end",
    "href": "posts/2021-05-15-jh-meetup.html#the-end",
    "title": "The sad state of AI and tech startups in Australia today and what can we do about it",
    "section": "11 The end",
    "text": "11 The end\nAA: As someone who was in the audience, I must say it was quite a pleasure to hear Jeremy share his journey. There’s plenty in there to learn from and I am glad I got to witness it live and also got a chance to meet him. I am also secretly very happy that he’s in Australia and hope that this will change the state of AI investment here. I am also happy to do my bit and help in any way I can."
  },
  {
    "objectID": "posts/2021-01-11-efficientdet.html",
    "href": "posts/2021-01-11-efficientdet.html",
    "title": "EfficientDet - Scalable and Efficient Object Detection",
    "section": "",
    "text": "First of all, a very happy new year to you! I really hope that 2021 turns out to be a lot better than 2020 for all of us. 2020 was the first year when I started working as an Applied AI Scientist in the Medical field - my first ever deep learning job! In 2021 I hope to build upon this platform and introduce many more key research papers in a simple and easy to grasp manner. This blog post is the first of many more blogposts to be released this year.\nToday, we will be looking at the EfficientDet: Scalable and Efficient Object Detection research paper. With single-model and single-scale, EfficientDet-D7 was able to achieve SOTA results at the time of release of the paper. Even after a year later, at the time of writing, the results are still in the top-5 positions on the COCO leaderboard. Also, recently, in the NFL 1st and Future - Impact Detection Kaggle competition, EfficientDets figured in almost all of the top winning solutions. So, in this blog post we will be uncovering all the magic that leads to such tremendous success for EfficientDets.\nAlso, while multiple blog posts previously exist on EfficientDets [1, 2, 3..], there isn’t one that explains how to implement EfficientDets in code. In a later blog post that follows this one, I will explain how to implement EfficientDets in PyTorch. The implementation would be directly copied from Ross Wightman’s excellent repo efficientdet-pytorch and the code from the repo used as reference to explain how to implement EfficientDets in PyTorch.\nBut, code comes later. First, let’s understand the novel contributions of the EfficientDet paper."
  },
  {
    "objectID": "posts/2021-01-11-efficientdet.html#structure",
    "href": "posts/2021-01-11-efficientdet.html#structure",
    "title": "EfficientDet - Scalable and Efficient Object Detection",
    "section": "1 Structure",
    "text": "1 Structure\nThe blog post is structured in the following way:\n\nTOC {:toc}\n\nFirst, we take a step back and understand multi-scale feature fusion before the introduction of Feature Pyramid Networks. Next, understand the key advancements introduced by FPNs. Next, we look at the various adaptations of FPN such as PANet, NAS-FPN and finally look into BiFPN that was introduced in the EfficientDet paper.\nHaving looked at BiFPN in great detail, we then finally look into the EfficientDet Architecture and understand how the authors used BiFPN + Compound Scaling to get SOTA results."
  },
  {
    "objectID": "posts/2021-01-11-efficientdet.html#prerequisites",
    "href": "posts/2021-01-11-efficientdet.html#prerequisites",
    "title": "EfficientDet - Scalable and Efficient Object Detection",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nI assume that the reader has some knowledge about Object Detection. If you are completely new to the field or simply want to apply EfficientDets to an object detection problem, there are plenty examples on Kaggle that show how to use EfficientDets. This post is more meant for those who want to understand what’s inside an EfficientDet.\nAlso, here is a great video of the fastai 2018 course by Jeremy Howard that introduces object detection. I started here too about a year ago. :)\nAlso, since it would be an overkill to put EfficientNets as a prerequisite, I will just say that it would be great if the reader has a good enough understanding of EfficientNets. If you want a refresher, please refer to this blog post that explains EfficientNets in detail step-by-step."
  },
  {
    "objectID": "posts/2021-01-11-efficientdet.html#contributions",
    "href": "posts/2021-01-11-efficientdet.html#contributions",
    "title": "EfficientDet - Scalable and Efficient Object Detection",
    "section": "3 Contributions",
    "text": "3 Contributions\nThere are two main contributions from the paper: 1. A new version of a Feature Pyramid Network called BiFPN. 2. And two, Compund Scaling.\nWhile the idea of compound scaling was first introduced in the EfficientNet paper, the authors apply it to object detection and achieve SOTA results. Also, that the authors of the EfficientDet research paper are the same authors who introduced EfficientNets. We won’t be looking into Compound Scaling in this blog post as it has already been explained in my previous blog post here.\nWe will be looking at what the BiFPN network is in a lot more detail at a later stage. To understand what a BiFPN is, one must first look into what a FPN (Feature Pyramid Network) is."
  },
  {
    "objectID": "posts/2021-01-11-efficientdet.html#introduction",
    "href": "posts/2021-01-11-efficientdet.html#introduction",
    "title": "EfficientDet - Scalable and Efficient Object Detection",
    "section": "4 Introduction",
    "text": "4 Introduction\nThis paper starts out with a similar introduction as EfficientNets where the authors explain why model efficiency becomes increasingly important for object detection. From the paper: > Tremendous progresses have been made in recent years towards more accurate object detection. meanwhile, state-of-the-art object detectors also become increasingly more expensive. For example, the latest AmoebaNet-based NASFPN detector requires 167M parameters and 3045B FLOPs (30x more than RetinaNet) to achieve state-ofthe-art accuracy. The large model sizes and expensive computation costs deter their deployment in many real-world applications such as robotics and self-driving cars where model size and latency are highly constrained. Given these real-world resource constraints, model efficiency becomes increasingly important for object detection.\nThe key question that this paper tries to solve is “Is it possible to build a scalable detection architecture with both higher accuracy and better efficiency across a wide spectrum of resource constraints”?\nThe authors identified two main challenges when it comes to answering this question: 1. Efficient multi-scale feature fusion 2. And two, Model Scaling\nTo explain multi-scale feature fusion, we will need to take a step back here and before continuing with the EfficientDet paper, we will have to first look into Feature Pyramid Networks and understand the key idea behind them. But basically, multi-scale feature fusion aims to aggregate features at different resolutions.\nSo, the next section is merely a summary and introduction to Feature Pyramid Networks. Feel free to skip if you already know what a FPN is.\n\n\n4.1 Summary: Feature Pyramid Network for Object Detection\nHe et al were the one of the first to exploit the inherent multi-scale pyramid heirarchy of CNNs and construct feature pyramids and apply to object detection. If this doesn’t make sense right now - it’s okay! I really havent explained in much detail what this means yet. That happens next.\nRecognizing objects at vastly different scales is a fundamental challenge in computer vision. Different authors have tried to solve this differently. However, there are three main categories (total four including FPN) of solutions that existed before the introduction of FPN.\n\n\n4.1.1 Featurized Image Pyramid\nThis is the first way and possibly the simplest to understand to recognize objects at different scales. Given an input image, resize the image to using different scales, pass the original image and the resized images through a CNN, make a prediction at each scale and simply take the average of these predictions to get a final prediction. Intuitively, this enables a model to detect objects accross a large range of scales by scanning the model over both positions and pyramid levels. Chris Deotte explains this too in simple words here.\nBut, can you think of the possible problems with this possible solution?\nFor one, inference time would increase. For each image, we would need to rescale it to various new sizes and then average the predictions. Second, it would also not be possible to do this during train time as this would be infeasible in terms of memory and hardware requirements. Therefore, featurized image pyramid technique can only be used at test time which creates an inconsistency between train/test time inference.\n\n\n4.1.2 Single Feature Map (Faster RCNN)\nAnother way is to use the inherent scale invariance property of CNNs. As you know, during the forward pass, a deep CNN computes a feature heirarchy layer by layer, and therefore, has an inherent multi scale pyramidal shape. See VGG-16 network below as an example:\n\nThe later layers are much smaller in spatial dimensions compared to the earlier layers. Thus, the scales are different.\nTherefore, one could just accept an original image, do a forward pass through a CNN, and get bouding box and class predictions just using this single original scaled image making use of the inherent scale invariance property of CNNs. In fact, this is exactly what was done in the Faster RCNN research paper.\n\nAs can be seen in the image above, given an input image, we pass it through a CNN to get a 256-d long intermediate representation of the image. Finally, we use cls layer and reg layer to get classification and bounding box predictions in Faster RCNN method. This has been also explained very well by Jeremy in the video I referenced before.\nCan you think of why this might not work? Maybe take a break to think about the possible reasons why this won’t work.\nAs mentioned by Zeilur and Fergus in Visualizing and Understanding Convolutional Networks research paper, we know that the earlier layers of a CNN have low-level features whereas the deeper layers learn more and thus, have high-level features. The low-level features (understanding) in earlier layers of a CNN harm their representational capacity for object recognition.\n\n\n4.1.3 Pyramidal Feature Heirarchy (SSD)\nA third way could be to first have a stem (backbone) that extracts some meaning from the image and then have another convolutional network head on top to extract the features and perform predictions on each of the extracted features. This way, we do not need to worry about the representational capacity of earlier layers of a CNN. This sort of approach was introduced in the SSD research paper.\n\nAs can be seen in the image above, the authors of the research paper used earlier layers of VGG-16 (until Conv5_3 layer) to extract some meaning/representation of the image first. Then, they build another CNN on top of this and get predictions at each step or after each convolution. Infact, the SSD was one of the first attempts at using CNNs pyramidal feature heirarchy as if it were a featurized image pyramid.\nBut can you think of ways to improve this? Well, to avoid using low-level features from earlier layers in a CNN, SSD instead builts the pyramid starting from high up in the network already (VGG-16) and then adds several new layers. But, while doing this, it misses the opportunity to reuse the earlier layers which are important for detecting small objects as shown in the FPN research paper.\n\n\n4.1.4 Feature Pyramid Network\nSo, finally to the Feature Pyramid Network. Having had a look at all the other approaches, now we can appreciate what the FPN paper introduced and why it was such a success. In the FPN paper, a new architecture was introduced that combines the low-resolution, semantically strong features in the later layers with high-resolution, semantically weak features in the earlier layers via a top-down pathway and lateral connections. Thus, leading to Multi-scale feature fusion.\nThe result is a feaure pyramid that has rich semantics at all levels because the lower semantic features are interconnected to the higher semantics. Somewhat similar idea to a U-Net. Also, since the predictions are generated from a single original image, the FPN network does not compromise on power, speed or memory.\n\nGreat, so now if someone woke you up while you were sleeping and asked “what are the main contributions in the Feature Pyramid Network research paper?” You should be able to say: > In a feature pyramid network, the authors used the inherent pyramidal structure and multi-scale property of a CNN and used top-down and lateral connections to connect high semantic layers (later layers of a CNN) with low semantic layers (earlier layers of a CNN) such that a high-level semantic feature maps exists at all scales and thus leading to better multi-scale object detection from a single sized original image.\n\n\n\n4.2 BiFPN: Bi-directional Feature Pyramid Network\nNow that you know what a FPN is, in this section we will look into one of the main contributions from the EfficientDet research paper and understand what’s a BiFPN. In fact, as mentioned, the authors identified multi-scale feature fusion as one of the key challenges that must be solved to create an Efficient object detector. So how do you achieve that? What are the various ways one could do multi-scale feature fusion?\nWell, we just looked at some in the previous section that existed before FPNs were introduced. As you can imagine, FPNs became the default go to network for a while after their introduction for multi-scale feature fusion. Recently, PANet, NAS-FPN and other studies developed more network structures for cross-scale feature fusion.\nThe EfficientDet research paper has an excellent image on Feature Network Design that shows the various variations below:\n\n\n4.2.1 FPN\nWe already know what a FPN is, the above figure is just a different way to show what we have already learnt in the previous section. As we know, FPN introduces top-down and lateral connections to combine multi-scale features. One thing worth mentioning is that while fusing different features, FPNs simply sum them up without disctinction.\nLooking at fig-5 (a), we can see that P7out and other output features can be calculated as:\n\nwhere, Resize is usually a upsampling or downsampling operation for resolution matching, and Conv is usually a convolution operation for feature processing.\n\n\n4.2.2 PANet\nAround 3 years after the FPNs were first introduced, PANet adds ab extra bottom-up path aggregation on top of FPN since the conventional FPN is inherently limited by the one-way information flow. The authors of PANet were able to enhave the feature heirarchy by not only sharing information from top-bottom layers but also by bottom-up path augmentation. Using this simple improvement over FPN, the PANet was able to achieve 1st place of COCO 2017 Challenge Instance Segmention Task and 2nd place in Object Detection Task without large-batch training.\n\n\n4.2.3 NAS-FPN\nMore recently, researchers from google brain adopted Neural Architecture Search and aimed to discover a new feature pyramid architecture in a scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consisted of a combination of top-down and bottom-up connections to fuse features across scales. Although NAS-FPN ahieved better performance, it requires thousands of GPU hours during search, and the resulting feature network as shown in fig-5 (c) is difficult to interpret.\n\n\n4.2.4 Bi-FPN\nNow that we have looked at various ways of multi-scale feature fusion, we are now ready to look into BiFPN. To get BiFPN network, the authors of the EfficientDet paper proposed several optimizations:\n\nRemove all those nodes that have only one input edge. The intuition is that if a node has only one input edge with no feature fusion, then it will have less contribution to the feature network.\nAdd an extra edge from the original input to the output node if they are at the same level in order to fuse more features without adding much cost\nTreat each bidirectional path as one single layer and have multiple of these to enable more high-level feature fusion.\n\nI believe that the optimizations 1,2 would be clear to the reader but perhaps not 3. That’s okay - 3 should become clearer once we look at the EfficientDet Architecture in the next section.\nThere were also other notable optimizations that were introduced with the introduction of BiFPN such as Weighted Feature Fusion. As mentioned before and as seen in eq-1, when fusing features with different resolutions, a common way is to first resize them to the same resolution and then simply sum them up treating all input features equally without distinction.\nHowever, since different input features are at different resolutions, it is possible that they might contribute differently to the output feature. Thus, the authors introduced an additional weight for each input in Bi-FPN to address this issue and this is referred to as Weighted Feature Fusion. The weights are trained using backward propagation by the network. I won’t cover Fast-normalized fusion in this blog but it is merely a small improvement over Softmax-based fusion to avoid slowdown of GPU hardware by the extra softmax layer.\n\n\n\n4.3 Compound Scaling\nThey key idea of compound scaling has already been introduced here. While previous object detection frameworks relied on bigger backbone networks or larger input image sizes, for higher accuracy, the authors of the EfficientDet paper observed that scaling up feature network and box/class prediction network is also critical when taking into account both accuracy and efficiency,\nThus, by combining EfficientNet backbones with the proposed BiFPN feature fusion, a new family of object detectors EfficientDets were developed which consistently achieve better accuracy with much fewer parameters and FLOPs than previous object detectors."
  },
  {
    "objectID": "posts/2021-01-11-efficientdet.html#efficientdet-architecture",
    "href": "posts/2021-01-11-efficientdet.html#efficientdet-architecture",
    "title": "EfficientDet - Scalable and Efficient Object Detection",
    "section": "5 EfficientDet Architecture",
    "text": "5 EfficientDet Architecture\nAs mentioned earlier, the two main contributions of the EfficientDet architecture were: 1. BiFPN 2. Compound Scaling\nFrom the EfficientNet paper, we already know that compound scaling simply refers to scaling up all fimensions such as backbone, input image size, network width and depth at the same time to achieve maximum performance. For more information, refer here.\nAnd we have also already looked at the BiFPN network - thus, we are now ready to cover off the EfficientDet Architecture. We have done all the hard work and not much is left in understanding the EfficientDet Architecture. If you have followed and understood my blog so far, I am sure that you will find the remaining part fairly easy to grasp.\n\nTo get the EfficientDet Architecture, the authors simply used the ImageNet pretrained EfficientNet as the backbone network. This backbone extracts features from the Input P1-P7 where Pi represents feature level with resolution of 1/2i of the input image. For instance, if the input resolution is 640x640 then P3in represents feature level with resolution 80x80.\nThe proposed Bi-FPN accepts the P3-P7 features as input features and repeatedly applies cross-scale multi-scale fusion on these features as shown in fig-6 to get a multi-scale feature representation of the image which is then fed to Class predciction net and Box prediction net to finally get class and bounding box outputs.\nThis is it really."
  },
  {
    "objectID": "posts/2021-01-11-efficientdet.html#conclusion",
    "href": "posts/2021-01-11-efficientdet.html#conclusion",
    "title": "EfficientDet - Scalable and Efficient Object Detection",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nI hope that today, I was able to help with your understanding of EfficientDets. In case, you haven’t understood something - that’s okay. Please wait for my next blog that explains how to implement the EfficientDet Architecture in PyTorch. By then, every small detail should become crystal clear.\nAs usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html",
    "href": "posts/2020-02-18-annotatedGPT2.html",
    "title": "The Annotated GPT-2",
    "section": "",
    "text": "Welcome to “The Annotated GPT-2”.\nOne of the most brilliant and well-explained articles I have ever read is The Annotated Transformer. It introduced Attention like no other post ever written. The simple idea was to present an “annotated” version of the paper Attention is all you need along with code.\nSomething I have come to realize with my little experience in Machine Learning, when you write things in code, the implementation and the secrets become clearer. It is not magic anymore.\n\nThere is nothing magic about magic. The magician merely understands something simple which doesn’t appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can “do magic.”\n– Jeffrey Friedl in the book Mastering Regular Expressions\n\nThe GPT-2 might seem like magic at first with all it’s glitter and beauty too, but hopefully I would have uncovered that magic for you and revealed all the tricks by the time you finish reading this post. That is my goal. To make it as simple as possible for the keen to understand how the GPT-2 model works underneath.\nNote: Pretty much the entirety of the code has been copied, inspired and referenced from Hugging Face’s implementation of the GPT-2, keeping merely the essentials for simplicity. If you want to train the GPT-2 model on parallel GPUs, save checkpoints while fine-tuning, run inference tasks on multiple CPUs and much more, I would recommend using the Hugging Face API. A simple tutorial on how to do so was recently released by Hugging Face and can be found here.\nIn this post, I am not trying to reinvent the wheel, but merely bringing together a list of prexisting excellent resources to make it easier for the reader to grasp GPT-2. I leave it up to the reader to further build upon these foundations in any area they choose.\n\nYou can’t build a great building on a weak foundation. You must have a solid foundation if you’re going to have a strong superstructure.\n– Gordon B. Hinckley"
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#prerequisites",
    "href": "posts/2020-02-18-annotatedGPT2.html#prerequisites",
    "title": "The Annotated GPT-2",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nThis post assumes that the reader has a solid understanding of Attention and Transformers. The GPT-2 utilizes a 12-layer Decoder Only Transformer architecture. If you want a refresher or understand Attention and Transformers, here is an excellent list of resources to aid your understanding regarding:\n\nThe illustrated Transformer by Jay Alammar\nThe Annotated Transformer by Harvard NLP\nIntroduction to the Transformer by Rachel Thomas and Jeremy Howard\n\n\nIf you’re just beginning your journey into NLP or you’re an expert, I would definitely recommend the fast.ai NLP course taught by Rachel Thomas and Jeremy Howard. The course starts with the basics including Sentiment Classification using Naive Bayes and Logistic Regression, moves on to RNNs and also talks about Transfer Learning, ULMFiT, Seq2Seq translation and Transformers amongst other things. It is an excellent resource put together by the fast.ai team free of cost.\nAnother amazing resource on GPT-2 itself, is The Illustrated GPT-2 by Jay Alammar. This post starts with a basic introduction to Language Models and explains the GPT-2 model step-by-step in a very easy to understand manner. I would highly recommend the reader to give this post a read.\nThe Annotated Transformer by Harvard NLP implements the complete Transformer architecture using PyTorch and is great way to understand Attention in depth.\nLet’s then build upon these excellent existing resources and implement GPT-2 in code."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#abstract",
    "href": "posts/2020-02-18-annotatedGPT2.html#abstract",
    "title": "The Annotated GPT-2",
    "section": "3 Abstract",
    "text": "3 Abstract\nNatural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\n\nA Zero-shot setting is one where you do not finetune the language model and directly run inference on the target dataset. For example, pretrain a LM on WebText and directly try and predict the next words of Amazon Movie reviews dataset."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#model-architecture-gpt-2",
    "href": "posts/2020-02-18-annotatedGPT2.html#model-architecture-gpt-2",
    "title": "The Annotated GPT-2",
    "section": "4 Model Architecture (GPT-2)",
    "text": "4 Model Architecture (GPT-2)\nWe use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers. The vocabulary is expanded to 50,257 words. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used.\n\nThis is the entirety of model explanation inside the GPT-2 research paper. This warrants a need for us to look at the architecture inside the GPT model."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#model-specifications-gpt",
    "href": "posts/2020-02-18-annotatedGPT2.html#model-specifications-gpt",
    "title": "The Annotated GPT-2",
    "section": "5 Model Specifications (GPT)",
    "text": "5 Model Specifications (GPT)\nOur model largely follows the original transformer work. We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in, with w = 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU).\n\n\nAs can be seen from the GPT Architecture, to implement it, we will first need to implement Masked Self Attention and Feed Forward layer."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#imports",
    "href": "posts/2020-02-18-annotatedGPT2.html#imports",
    "title": "The Annotated GPT-2",
    "section": "6 Imports",
    "text": "6 Imports\nimport torch\nimport copy\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules import ModuleList\nfrom torch.nn.modules.normalization import LayerNorm\nimport numpy as np\nimport os\nfrom tqdm import tqdm_notebook, trange\nimport logging\nlogging.basicConfig(level = logging.INFO)\nlogger = logging.getLogger()"
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#transformer-decoder-inside-gpt-2",
    "href": "posts/2020-02-18-annotatedGPT2.html#transformer-decoder-inside-gpt-2",
    "title": "The Annotated GPT-2",
    "section": "7 Transformer Decoder inside GPT-2",
    "text": "7 Transformer Decoder inside GPT-2\nTo re-use the terminology used to describe the Transformer, the attention is a function of a query (Q) and set of key (K) and value (V) pairs. To handle longer sequences, we modify the multi-head self-attention of the Transformer to reduce memory usage by limiting the dot products between Q and K in:\n\nclass Conv1D(nn.Module):\n    def __init__(self, nx, nf):\n        super().__init__()\n        self.nf = nf\n        w = torch.empty(nx, nf)\n        nn.init.normal_(w, std=0.02)\n        self.weight = nn.Parameter(w)\n        self.bias = nn.Parameter(torch.zeros(nf))\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(*size_out)\n        return x\n\n7.1 CONV1D Layer Explained\nThe CONV1D layer can be thought of as a LINEAR layer itself. Essentially, it is casting an initial tensor x (having the final dimension of x.size(-1)) being passed to it to have a final dimension of size self.nf.\nHere’s an example output of the same:\nd_model = 768\nconv1d  = Conv1D(d_model, d_model*3)\nx       = torch.rand(1,4,d_model) #represents a sequence of batch_size=1, seq_len=4 and embedding_sz=768, something like \"Hello how are you\"\nx       = conv1d(x)\nx.shape\n\n>> torch.Size([1, 4, 2304])\nAs can be seen in the example above, the final dimension of tensor returned by CONV1D is 3 times the initial size. We do this to be able to cast the input to query, key and value matrices.\nIt is possible then to retrieve the query, key and value matrices like so:\nquery, key, value = x.split(d_model, dim=-1)\n\nquery.shape, key.shape, value.shape \n>> (torch.Size([1, 4, 768]), torch.Size([1, 4, 768]), torch.Size([1, 4, 768]))\n\nAnother way to cast the input to Q, K and V matrices would have to been to have separate Wq, Wk and Wv matrices. I have explained this under the EXTRA section of this post at the bottom. I find this other approach more intuitive and relatable, but we use the CONV1D layer in this post, because we reuse the CONV1D pretrained weights from Hugging Face.\n\n\n\n7.2 FEEDFORWARD Layer Explained\nclass FeedForward(nn.Module):\n    def __init__(self, dropout, d_model=768, nx=768*4):\n        super().__init__()\n        self.c_fc    = Conv1D(d_model, nx)\n        self.c_proj  = Conv1D(nx, d_model)\n        self.act     = F.gelu\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\nSomething, that’s just so well explained in Jay Alammar’s post - also referenced above, is how the inputs are passed through ATTENTION layer first and then on to FEEDFORWARD layer. The Feedforward network, is a normal neural network that accepts the outputs from the ATTENTION layer (768), casts them to nx (768*4) dimension, adds an activation function self.act (GELU), casts them back to d_model (768) and adds dropout (0.1).\nThis is also mentioned in the GPT research paper referenced below. > For the position-wise feed-forward networks, we used 3072 dimensional inner states\n\n\n7.3 ATTENTION Layer Explained\n\nThe below extract is from the paper Attention is all you need.\n\n\n7.3.1 Scaled Dot-Product Attention\nWe call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values.\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n\nThe two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1/√dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/√dk.\n\nTo implement the The Attention layer in code, we first utilize the CONV1D layer and get the q, k and v matrices as explained before.\nOnce we have the q, k and v matrices, we can perform attention using the function _attn. This function replicates the formula mentioned above inside Attention Dot Product.\n\nclass Attention(nn.Module):\n    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n        super().__init__()\n        self.n_head  = n_head\n        self.d_model = d_model\n        self.c_attn  = Conv1D(d_model, d_model*3)\n        self.scale   = scale\n        self.softmax = nn.Softmax(dim=-1)\n        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.dropout = nn.Dropout(0.1)\n        self.c_proj  = Conv1D(d_model, d_model)\n        \n    def split_heads(self, x):\n        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) \n        x = x.view(*new_shape)\n        return x.permute(0, 2, 1, 3) \n    \n    def _attn(self, q, k, v, attn_mask=None):\n        scores  = torch.matmul(q, k.transpose(-2, -1))\n        if self.scale: scores = scores/math.sqrt(v.size(-1))\n        nd, ns  = scores.size(-2), scores.size(-1)\n        if attn_mask is not None: scores = scores + attn_mask\n        scores  = self.softmax(scores)\n        scores  = self.dropout(scores)\n        outputs = torch.matmul(scores, v)\n        return outputs\n    \n    def merge_heads(self, x):\n        x         = x.permute(0, 2, 1, 3).contiguous()\n        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n        return x.view(*new_shape)\n        \n    def forward(self, x):\n        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n        q, k, v  = x.split(self.d_model, dim=2)\n        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n        out      = self._attn(q, k, v)\n        out      = self.merge_heads(out)\n        out      = self.c_proj(out)\n        return out\n\nAnother way to implement Attention is explained in the Extras section at the bottom of this blog. I find it to be more intuitive and easy to compare with the research paper. It utilizes Linear layers instead of CONV1D to cast inputs to Q, K and V matrices. The reason why we haven’t used it is because we use the pretrained weights for CONV1D layer from Hugging Face.\n\n\n\n7.3.2 Multi-Head Attention\n\nThe below extract is from the paper Attention is all you need.\n\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure below.\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n\nNot to be confused by this, in essence all that’s being done is to add another dimension to the Q, K and V matrices. That is, if the matrices were before of size [1, 4, 768] which represents [bs, seq_len, d_model], these matrices are projected to dimension [1, 12, 4, 64] which represents [bs, n_head, seq_len, d_model//n_head]. GPT-2 utizes 12 parallel heads. We split the Q, K, V matrices inside split_heads function. Finally, once we get an output from applying parallel attentions we concatenate it inside merge_heads back to matrices of dimension [bs, seq_len, d_model]."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#gpt-2-model-architecture-in-code",
    "href": "posts/2020-02-18-annotatedGPT2.html#gpt-2-model-architecture-in-code",
    "title": "The Annotated GPT-2",
    "section": "8 GPT-2 Model Architecture in Code",
    "text": "8 GPT-2 Model Architecture in Code\n\nSo far, we have implemented Multi Head Attention and FeedForward layers. The two layers form the building blocks of the Transformer Decoder block, shown in the picture above. The GPT-2 consists of 12 of these Transformer Blocks.\nThis has been shown in Jay Alammar’s post like so: \n\n8.1 Transformer Decoder Block Explained\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        self.attn        = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n        self.ln_1        = LayerNorm(d_model)\n        self.ln_2        = LayerNorm(d_model)\n                \n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.feedforward(self.ln_2(x))\n        return x\nThe Transformer Block consists of Attention and FeedForward Layers. As referenced from the GPT-2 Architecture Model Specification, > Layer normalization (Ba et al., 2016) was moved to the input of each sub-block Here are the sub-blocks are Attention and FeedForward.\nThus, inside a Transformer Decoder Block, essentially we first pass the inputs to a LayerNorm followed by the first sub-block Attention. Next, we pass the outputs of this sub-block to LayerNorm again and finally to FeedForward layer."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#the-gpt-2-architecture-explained",
    "href": "posts/2020-02-18-annotatedGPT2.html#the-gpt-2-architecture-explained",
    "title": "The Annotated GPT-2",
    "section": "9 The GPT-2 Architecture Explained",
    "text": "9 The GPT-2 Architecture Explained\nAs referenced from the GPT paper,\n\nWe trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads).\n\nThus, the complete GPT-2 architecture is the TransformerBlock copied over 12 times.\ndef _get_clones(module, n):\n    return ModuleList([copy.deepcopy(module) for i in range(n)])\n\nclass GPT2(nn.Module):\n    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n        super(GPT2, self).__init__()\n        self.nlayers = nlayers\n        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n        self.h       = _get_clones(block, 12)\n        self.wte     = nn.Embedding(vcb_sz, d_model)\n        self.wpe     = nn.Embedding(n_ctx, d_model)\n        self.drop    = nn.Dropout(0.1)\n        self.ln_f    = LayerNorm(d_model)\n        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.init_weights()\n    \n    def init_weights(self):\n        self.out.weight = self.wte.weight\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    def forward(self, src, labels=None, pos_ids=None):\n        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n        for i in range(self.nlayers): inp = self.h[i](inp)\n        inp     = self.ln_f(inp)\n        logits  = self.out(inp)\n        outputs = (logits,) + (inp,)\n        \n        if labels is not None:\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            outputs = (loss,) + outputs\n            return outputs\n        return logits\n\nSomething I have not mentioned yet is Positional Encoding and Token Embeddings. Since, we cannot pass words such as “hey” or “hello” directly to the model, we first Tokenize our inputs. Next, we use Embeddings to represent the tokens as numbers. This post by Jay Alammar again explains Embeddings very well.\nAlso, since unlike the RNNs where the input words are passed sequentially, Transformers take input matrices in parallel thus losing the sense of position for the words being input. To make up for the loss, before handling the Token Embeddings to the model, we add Positional Encoding - a signal that indicates the order of the words in the sequence. Since, as mentioned before, the context size of GPT-2 is 1024, the positional encodings are of dimensions [1024, 768].\n\n\nThus, the inputs to the GPT-2 architecture is the sum of Token Embeddings and Positional Encodings passed through a Dropout, to add regularization. Once, we have the input matrix, we pass this through each of the 12 Layers of the GPT-2 architecure, where each layer is a Transformer Decoder Block that consists of two sublayers - Attention and FeedForward Network.\n\n9.0.1 Language Modeling or Classification\nWhen using GPT-2 as a language model, we pass the inputs to a final LayerNorm and through a Linear layer with a final dimension of size [768, vocab_sz] (50257) and get an output of size [1, 4, 50257]. This output represents the next word logits and we can very easily now pass this through a Softmax layer and take argmax to get the positional of the word inside the vocabulary with the highest probability.\nFor classification task, we can pass the outputs received from the GPT-2 architecture through a Linear layer with a dimension of size [768, n] to get probabilities for each category (where n represents number of categories), pass it through a softmax, get the highest predicted category and use CrossEntropyLoss to train the architecture to do classification.\n\nAnd that’s really all the magic behind GPT-2. It’s a Decoder only Transformer Based architecture that takes inputs parallely with Positional Encodings unlike RNNs, passes them through each of it’s 12 Transformer Decoder layers (which consist of Multi head Attention and FeedForward Network) to return the final output.\nLet’s see this model in action in a language model task."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#sample-text-generation-using-hugging-face-pretrained-weights",
    "href": "posts/2020-02-18-annotatedGPT2.html#sample-text-generation-using-hugging-face-pretrained-weights",
    "title": "The Annotated GPT-2",
    "section": "10 Sample text generation using Hugging Face Pretrained Weights",
    "text": "10 Sample text generation using Hugging Face Pretrained Weights\nFirst, let’s initialize the model with the Pretrained Weights already provided by Hugging Face.\nmodel = GPT2()\n# load pretrained_weights from hugging face\n# download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin to `.`\n\nmodel_dict = model.state_dict() #currently with random initialization\nstate_dict = torch.load(\"./gpt2-pytorch_model.bin\") #pretrained weights\n\nold_keys = []\nnew_keys = []\nfor key in state_dict.keys(): \n    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n        new_key = key.replace(\"mlp\", \"feedforward\")\n        new_keys.append(new_key)\n        old_keys.append(key)\n\nfor old_key, new_key in zip(old_keys, new_keys): \n    state_dict[new_key]=state_dict.pop(old_key)\n\npretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n\nmodel_dict.update(pretrained_dict)\nmodel.load_state_dict(model_dict)\nmodel.eval() #model in inference mode as it's now initialized with pretrained weights\nLet’s now generate text. We will utilize Hugging Face’s pretrained Tokenizer to convert words to input embeddings.\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ncontext   = torch.tensor([tokenizer.encode(\"The planet earth\")])\n\n\ndef generate(context, ntok=20):\n    for _ in range(ntok):\n        out = model(context)\n        logits = out[:, -1, :]\n        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n        logits[indices_to_remove] = np.NINF\n        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n    return context\n\nout = generate(context, ntok=20)\ntokenizer.decode(out[0])\n\n>> 'The planet earth is the source of all of all the light,\" says the study that the government will'"
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#extras",
    "href": "posts/2020-02-18-annotatedGPT2.html#extras",
    "title": "The Annotated GPT-2",
    "section": "11 Extras",
    "text": "11 Extras\nAnother way to implement Attention as shown in the NLP Course by fast.ai referenced from here, that I find to be more intuitive is as below:\nclass Attention_FASTAI(nn.Module):\n    def __init__(self, d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False):\n        super().__init__()\n        self.n_head   = n_head\n        self.d_head   = d_head\n        self.softmax  = nn.Softmax(dim=-1)\n        self.scale    = scale\n        self.atn_drop = nn.Dropout(0.1)\n        self.wq, self.wk, self.wv = [nn.Linear(d_model, n_head*d_head, \n                                               bias=bias) for o in range(3)]\n    \n\n    def split_heads(self, x, layer, bs):\n        x = layer(x)\n        return x.view(bs, x.size(1), self.n_head, self.d_head).permute(0,2,1,3)\n        \n    def _attn(self, q, k, v, attn_mask=None):\n        scores  = torch.matmul(q, k.transpose(-2, -1))\n        if self.scale: scores = scores/math.sqrt(v.size(-1))\n        if attn_mask is not None: \n            scores = scores.float().masked_fill(attn_mask, -float('inf')).type_as(scores)\n        attn_prob  = self.atn_drop(self.softmax(scores))\n        attn_vec   = attn_prob @ v\n        return attn_vec\n    \n    def merge_heads(self, x, bs, seq_len):\n        x         = x.permute(0, 2, 1, 3).contiguous()\n        return x.view(bs, seq_len, -1)\n        \n    def forward(self, q, k, v, mask=None):\n        bs, seq_len = q.size(0), q.size(1)\n        wq, wk, wv  = map(lambda o:self.split_heads(*o, bs),\n                        zip((q,k,v), (self.wq, self.wk, self.wv)))\n        attn_vec    = self._attn(wq, wk, wv)\n        attn_vec    = self.merge_heads(attn_vec, bs, seq_len)\n        return attn_vec\nThe key difference between the implementation above and the one we have used is that this implementation does not use CONV1D. Instead, we first pass the input x to self.wq, self.wk and self.wv Linear Layers to get wq, wk and wv matrices and then perform attention as before."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#credits",
    "href": "posts/2020-02-18-annotatedGPT2.html#credits",
    "title": "The Annotated GPT-2",
    "section": "12 Credits",
    "text": "12 Credits\n\nI just want to take the time to thank Rachel Thomas and Jeremy Howard for a great NLP course and the fast.ai course in general, which has helped me bolster my understanding of RNNs, GRUs, AWD-LSTM and Transformers. Also, a special thanks to Hugging Face for creating an open source NLP library and providing a number of Pretrained Models to use. As mentioned the code in this blog post comes directly from the Hugging Face library. And, Jay Alammar for the excellent work that he has been doing to Visualise machine learning concepts. The Illustrated GPT-2 is one of the most comprehensive blog posts on GPT-2. Finally, to Harvard NLP, for The Annotated Transformer, a beautiful and easy to follow implementation of Transformers in PyTorch."
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#feedback",
    "href": "posts/2020-02-18-annotatedGPT2.html#feedback",
    "title": "The Annotated GPT-2",
    "section": "13 Feedback",
    "text": "13 Feedback\nComments or feedback? Please tweet me at @amaarora\n\n\nThis is a really wonderful resource, and draws together many very nice pieces of work. https://t.co/CM16ByNrbt\n\n— Jeremy Howard (@jeremyphoward) February 19, 2020\n\n\n\n\nGreat work pairing GPT2 concepts with the key excerpts from the code. https://t.co/IkFlAf3Ua8\n\n— Jay Alammar جهاد العمار (@jalammar) February 20, 2020\n\n\n\n\n\"The Annotated GPT-2\" blogpost seems to start out a a simple question of asking why use conv-1d vs linear. An awesome read!! https://t.co/GCju0z3Wri #nlproc #nlposs #distiller https://t.co/cvSdEah8gT\n\n— Liling Tan (@alvations) February 20, 2020\n\n\n\n\nA must-read blog about GPT-2. https://t.co/EuDil5Dm07\n\n— Xinhao Li (@XinhaoLi1) February 20, 2020\n\n\n\n\nOne of the best NLP Blogposts I've read: A definitive and complete writeup. 🍵This is a blog, I wish I had when I was tinkering with the GPT-2. Must read for everyone: https://t.co/yLRFywYgm6\n\n— Sanyam Bhutani (@bhutanisanyam1) February 20, 2020\n\n\n\n\nNeat! https://t.co/eLt3o180Qr\n\n— Antônio Horta Ribeiro (@ahortaribeiro) February 20, 2020\n\n\n\n\nFantastic work!! Looking forward to learning what is it behind the scenes of this language model! https://t.co/Ml1DY22NxQ\n\n— Data Enigma (@EnigmaData) February 19, 2020\n\n\n\n\nThe Annotated GPT-2 - Understand how the GPT-2 model works underneath with explanations and source codeBlogpost https://t.co/anLqVhZQPN@amaarora𝐬𝐩𝐫𝐞𝐚𝐝 𝐭𝐡𝐞 𝐰𝐨𝐫𝐝 𝐨𝐟 #𝐍𝐋𝐏 💜#datascience #pytorch #deeplearning #machinelearning pic.twitter.com/n5QIQBAIfH\n\n— Philip Vollet (ﾉ◕ヮ◕)ﾉ*:・ﾟ✧ (@philipvollet) February 20, 2020\n\n\n\n\nThis is fantastic @amaarora, thanks 👍\n\n— Manpreet Singh (@ms_ghotratweet) February 20, 2020\n\n\n\n\nThis is brilliant stuff!!!\n\n— Arron Hovingham (@AnalystiaArron) February 20, 2020"
  },
  {
    "objectID": "posts/2020-08-02-densenets.html",
    "href": "posts/2020-08-02-densenets.html",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "",
    "text": "In this post today, we will be looking at DenseNet architecture from the research paper Densely Connected Convolutional Networks.\nThe overall agenda is to: - Understand what DenseNet architecture is - Introduce dense blocks, transition layers and look at a single dense block in more detail - Understand step-by-step the TorchVision implementation of DenseNet"
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#densenet-architecture-introduction",
    "href": "posts/2020-08-02-densenets.html#densenet-architecture-introduction",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "2 DenseNet Architecture Introduction",
    "text": "2 DenseNet Architecture Introduction\nIn a standard Convolutional Neural Network, we have an input image, that is then passed through the network to get an output predicted label in a way where the forward pass is pretty straightforward as shown in the image below:\n\nEach convolutional layer except the first one (which takes in the input image), takes in the output of the previous convolutional layer and produces an output feature map that is then passed to next convolutional layer. For L layers, there are L direct connections - one between each layer and its subsequent layer.\nThe DenseNet architecture is all about modifying this standard CNN architecture like so:\n\nIn a DenseNet architecture, each layer is connected to every other layer, hence the name Densely Connected Convolutional Network. For L layers, there are L(L+1)/2 direct connections. For each layer, the feature maps of all the preceding layers are used as inputs, and its own feature maps are used as input for each subsequent layers.\nThis is really it, as simple as this may sound, DenseNets essentially conect every layer to every other layer. This is the main idea that is extremely powerful. The input of a layer inside DenseNet is the concatenation of feature maps from previous layers.\nFrom the paper: > DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters."
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#but-is-feature-concatenation-possible",
    "href": "posts/2020-08-02-densenets.html#but-is-feature-concatenation-possible",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "3 But is feature concatenation possible?",
    "text": "3 But is feature concatenation possible?\nOkay, so then, now we know the input of Lth layer are the feature maps from [L1, L1, L1.. L-1th] concatenated but is this concatenation possible?\nAt this point in time, I want you to think about whether we can concat the features from the first layer of a DenseNet with the last layer of the DenseNet? If we can, why? If we can’t, what do we need to do to make this possible?\nThis is a good time to take a minute and think about this question.\nSo, here’s what I think - it would not be possible to concatenate the feature maps if the size of feature maps is different. So, to be able to perform the concatenation operation, we need to make sure that the size of the feature maps that we are concatenating is the same. Right?\nBut we can’t just keep the feature maps the same size throughout the network - an essential part of concvolutional networks is down-sampling layers that change the size of feature maps. For example, look at the VGG architecture below:\n\nThe input of shape 224x224x3 is downsampled to 7x7x512 towards the end of the network.\nTo facilitate both down-sampling in the architecture and feature concatenation - the authors divided the network into multiple densely connected dense blocks. Inside the dense blocks, the feature map size remains the same.\n\nDividing the network into densely connected blocks solves the problem that we discussed above.\nNow, the Convolution + Pooling operations outside the dense blocks can perform the downsampling operation and inside the dense block we can make sure that the size of the feature maps is the same to be able to perform feature concatenation.\n\n3.1 Transition Layers\nThe authors refer to the layers between the dense blocks as transition layers which do the convolution and pooling.\nFrom the paper, we know that the transition layers used in the DenseNet architecture consist of a batch-norm layer, 1x1 convolution followed by a 2x2 average pooling layer.\nGiven that the transition layers are pretty easy, let’s quickly implement them here:\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\nEssentially the 1x1 conv performs the downsampling from num_input_features to num_output_features."
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#dense-connectivity",
    "href": "posts/2020-08-02-densenets.html#dense-connectivity",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "4 Dense connectivity",
    "text": "4 Dense connectivity\nLet’s consider a network with L layers, each of which performs a non-linear transformation HL. The output of the Lth layer of the network is denoted as xL and the input image is represented as x0.\nWe know that traditional feed-forward netowrks connect the output of the Lth layer to L+1th layer. And the skip connection can be represented as:\n\nIn DenseNet architecture, the dense connectivity can be represented as:\n\nwhere [x0, x1, x2..] represents concatenation of the feature maps produced by [0,1,.. Lth] layers.\n\n4.1 Inside a single DenseBlock\nNow that we understand that a DenseNet architecture is divided into multiple dense blocks, let’s look at a single dense block in a little more detail. Essentially, we know, that inside a dense block, each layer is connected to every other layer and the feature map size remains the same.\n\nLet’s try and understand what’s really going on inside a dense block. We have some gray input features that are then passed to LAYER_0. The LAYER_0 performs a non-linear transformation to add purple features to the gray features. These are then used as input to LAYER_1 which performs a non-linear transformation to also add orange features to the gray and purple ones. And so on until the final output for this 3 layer denseblock is a concatenation of gray, purple, orange and green features.\nSo, in a dense block, each layer adds some features on top of the existing feature maps.\nTherefore, as you can see the size of the feature map grows after a pass through each dense layer and the new features are concatenated to the existing features. One can think of the features as a global state of the network and each layer adds K features on top to the global state.\nThis parameter K is referred to as growth rate of the network."
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#densenet-architecture-as-a-collection-of-denseblocks",
    "href": "posts/2020-08-02-densenets.html#densenet-architecture-as-a-collection-of-denseblocks",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "5 DenseNet Architecture as a collection of DenseBlocks",
    "text": "5 DenseNet Architecture as a collection of DenseBlocks\nWe already know by now from fig-4, that DenseNets are divided into multiple DenseBlocks.\nThe various architectures of DenseNets have been summarized in the paper.\n\nEach architecture consists of four DenseBlocks with varying number of layers. For example, the DenseNet-121 has [6,12,24,16] layers in the four dense blocks whereas DenseNet-169 has [6, 12, 32, 32] layers.\nWe can see that the first part of the DenseNet architecture consists of a 7x7 stride 2 Conv Layer followed by a 3x3 stride-2 MaxPooling layer. And the fourth dense block is followed by a Classification Layer that accepts the feature maps of all layers of the network to perform the classification.\nAlso, the convolution operations inside each of the architectures are the Bottle Neck layers. What this means is that the 1x1 conv reduces the number of channels in the input and 3x3 conv performs the convolution operation on the transformed version of the input with reduced number of channels rather than the input.\n\n5.1 Bottleneck Layers\nBy now, we know that each layer produces K feature maps which are then concatenated to previous feature maps. Therefore, the number of inputs are quite high especially for later layers in the network.\nThis has huge computational requirements and to make it more efficient, the authors decided to utilize Bottleneck layers. From the paper: > 1×1 convolution can be introduced as bottleneck layer before each 3×3 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. In our experiments, we let each 1×1 convolution produce 4k feature-maps.\nWe know K refers to the growth rate, so what the authors have finalized on is for 1x1 conv to first produce 4*K feature maps and then perform 3x3 conv on these 4*k size feature maps."
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#densenet-implementation",
    "href": "posts/2020-08-02-densenets.html#densenet-implementation",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "6 DenseNet Implementation",
    "text": "6 DenseNet Implementation\nWe are now ready and have all the building blocks to implement DenseNet in PyTorch.\n\n6.1 DenseLayer Implementation\nThe first thing we need is to implement the dense layer inside a dense block.\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n                                           growth_rate, kernel_size=1, stride=1,\n                                           bias=False)),\n        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module('relu2', nn.ReLU(inplace=True)),\n        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                                           kernel_size=3, stride=1, padding=1,\n                                           bias=False)),\n        self.drop_rate = float(drop_rate)\n        self.memory_efficient = memory_efficient\n\n    def bn_function(self, inputs):\n        \"Bottleneck function\"\n        # type: (List[Tensor]) -> Tensor\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n        return bottleneck_output\n\n    def forward(self, input):  # noqa: F811\n        if isinstance(input, Tensor):\n            prev_features = [input]\n        else:\n            prev_features = input\n\n        bottleneck_output = self.bn_function(prev_features)\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate,\n                                     training=self.training)\n        return new_features\nA DenseLayer accepts an input, concatenates the input together and performs bn_function on these feature maps to get bottleneck_output. This is done for computational efficiency. Finally, the convolution operation is performed to get new_features which are of size K or growth_rate.\nIt should now be easy to map the above implementation with fig-5 shown below for reference again:\n\nLet’s say the above is an implementation of LAYER_2. First, LAYER_2 accepts the gray, purple, orange feature maps and concatenates them. Next, the LAYER_2 performs a bottleneck operation to create bottleneck_output for computational efficiency. Finally, the layer performs the HL operation as in eq-2 to generate new_features. These new_features are the green features as in fig-5.\nGreat! So far we have successfully implemented Transition and Dense layers.\n\n\n6.2 DenseBlock Implementation\nNow, we are ready to implement the DenseBlock which consists of multiple such DenseLayers.\nclass _DenseBlock(nn.ModuleDict):\n    _version = 2\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient,\n            )\n            self.add_module('denselayer%d' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for name, layer in self.items():\n            new_features = layer(features)\n            features.append(new_features)\n        return torch.cat(features, 1)\nLet’s map the implementation of this DenseBlock with fig-5 again. Let’s say we pass the number of layers num_layers as 3 to create fig-5 block. In this case, let’s imagine that the num_input_features in gray in the figure is 64. We already know that the authors choose the bottleneck size bn_size for 1x1 conv to be 4. Let’s consider the growth_rate is 32 (same for all networks as in the paper).\nGreat, so the first layer LAYER_0 accepts 64 num_input_features and outputs extra 32 features. Excellent. Now, LAYER_1 accepts the 96 features num_input_features + 1 * growth rate and outputs extra 32 features again. Finally, LAYER_2 accepts 128 features num_input_features + 2 * growth rate and adds the 32 green features on top with are then concatenated to existing features and returned by the DenseBlock.\nAt this stage, it should be really easy for you to map the implementation of dense block with fig-5."
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#densenet-architecture-implementation",
    "href": "posts/2020-08-02-densenets.html#densenet-architecture-implementation",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "7 DenseNet Architecture Implementation",
    "text": "7 DenseNet Architecture Implementation\nFinally, we are now ready to implement the DenseNet architecture as we have already implemented the DenseLayer and DenseBlock.\nclass DenseNet(nn.Module):\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):\n\n        super(DenseNet, self).__init__()\n\n        # Convolution and pooling part from table-1\n        self.features = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n                                padding=3, bias=False)),\n            ('norm0', nn.BatchNorm2d(num_init_features)),\n            ('relu0', nn.ReLU(inplace=True)),\n            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Add multiple denseblocks based on config \n        # for densenet-121 config: [6,12,24,16]\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient\n            )\n            self.features.add_module('denseblock%d' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                # add transition layer between denseblocks to \n                # downsample\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=num_features // 2)\n                self.features.add_module('transition%d' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1))\n        out = torch.flatten(out, 1)\n        out = self.classifier(out)\n        return out\nLet’s use the above implementation to create densenet-121 architecture.\ndef _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n              **kwargs):\n    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n    return model\n\ndef densenet121(pretrained=False, progress=True, **kwargs):\n    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n                     **kwargs)\nHere’s what happens. First, we initialize the stem of the DenseNet architecture - this is the convolution and pooling part from table-1.\nThis part of the code does that:\n        self.features = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n                                padding=3, bias=False)),\n            ('norm0', nn.BatchNorm2d(num_init_features)),\n            ('relu0', nn.ReLU(inplace=True)),\n            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\nNext, based on the config, we create a DenseBlock based on the number of layers in the config.\nThis part of the code does this:\nfor i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient\n            )\n            self.features.add_module('denseblock%d' % (i + 1), block)\nFinally, we add Transition Layers between DenseBlocks.\n            if i != len(block_config) - 1:\n                # add transition layer between denseblocks to \n                # downsample\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=num_features // 2)\n                self.features.add_module('transition%d' % (i + 1), trans)\n                num_features = num_features // 2\nAnd that’s all the magic behind DenseNets!"
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#conclusion",
    "href": "posts/2020-08-02-densenets.html#conclusion",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nCongratulations! Today, together, we successfully understood what DenseNets are and also understood the torchvision implementation of DenseNets. I hope that by now you have a very thorough understanding of the DenseNet architecture.\nAs always, constructive feedback is always welcome at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#credits",
    "href": "posts/2020-08-02-densenets.html#credits",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "9 Credits",
    "text": "9 Credits\nAll code implementations have been directly copied from torchvision."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html",
    "href": "posts/2021-01-18-ViT.html",
    "title": "Vision Transformer",
    "section": "",
    "text": "This blog is the beginning of some truly exciting times - want to know why?\nIn this blog post, we would not only be looking at the architectures in detail, but we will also understand how they are implemented in timm, thus, looking at the code implementation too!\nAnd also please welcome Dr Habib Bukhari! You might know of him as DrHB on Kaggle - at the time of writing, ranked 186/154,204 on the Kaggle ranking system! I am sure we all knew that he is great at deep learning but did you also know that he can also do some kickass visualizations to explain complicated concepts easily? And that, we have decided to team up for this and many more future blog posts to explain the concepts in an easy and visual manner. Together, we hope that you like what you read and see! Much effort and planning has gone into writing this blog post.\nSo, let’s get started! This blog post has been structured in the following way:"
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#prerequisite",
    "href": "posts/2021-01-18-ViT.html#prerequisite",
    "title": "Vision Transformer",
    "section": "1 Prerequisite",
    "text": "1 Prerequisite\nIn this blog post, I assume that the reader knows about the Transformer Architecture. While it will be introduced briefly as part of this post, our main focus will on understanding how the authors of An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale from Google Brain applied the Transformer Architecture to computer vision.\nHere are some of my personal favourite resources on Transformers: 1. The illustrated Transformer by Jay Alammar 2. The Annotated Transformer by Harvard NLP 3. Introduction to the Transformer by Rachel Thomas and Jeremy Howard\nThough the next one is a biased recommendation, I would also like to recommend the reader to my previous post The Annotated GPT-2, for further reading on Transformers."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#introduction",
    "href": "posts/2021-01-18-ViT.html#introduction",
    "title": "Vision Transformer",
    "section": "2 Introduction",
    "text": "2 Introduction\nAt the time of release, the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale received quite a bit of “attention” from the community. This was the first paper to get some astonishing results on the ImageNet dataset using the Transformer architecture. While there had been attempts made in the past to apply Transformers in the context of image processing (1, 2, 3), this paper was one of the first to apply Transformers to full-sized images.\nNOTE: I will be using the terms Vision Transformer & ViT interchangeably throughout this blog post and both refer to architecture described in An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#key-contributions",
    "href": "posts/2021-01-18-ViT.html#key-contributions",
    "title": "Vision Transformer",
    "section": "3 Key Contributions",
    "text": "3 Key Contributions\nThe key contributions from this paper were not in terms of a new architecture, but rather the application of an existing architecture (Transformers), to the field of Computer Vision. It is the training method and the dataset used to pretrain the network, that were key for ViT to get excellent results compared to SOTA (State of the Art) on ImageNet.\nSo, there aren’t a lot of new things to introduce in this post, but rather how to use the existing Transformer architecutre and apply it to Computer Vision. Thus, if the reader knows about Transformers, this blog post and the research paper itself should be a fairly simple read."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#the-vision-transformer",
    "href": "posts/2021-01-18-ViT.html#the-vision-transformer",
    "title": "Vision Transformer",
    "section": "4 The Vision Transformer",
    "text": "4 The Vision Transformer\nWe will be using a top down approach to understand the Visual Transformer architecture. We will first start by looking at the overall architecture and then dig deeper into each of the five steps in the overall architecture.\nAs an overall method, from the paper: > We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017)\n\nThe overall architecture can be described easily in five simple steps below: 1. Split an input image into patches. 2. Get linear embeddings (representation) from each patch referred to as Patch Embeddings. 3. Add position embeddings and a [cls] token to each of the Patch Embeddings. 4. Pass through a Transformer Encoder and get the output values for each of the [cls] tokens. 5. Pass the representations of [cls] tokens through a MLP Head to get final class predictions.\n\nNote that there is a MLP inside the Transformer Encoder and a MLP Head that gives the class predictions, these two are different.\n\nNot as descriptive? Enter Dr Habib to the rescue.\n\nLet’s now look at the five steps again with the help of fig-2. Let’s imagine that we want to classify a 3 channel (RGB) input image of a frog of size 224 x 224.\nThe first step is to create patches all over the image of patch size 16 x 16. Thus we create 14 x 14 or 196 such patches. We can have these patches in a straight line as in fig-2 where the first patch comes from the top-left of the input image and the last patch comes from the bottom-right. As can be seen from the figure, the patch size is 3 x 16 x 16 where 3 represents the number of channels (RGB).\nIn the second step, we pass these patches through a linear projection layer to get 1 x 768 long vector representation for each of the image patches and these representations have been shown in purple in the figure. In the paper, the authors refer to these representations of the patches as Patch Embeddings. Can you guess what’s the size of this patch embedding matrix? It’s 196 x 768. Because we had a total of 196 patches and each patch has been represented as a 1 x 768 long vector. Therefore, the total size of the patch embedding matrix is 196 x 768.\n\nYou might wonder why is the vector length 768? Well, 3 x 16 x 16 = 768. So, we are not really losing any information as step of this process of getting these patch embeddings.\n\nIn the third step, we take this patch embedding matrix of size 196 x 768 and similar to BERT, the authors prepend a [cls] token to this sequence of embedded patches and then add Position Embeddings. As can be seen from fig-2, the size of the Patch Embeddings becomes 197 x 768 after adding the [cls] token and also the size of the Position Embeddings is 197 x 768.\n\nWhy do we add this class token and position embeddings? You will find a detailed answer in the original Transformer and Bert papers, but to answer briefly, the [class] tokens are added as a special tokens whose outputs from the Transformer Encoder serve as the overall image patch representation. And we add the positional embeddings to retain the positional information of the patches. The Transformer model on it’s own does not know about the order of the patches unlike CNNs, thus we need to manually inject some information about the relative or absolute position of the patches.\n\nIn the fourth step, we pass these preprocessed patch embeddings with positional information and prepended [cls] token to the Transformer Encoder and get the learned representations of the [cls] token. Thus, the output frpm the Transformer Encoder would be of size 1 x 768 which is then fed to the MLP Head (which is nothing but a Linear Layer) as part of the final fifth step to get class predictions.\nHaving looked at the overall architecture, we will now look at the individual steps in detail in the following sections."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#patch-embeddings",
    "href": "posts/2021-01-18-ViT.html#patch-embeddings",
    "title": "Vision Transformer",
    "section": "5 Patch Embeddings",
    "text": "5 Patch Embeddings\nIn this section we will be looking at steps one and two in detail. That is the process of getting patch embeddings from an input image.\n\nSo far in the blog post I have mentioned that the way we get patch embeddings from an input image is to first split an image into fixed-size patches and then linearly embed each one of them using a linear projection layer as shown in fig-2.\nBut, it is actually possible to combine both steps into a single step using 2D Convolution operation. It is also better from an implementation perspective to do it this way as our GPUs are optimized to perform the convolution operation and it takes away the need to first split an image into patches. Let’s see why this works?\nIf we set the the number of out_channels to 768, and both kernel_size & stride to 16, then as shown in fig-3, once we perform the convolution operation (where the 2-D Convolution has kernel size 3 x 16 x 16), we can get the Patch Embeddings matrix of size 196 x 768 like below:\n# input image `B, C, H, W`\nx = torch.randn(1, 3, 224, 224)\n# 2D conv\nconv = nn.Conv2d(3, 768, 16, 16)\nconv(x).reshape(-1, 196).transpose(0,1).shape\n\n>> torch.Size([196, 768])"
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#cls-token-position-embeddings",
    "href": "posts/2021-01-18-ViT.html#cls-token-position-embeddings",
    "title": "Vision Transformer",
    "section": "6 [cls] token & Position Embeddings",
    "text": "6 [cls] token & Position Embeddings\nIn this section, let’s look at the third step in more detail. In this step, we prepend [cls] tokens and add Positional Embeddings to the Patch Embeddings.\nFrom the paper: > Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embedded patches, whose state at the output of the Transformer encoder (referred to as ZL0) serves as the image representation. Both during pre-training and fine-tuning, a classification head is attached to ZL0.\n\nPosition embeddings are also added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings and the resulting sequence of embedding vectors serves as input to the encoder.\n\nThis process can be easily visualized as below:\n\nAs can be seen from fig-4, the [cls] token is a vector of size 1 x 768. We prepend it to the Patch Embeddings, thus, the updated size of Patch Embeddings becomes 197 x 768.\nNext, we add Positional Embeddings of size 197 x 768 to the Patch Embeddings with [cls] token to get combined embeddings which are then fed to the Transformer Encoder. This is a pretty standard step that comes from the original Transformer paper - Attention is all you need.\n\nNote that the Positional Embeddings and cls token vector is nothing fancy but rather just a trainable nn.Parameter matrix/vector."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#the-transformer-encoder",
    "href": "posts/2021-01-18-ViT.html#the-transformer-encoder",
    "title": "Vision Transformer",
    "section": "7 The Transformer Encoder",
    "text": "7 The Transformer Encoder\nThis section of the blog post is not specific to Vision Transformers but a repeat of the Transformer Architecture and covers step-4. If you already know of the Transformer Architectures, then, feel free to skip this section. If you are not aware of the Transformer Architecture, then I recommend you to go through any of the resources mentioned in the Prerequisite section of this blog post.\nIn this section, we will be looking into the Transformer Encoder from fig-1 in detail. As shown in fig-1, the Transformer Encoder consists of alternating layers of Multi-Head Attention and MLP blocks. Also, as shown in fig-1, Layer Norm is used before every block and residual connections after every block.\nA single layer/block of the Transformer Encoder can be visualized as below:\n\nThe first layer of the Transformer Encoder accepts combined embeddings of shape197 x 768 as input. For all subsequent layers, the inputs are the outputs Out matrix of shape 197 x 768 from the previous layer of the Transformer Encoder. There are a total of 12 such layers in the Transformer Encoder of the ViT-Base architecture.\nInside the layer, the inputs are first passed through a Layer Norm, and then fed to Multi-Head Attention block.\nInside the Multi-Head Attention, the inputs are first converted to 197 x 2304 (768*3) shape using a Linear layer to get the qkv matrix. Next we reshape this qkv matrix into 197 x 3 x 768 where each of the three matrices of shape 197 x 768 represent the q, k and v matrices. These q, k and v matrices are further reshaped to 12 x 197 x 64 to represent the 12 attention heads. Once we have the q, k and v matrices, we finally perform the attention operation inside the Multi-Head Attention block which is given by the equation:\n\nOnce we get the outputs from the Multi-Head Attention block, these are added to the inputs (skip connection) to get the final outouts that again get passed to Layer Norm before being fed to the MLP Block.\nThe MLP, is a Multi-Layer Perceptron block consists of two linear layers and a GELU non-linearity. The outputs from the MLP block are again added to the inputs (skip connection) to get the final output from one layer of the Transformer Encoder.\nHaving looked at a single layer inside the Transformer Encoder, let’s now zoom out and look at the complete Transformer Encoder.\n\nAs can be seen from the image above, a single Transformer Encoder consists of 12 layers. The outputs from the first layer are fed to the second layer, outputs from the second fed to the third until we get the final outputs from the 12th layer of the Transformer Encoder which are then fed to the MLP Head to get class predictions. The above image is another way to summarize fig-1."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#the-vision-transformer-in-pytorch",
    "href": "posts/2021-01-18-ViT.html#the-vision-transformer-in-pytorch",
    "title": "Vision Transformer",
    "section": "8 The Vision Transformer in PyTorch",
    "text": "8 The Vision Transformer in PyTorch\nHaving understood the Vision Transformer Architecture in great detail, let’s now look at the code-implementation and understand how to implement this architecture in PyTorch. We will be referencing the code from timm to explain the implementation. The code below has been directly copied from here.\nWe will build Vision Transformer using a bottom-up approach. We will take what we have learnt so far and start implementing the overall architecture piece-by-piece. First things first, how do get Patch Embeddings?\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\nAs we know, we use a 2-D Convolution where stride, kernel_size are set to patch_size. Thus, that is exactly what the class above does. We set self.proj to be a nn.Conv2d which goes from 3-channels to 768 and to get 196 x 768 patch embedding matrix.\npatch_embed = PatchEmbed()\nx = torch.randn(1, 3, 224, 224)\npatch_embed(x).shape \n\n>> torch.Size([1, 196, 768])\nOkay, so that’s that. It is also pretty easy to implement the MLP Block inside the Transformer Encoder below:\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\nBasically, it consists of two layers and a GELU activation layer. There isn’t a lot happening in this class and is pretty easy to implement. Next, we implement Attention as below:\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\nAs described inside the Multi-Head Attention block, we use a Linear layer to get the qkv matrix. Also, we apply the attention operation inside the forward method above like so:\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\nThe above code implements eq-1. Since we have already implemented the Attention Layer and MLP block, let’s quickly implement a single layer of the Transformer Encoder. As we already know from fig-5, a single Block consists of Layer Norm, Attention and MLP block.\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\nAs can be seen in the forward method above, this Block accepts inputs x, passes them through self.norm1 which is LayerNorm followed by the attention operation. Next, we normalize the output after the attention operation again before passing through self.mlp followed by Dropout to get the outputs Out matrix from this single block as in fig-5.\nNow that we have all the pieces, the complete architecture for the Vision Transformer can be implemented below like so:\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n\n        # Classifier head\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  \n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\nI leave it as an exercise to the reader to understand the implementation of the Vision Transformer. It merely brings all the pieces together and performs the steps as described in fig-2."
  },
  {
    "objectID": "posts/2021-01-18-ViT.html#conclusion",
    "href": "posts/2021-01-18-ViT.html#conclusion",
    "title": "Vision Transformer",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nI hope that through this blog, both Dr Habib and I have been able to explain all the magic that goes on inside the Vision Transformer Architecture.\nOnce again, thanks to Dr Habib, I have been able to reference his beautiful and simplistic visualizations to explain each step in detail.\nAs usual, in case we have missed anything or to provide feedback, please feel free to reach out to me at @amaarora or Dr Habib at @dr_hb_ai.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html",
    "href": "posts/2022-07-04-swintransformerv1.html",
    "title": "Swin Transformer",
    "section": "",
    "text": "For someone who was actively releasing blogs almost all throughout 2020 & 2021, I am kinda sad to admit that this is my first blog for the year 2022. But, at the same time, I am super excited to be back. My personal responsibilities took priority for the last 1 year and I had to give up on releasing blog posts. Now that the storm has settled, I am happy to be back.\nI also resigned from my position as Machine Learning Engineer from Weights and Biases (W&B) earlier this year and have joined REA Group as Data Science Lead. It’s quite a big change in my day to day work life, but I am up for the challenge and enjoying every second of my new job so far. :)\nI wrote many blogs on various different research papers during my time at W&B that can be found here.\nA lot has changed in the past 1 year or so since I have been away. As I catch-up with the latest research, I hope to continue releasing more blog posts fortnightly and take you on this journey with me as well. Let’s learn together!"
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html#prerequisites",
    "href": "posts/2022-07-04-swintransformerv1.html#prerequisites",
    "title": "Swin Transformer",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nAs part of this blog post I am going to assume that the reader has a basic understanding of CNNs and the Transformer architecture.\nHere are a couple good resources on the Transformer architecture if you’d like some revision: 1. The Annotated Transformer 2. Vision Transformer (ViT)\nFor CNNs, there are various architectures that have been introduced. I have previously written blogs about a few of them: 1. Squeeze and Excitation Networks 2. DenseNet 3. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html#introduction",
    "href": "posts/2022-07-04-swintransformerv1.html#introduction",
    "title": "Swin Transformer",
    "section": "3 Introduction",
    "text": "3 Introduction\nAs part of today’s blog post, I want to cover Swin Transformers. As is usual for my blog posts, I will be covering every related concept in theory along with a working PyTorch implementation of the architecture from TIMM. Also, all text presented in this blog post copied directly from the paper will be in Italics.\n\nNote: At the time of writing this blog post, we already have a Swin Transformer V2 architecture. This architecture will be covered in a future blog post.\n\nWhile the Transformer architecture before this paper had proved to be performing better than CNNs on the ImageNet dataset, it was yet to be utilised as a general purpose backbone for other tasks such as object detection & semantic segmentation. This paper solves that problem and Swin Transformers can capably serve as general purpose backbones for computer vision.\nFrom the Abstract of the paper:\nSwin Transformer is compatible for a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures."
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html#key-conceptsideas",
    "href": "posts/2022-07-04-swintransformerv1.html#key-conceptsideas",
    "title": "Swin Transformer",
    "section": "4 Key Concepts/Ideas",
    "text": "4 Key Concepts/Ideas\nI might be oversimplifying here, but in my head there are only two new key concepts that we need to understand on top of ViT to get a complete grasp of the Swin Transformer architecture.\n\nShifted Window Attention\nPatch Merging\n\nEverything else to me looks pretty much the same as ViT (with some minor modifications). So, what are the two concepts? We will get to them later in this blog post.\nFirst, let’s get a high level overview of the architecture."
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html#swin-transformer-overview",
    "href": "posts/2022-07-04-swintransformerv1.html#swin-transformer-overview",
    "title": "Swin Transformer",
    "section": "5 Swin Transformer Overview",
    "text": "5 Swin Transformer Overview\n\nFrom section 3.1 of the paper:\nAn overview of the Swin Transformer architecture is presented in the Figure above, which illustrates the tiny version (SwinT). It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT. Each patch is treated as a “token” and its feature is set as a concatenation of the raw pixel RGB values. In our implementation, we use a patch size of 4 × 4 and thus the feature dimension of each patch is 4 × 4 × 3 = 48. A linear embedding layer is applied on this raw-valued feature to project it to an arbitrary dimension (denoted as C).\n\n5.1 Patch Partition/Embedding\nSo first step is to take in an input image and convert it to Patch Embeddings. This is the exact same as ViT with the difference being that each patch size in Swin Transformer is 4x4 instead of 16x16 as in ViT. I have previously explained Patch Embeddings here and therefore won’t be going into detail here.\nfrom timm.models.layers import PatchEmbed\nx = torch.randn(1, 3, 224, 224)\npatch_embed = PatchEmbed(img_size=224, patch_size=4, embed_dim=96)\npatch_embed(x).shape\n\n>> torch.Size([1, 3136, 96])\nAs can be seen, that the output of the Patch Embedding layer is of shape \\((1, 3136, 96)\\), that is, \\((1, (H/4, W/4), 96)\\) where 96 is the embedding dimension C.\n\nNOTE: The embedding dimension 96 for Swin-T (architecture covered as part of this blog post) has been mentioned in section 3.3 of the paper under Architecture Variants.\n\n\n\n5.2 Swin Transformer Stages Overview\nContinuing from section 3.1 of the paper:\nSeveral Transformer blocks with modified self-attention computation (Swin Transformer blocks) are applied to these patch tokens. The Transformer blocks maintain the number of tokens H/4 × W/4, and together with the linear embedding are referred to as “Stage 1”. To produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper. The first block of patch merging and feature transformation is denoted as “Stage 2”. The procedure is repeated twice, as “Stage 3” and “Stage 4”\nstage_1 = BasicLayer(dim=96, out_dim=192,\n                     input_resolution=(56, 56),\n                     depth=2)\ninp = torch.randn(1, 56*56, 96)\nstage_1(inp).shape\n\n>> torch.Size([1, 3136, 96])\nAs can be seen from the code snippet above, there is no change in the dimension of the input as it passes through “Stage-1”. In fact, the dimension of the inputs doesn’t change as it passes through every stage. It is in between stages, that a Patch Merging layer is applied to reduce the number of tokens as the network get’s deeper.\n\n\n5.3 Patch Merging Layer\nFrom section 3.1 of the paper:\nThe first patch merging layer concatenates the features of each group of 2×2 neighboring patches, and applies a linear layer on the 4C - dimensional concatenated features. This reduces the number of tokens by a multiple of 2×2 = 4 (2× downsampling of resolution), and the output dimension is set to 2C.\nHere, \\(C\\) is the number of channels (embedding dimension). For the tiny version that has been explained as part of this blog post, \\(C=96\\).\n\nAs can be seen below, the Patch-Merging layer merges four patches. So with every merge, both height and width of the image are further reduced by a factor of 2. In stage-1, the input resolution is \\((H/4,W/4)\\), but after patch merging, the resolution will change to \\((H/8, W/8)\\) which will be the input for stage-2. For stage-3 the input resolution would be \\((H/16, W/16)\\) and for stage-4, the input resolution would be \\((H/32, W/32)\\).\nLet’s understand the inputs and outputs for PatchMerging in code:\nfrom timm.models.swin_transformer import PatchMerging\nx = torch.randn(1, 56*56, 96)\nl = PatchMerging(input_resolution=(56, 56), dim=96, out_dim=192, norm_layer=nn.LayerNorm)\nl(x).shape\n\n>> torch.Size([1, 784, 192]) # (1, 28x28, 192)\nAs can be seen, the output width and height are both reduced by a factor of 2 and the number of output channels is 2C where C is the number of input channels, here for Swin-T, \\(C=96\\).\nLet’s look at the source code for Patch Merging now that we understand it’s functionality:\nclass PatchMerging(nn.Module):\n    def __init__(self, input_resolution, dim, out_dim=None, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.out_dim = out_dim or 2 * dim\n        self.norm = norm_layer(4 * dim)\n        self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        B: Batch size \n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C      \n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n        return x\nTo help understand the code above, I used Microsoft Excel again. In the figure below,\n\n\\(X_0\\) is represented by ✔️ - starting at row 0, column 0;\n\\(X_1\\) is represented by ❌ - starting at row 1, column 0;\n\\(X_2\\) is represented by ⚫ - starting at row 0, column 1;\n\\(X_0\\) is represented by ⬛ - starting at row 1, column 1\n\n\nTherefore, when we concatenate in code using x = torch.cat([x0, x1, x2, x3], -1), we are actually merging four patches together, and therefore here \\(X\\) would have dimension size of 4C. Next, as was mentioned in the paper - the output dimension is set to 2C, therefore, we make use of a nn.Linear layer in code to reduce the dimension side to 2C.\nNow that we’ve looked at Patch Merging layer, let’s get to the meat of the paper - which is the Swin Transformer Block."
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html#swin-transformer-block",
    "href": "posts/2022-07-04-swintransformerv1.html#swin-transformer-block",
    "title": "Swin Transformer",
    "section": "6 Swin Transformer Block",
    "text": "6 Swin Transformer Block\nAt every stage in Swin-T Architecture, there are at two consecutive Swin Transformer Blocks except in Stage-3, where there are 6 Swin Transformer Blocks in tandem.\n\nFrom section Swin Transformer Block heading under section 3.1 of the paper:\nSwin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows, with other layers kept the same. As illustrated in Figure above, a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module.\nOkay, so pretty much everything is the same as ViT except this idea of shifted windows based attention. So, that’s what we should really at next before looking at the code implementation of Swin Transformer Block.\n\n6.1 Shifted Windows based Self Atention\nFrom section 3.2 of the paper:\nThe standard Transformer architecture and its adaptation for image classification both conduct global selfattention, where the relationships between a token and all other tokens are computed. The global computation leads to quadratic complexity with respect to the number of tokens, making it unsuitable for many vision problems requiring an immense set of tokens for dense prediction or to represent a high-resolution image.\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.\nAs illustrated in Figure below, the first module uses a regular window partitioning strategy which starts from the top-left pixel, and the 8 × 8 feature map is evenly partitioned into 2 × 2 windows of size 4 × 4 (M = 4). Then, the next module adopts a windowing configuration that is shifted from that of the preceding layer, by displacing the windows by \\(([M/2], [M/2])\\) pixels from the regularly partitioned windows.\n\nBefore looking at the code implementation of shifted window based attention, we first need to understand what’s exactly going on. And if the above doesn’t make much sense, let me try and break it down for you. It’s really simple. Trust me!\nSo on the left, we have an 8x8 feature map which is evenly partitioned into 4 windows of size 4 x 4. Here, the window size \\(M=4\\). Now in the first part of the two successive blocks, we calculate attention inside these windows. But, remember we also need cross-window attention for our network to learn better! Why? (Because we are no longer using a global context). So, in the second part of the swin transformer block, we displace the windows by \\(([M/2], [M/2])\\) pixels from the regularly partitioned windows, and perform attention between these new windows! This leads to cross-window connections. In this case, since \\(M=4\\), we displace the windows by \\((2, 2)\\). Now, we perform self-attention inside the shifted local windows.\n\nNote: After the shifting windows by (2,2), we have a total of 9 windows, whereas previously we only had 4 windows. There might be a better way to perform shifted window attention. That has been explained in the next section.\n\n\n\n6.2 Efficient batch computation for shifted configuration\n\nFrom section 3.2 of the paper:\nAn issue with shifted window partitioning is that it will result in more windows, from \\([h/M] x [w/M]\\) to \\(([h/M]+1) X ([w/M]+1)\\) in the shifted configuration, and some of the windows will be smaller than \\(M × M\\). Here, we propose a more efficient batch computation approach by cyclic-shifting toward the top-left direction, as illustrated in Figure 4. After this shift, a batched window may be composed of several sub-windows that are not adjacent in the feature map, so a masking mechanism is employed to limit self-attention computation to within each sub-window. With the cyclic-shift, the number of batched windows remains the same as that of regular window partitioning, and thus is also efficient.\nI’ll be honest, none of the above made complete sense to me for the first few days when I read the paper until recently! So, I’ll try to explain shifted window attention in the easiest way possible using Microsoft Excel.\n\n\n6.3 Window Attention and Shifted Window Attention using Microsoft Excel\nRemember that we always have two consecutive Swin Transformer Blocks. The first one performs window attention and the second one performs “shifted” window attention. Here, window attention just means attention inside local windows. Performing a “shifted” window attention makes sure that there is cross-window connections while maintaining the efficient computation of non-overlapping windows.\n\nLet’s assume that we have a 8x8 feature map as was shown in the paper. Let this be called as feature map “A”. Now, if the window size \\(M=4\\), then we can divide this feature map into 4 windows each of size 4x4. The first Swin Transformer block performs self-attention within these 4 local windows that have been highlighted with different colours.\nNow, before passing on the feature map to the second Swin Transformer Block, we shift the windows by \\((M/2, M/2)\\) as mentioned in the paper. Therefore, we shift the windows by \\((2, 2)\\). This leads to feature map “B” where again, each window has been again highlighted with a different color.\nBut, there is one simple problem - there are now 9 (3x3) windows. This was also mentioned in the paper before in this language - An issue with shifted window partitioning is that it will result in more windows, from \\([h/M] x [w/M]\\) to \\(([h/M]+1) X ([w/M]+1)\\) in the shifted configuration, and some of the windows will be smaller than \\(M × M\\).\nI hope that the paper’s language now makes sense. We have 3x3 windows after shifted the windows by (2, 2), and also some of the windows are smaller than size 4x4. A naive solution could have been to pad the windows that are of size less than 4x4, but this would lead to computation overhead.\nSo the authors of the paper suggested something really neat - “Efficient batch computation for shifted configuration”! The idea is to create a feature map “C” by cyclic shifting the feature map “A”. But as you can see, this leads to a feature map, where except the top-left window, all other windows consist of sub-windows that aren’t really next to each other. The local attention should be as per the highlighted colours in feature map “C” and not the dashed boundaries. Therefore, next step is to use masking when performing local attention. A mask for feature map “C” can be created using the following code:\ncnt = 0\nm   = np.arange(1, 65).reshape(8,8)\nfor h in (slice(0, -4), slice(-4, -2), slice(-2, None)):\n    for w in (slice(0, -4), slice(-4, -2), slice(-2, None)):\n        m[h, w] = cnt\n        cnt += 1\nm\n\n>> \narray([[0, 0, 0, 0, 1, 1, 2, 2],\n       [0, 0, 0, 0, 1, 1, 2, 2],\n       [0, 0, 0, 0, 1, 1, 2, 2],\n       [0, 0, 0, 0, 1, 1, 2, 2],\n       [3, 3, 3, 3, 4, 4, 5, 5],\n       [3, 3, 3, 3, 4, 4, 5, 5],\n       [6, 6, 6, 6, 7, 7, 8, 8],\n       [6, 6, 6, 6, 7, 7, 8, 8]])\nAs can be seen above, the mask completely matches the highlighted colors in feature map “C”. Thus we can perform attention with masking while using cyclic-shift. This will be equivalent to performing window-attention in feature map “B”! I hope that now this concept of window attention and shifted window attenion makes sense. And that now you understand how the authors proposed to use “Efficient batch computation for shifted configuration”! We are now ready to look at the PyTorch implementation of SwinTransformerBlock along with WindowAttention.\n\nPlease feel free to pause at this point, and re read the above three sections. Everything I’ve written above, should make complete sense. If it does, then, you’ve really understood how Swin Transformer works! If it doesn’t, feel free to reach out to me. :)\n\n\n\n6.4 Swin Transformer Block in PyTorch\nclass SwinTransformerBlock(nn.Module):\n    def __init__(\n            self, dim, input_resolution, num_heads=4, head_dim=None, window_size=7, shift_size=0,\n            mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, num_heads=num_heads, head_dim=head_dim, window_size=to_2tuple(self.window_size),\n            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            cnt = 0\n            for h in (\n                    slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None)):\n                for w in (\n                        slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None)):\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n            mask_windows = window_partition(img_mask, self.window_size)  # num_win, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        _assert(L == H * W, \"input feature has wrong size\")\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # num_win*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # num_win*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # num_win*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\nIn the implementation above, we are also using window_reverse & WindowAttention, but as part of this blog post I will be skipping over them as they contain minor details (such as relative position bias) that are not needed for an overall understanding of the SwinTransformerBlock.\n\nIf you’d still want me to go over, or want to pair with me on WindowAttention, please feel free to reach out to me.\n\nThe main part of the __init__ method is below:\nif self.shift_size > 0:\n    H, W = self.input_resolution\n    img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n    cnt = 0\n    for h in (\n            slice(0, -self.window_size),\n            slice(-self.window_size, -self.shift_size),\n            slice(-self.shift_size, None)):\n        for w in (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None)):\n            img_mask[:, h, w, :] = cnt\n            cnt += 1\nI hope this is easily understandable to the reader based on the explanations in Window Attention and Shifted Window Attention using Microsoft Excel. Apart from that, we are just creating the architecture as in Figure-4.\nWe can create Stage-1 of the Swin-T architecture using SwinTransformerBlock as below:\nfrom timm.models.swin_transformer import SwinTransformerBlock\nx = torch.randn(1, 56*56, 96)\nt_1 = SwinTransformerBlock(dim=96, input_resolution=(56, 56))\nt_2 = SwinTransformerBlock(dim=96, input_resolution=(56, 56), shift_size=3)\nt_1(x).shape, t_2(t_1(x)).shape\n\n>> (torch.Size([1, 3136, 96]), torch.Size([1, 3136, 96]))\nAs can be seen the only difference between the two transformer blocks is that the second one uses shifted WindowAttention, and the shift_size is set to 3 or window_size//2.\nIn the forward method of SwinTransformerBlock, if shift_size>0, we perform the cyclic shift. This is performed in PyTorch by using torch.roll. Essentially, this torch.roll will create feature map “B” from feature map “A” as in figure-7.\n\n\n6.5 Window Partition\nIt is much easier to partition an input into windows using Microsoft Excel, but how do we do this in PyTorch?\ndef window_partition(x, window_size: int):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\nWe take an input of shape \\((B, H, W, C)\\), next, we reshape it to \\((B, H/M, M, W/M, M, C)\\) and convert it to shape \\((B * (H/M * W/M), M, M, C)\\) where,\n\nB - Batch size\nH - Image height\nW - Image width\nC - Number of channels\nM - Window size"
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html#conclusion",
    "href": "posts/2022-07-04-swintransformerv1.html#conclusion",
    "title": "Swin Transformer",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nAnd that’s really it when it comes to Swin Transformers! I hope that I’ve been to explain the Swin Transformer architecture in detail and I hope that you’ve enjoed reading the blog post and and explanations using MS Excel.\nAs usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-08-23-siimisic.html",
    "href": "posts/2020-08-23-siimisic.html",
    "title": "SIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle",
    "section": "",
    "text": "Recently, Kaggle launched an interesting competition to identify melanoma in images of skin lesions. Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It’s also expected that almost 7,000 people will die from the disease. This was an important competition as with other cancers, early and accurate detection—potentially aided by data science can make treatment more effective.\nIt was also a particularly challenging competition with highly imbalanced dataset with only 2% positive cases and the rest 98% negative cases.\nIn this blog post, I will be sharing my journey from the start and also the solution that led to 153 private leaderboard rank from a total of 3314 teams."
  },
  {
    "objectID": "posts/2020-08-23-siimisic.html#key-learnings",
    "href": "posts/2020-08-23-siimisic.html#key-learnings",
    "title": "SIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle",
    "section": "2 Key learnings",
    "text": "2 Key learnings\nBefore we begin with the solution summary, I want to highlight some of my key learnings from having participated and worked on this competition for over two months.\n\n2.1 Keep fighting until the end\nI wasn’t much of a Kaggler until before this competition and as a result usually ended up in bottom 50% pretty much in all competitions I had participated in before this one. It was disheartening and I usually blamed hardware, lack of knowledge in CV or made up any other excuses I could come up with for my bad results and would leave past competitions mid way - I was wrong! As you’ll read, the complete solution for ISIC was built using a single GPU and Kaggle TPUs(free resource).\nSomething I promised myself at the start of this competition was to not make an excuse and actually finish the whole project regardless of the position I would end up with. This was one of the biggest and key difference in my approach compared to before. And with this mindset, I took the first step.\n\n\n2.2 Research past solutions\nI was quick to find out in the early days that this wasn’t the first time SIIM (Society for Imaging Informatics in Medicine) was hosting such a competition. In fact, such competitions have been held before in 2016, 2017, 2018 and 2019.\nThe first step I took was to read pretty much all the solutions that did well in ISIC 2019. For someone who didn’t know much about medical imaging - this was a gold mine of hidden treasures. I remember making notes on: - How to do image preprocessing - Which data augmentations to use - How to run inference - Which Model architectures work best for this type of data - Is it also possible to use patient metadata to improve results\nAnd many more similar details. Just doing this filled me with a lot of confidence - now my mind was filled with ideas and I couldn’t wait to try them on!\n\n\n\n2.3 Use a standard code structure\nThanks to Abhishek Thakur’s code walkthrough for this very competition - I was quick to adapt a standard code structure. This really proved as a good baseline to begin experimenting with.\n\n\n2.4 Keep a log of your experiments\nThis is one of the most important steps that really helped me steer towards a top 5% solution. Having a log of my experiments, I could quickly find out things that worked for me and things that didn’t and then try again.\nI ran more than 80 different experiments and you can find them here."
  },
  {
    "objectID": "posts/2020-08-23-siimisic.html#solution-summary",
    "href": "posts/2020-08-23-siimisic.html#solution-summary",
    "title": "SIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle",
    "section": "3 Solution Summary",
    "text": "3 Solution Summary\nAbove were some of the key learnings that really helped me get my first silver medal. In this section, we focus on the technical and theoretical details of the solution and the complete code can be found here.\n\n3.1 Image Preprocessing\nThe first challenge was to preprocess the medical images - particularly mostly all images were rectangular and of different shapes and sizes.\n\nAs can be seen in the image above, most of the images are of height 6000px and width 4000px. And the complete training set consists of images with various sizes. We cannot use this raw data directly as input to our network and therefore need to resize the images.\nSo the first step is to be able to resize images. There were many approaches that the winners used: - Center crop the image to square with size equal to shorter side and resize to required input training resolution - Use image segmentation to segment the images to area of interest - Directly resize the images to required input resolution\nWe ended up using another approach of resizing the images such that shorter side of the image is equal to the input training resolution while mantaining the aspect raio. This approach was also used by the 2019 ISIC competition winners as mentioned here.\nWhile the reshaped images are still rectangular, adding extra data augmentation of RandomCrop makes them square and also our models then see a different part of the image in every epoch - thus reducing overfitting. Also, this way we do not lose any information of the image in the resized images as opposed to the center crop approach.\nThe resized images look like below:\n\nBut as you can see, the images have different colors, varying brightness and contrast. This is because the dataset was collected by using images from different distributions. To make sure that our models don’t have to deal with this much noise, another key step was to use color constancy techniques. Preprocessing images using color constancy makes sure that the images appear to be from the same distribution with similar brightness and contrast.\nAfter applying this preprocessing step, the images now look like:\n\nAs you can see, now there is much less variability in between the images and just using this one step of preprocessing significantly improved validation scores. This approach was also used by the 2019 winners.\nTo preprocess images, using color constancy, simply follow the steps mentioned in code repository here.\nNow that we have preprocessed our images, we are ready to train the models.\n\n\n3.2 Cross Validation\nWe used Chris Deotte’s triple stratified folds for validation. These can be found here.\nThere were some duplicate images that were part of both train and test set and therefore, using a GroupKFold wasn’t sufficient.\n\n\n3.3 Model Architecture\nParticularly for this competition, we used EfficientNets. I have written a previous blog post explaining EfficientNet model architecture here.\nMany participants in this competition also found EfficientNets to work best for this competition - particularly an ensemble of them. We also tried SeResnext-50, DenseNet, MobileNet model architectures, but found EfficientNets to have the highest CV score.\nChris Deotte shared an excellent post on how to ensemble different model architectures trained on different input resolutions here. We used a similar approach and trained a total of 15 five-fold models and used weighted average predictions based on out of fold (OOF) score. Models performing better on OOF were given higher weights.\nThe final ensemble consisted of various models trained on various input resolutions - 224, 384, 480, 512 and 672.\n\n\n3.4 Loss Function\nAll models were trained using Focal Loss. We found this loss function to work better than Binary Cross Entropy loss due to the high imbalance in the dataset. We also tried Weighted Binary Cross Entropy but Focal Loss gave us the best CV score so we trained all models(except the ones from public kernel) using Focal Loss.\n\n\n3.5 External Data\nSome of the models were also trained using ISIC 2018 and 2019 external data. There were two strategies that were tried before adding external data: - Add only positive classes - Add complete training data\nWe found adding positive cases worked better than adding complete external data.\n\n\n3.6 Public Kernels\nChris Deotte’s kernel used TPUs and was one of the best Public kernel for this competition. We re-trained 5 different models of varying image sizes that were part of the final ensemble.\n\n\n3.7 Ensemble\nThe final predictions were based on a weighted average ensemble with higher weights given to those models with higher CV scores. The final ensemble consisted of the following models:\n\n384x384 eff-B6 TPU without cc\n224x224 eff-b1 GPU with cc\n256x256 eff-b3 GPU with cc\n384x384 eff-b4 GPU with cc\n672x672 eff-b3 GPU with cc\n480x480 eff-b4 GPU with cc and center crop images\n384x384 eff-b4 GPU with cc + external data\n512x512 eff-b4 TPU without cc + external data (only 2019)\n256x256 eff-b4 TPU without cc + external data (only 2018)\n256x256 eff-b4 TPU without cc + external data (2018 & 2019)\n384x384 eff-b5 TPU without cc + external data (2018)\n512x512 eff-b3 GPU without cc + external data (2018)\n512x512 eff-b5 GPU without cc\n384x384 eff-b5 GPU with cc pseudo labeled\n256x256 eff-b4 GPU with cc pseudo labeled\n\n\n\n3.8 Unstable Public LB\nDue to such a low number of positive classes, the Public LB scores were found to be pretty unstable and did not match the CV scores. To counter this, we created an ensemble of models trained using different data augmentation strategies, image sizes and preprocessing techniques."
  },
  {
    "objectID": "posts/2020-08-23-siimisic.html#conclusion",
    "href": "posts/2020-08-23-siimisic.html#conclusion",
    "title": "SIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIt doesn’t take something magical to produce a good quality solution, however it does take hard work. I remember working really hard for the past 2 months on this competition - starting from the scratch not knowing what works is quite a challenge. At the beginning of the competition running lots of different experiments with smaller image size helps get a better understanding of the challenge and generally of what works. Once, a baseline solution exists, it is then much easier to further improve and finetune the pipeline.\nTo re run the experiments - refer to this GitHub repo.\nIf you feel I have missed out on anything - please feel free to ask any questions regarding my approach at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-08-23-siimisic.html#credits",
    "href": "posts/2020-08-23-siimisic.html#credits",
    "title": "SIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle",
    "section": "5 Credits",
    "text": "5 Credits\nThanks to Chris Deotte for his selfless work in this competition - it definitely made participating in this competition a lot more easier and fun!"
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html",
    "href": "posts/2020-08-09-groupnorm.html",
    "title": "Group Normalization",
    "section": "",
    "text": "In this blog post today, we will look at Group Normalization research paper and also look at: - The drawback of Batch Normalization for smaller batch sizes\n- Introduction to Group Normalization as an alternative to BN - Other normalization techniques available and how does Group Normalization compare to those - Benefits of Group Normalization over other normalization techniques - Discuss the optimal number of groups as a hyperparameter in GN - Discuss effect of Group Normalization on deeper models (eg. Resnet-101) - Implement Group Normalization in PyTorch and Tensorflow - Implement ResNet-50 with [GroupNorm + Weight Standardization] on Pets dataset and compare performance to vanilla ResNet-50 with BatchNorm layer\nBatch Normalization is used in most state-of-the art computer vision to stabilise training. BN normalizes the features based on the mean and variance in a mini-batch. This has helped improve model performance, reduce training time and also helped very deep models converge.\nBut this technique also suffers from drawbacks - if batch size is too small, training becomes unstable with BN.\nThe aim of this blog post is not to study BN, many other wonderful posts have been written on that, but to look at other alternatives such as GN.\nThrough this blog post, I hope to introduce Group Normalization as an alternative to Batch Normalization and help the reader develop an intuition for cases where GN could perform better than BN.\n\n\nKnowingly or unknowingly, we have all used BN in our experiments when training a deep learning network. If you have trained a ResNet model or pretty much any other CV model using PyTorch or Tensorflow, you have made use of BN to normalize the deep learning network.\nFrom the Group Normalization research paper, > We all know that BN has been established as a very effective component in deep learning. BN normalizes the features by the mean and variance computed within a batch. But despite its great success, BN exhibits drawbacks that are also caused by its distinct behavior of normalizing along the batch dimension. In particular, it is required for BN to work with sufficiently large batch size. A small batch size leads to innacurate estimation of the batch statistics and reducing BN’s batch size increases the model error dramatically.\nEssentially, what that means is that BN is not very effective if the batch sizes are too small. Especially for CV applications other than Image classification such as object detection, segmentation, video classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes.\nEspecially in such cases, GN can be used a strong alternative to BN.\nOr, there could be cases where you might want to try a bigger capacity model leaving less space in the GPU to fit a bigger batch size. In such cases as well, you might want to try GN as an alternative.\n\n\n\nIn the paper, the authors introduce GN as a simple alternative to BN. From the paper:\n\nGN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes.\n\nEssentially, GN takes away the dependance on batch size for normalization and in doing so mitigates the problem suffered by BN. There are also other techniques that have been proposed to avoid batch dimension - but we will discuss them later. For now, it is essential for the reader to realize that instead of normalizing accross the batch dimension, GN normalizes accross the groups (channel dimension). This has been further explained in depth later in this post here.\nFirst, let’s look at how GN compares to BN for training accross various batch sizes keeping all else same.\n\nAs can be seen in the image above, because GN does not depend on the batch size, the validation classification error (when the deep learning model is normalized using GN) is stable accross various batch sizes compared to BN.\n\nThe same trend as in fig-1 can also be observed in fig-2 where the validation error is consistent accross various batch sizes for GN as opposed to BN. Another key thing to note, the validation error for GN as reported in the research paper is very similar to that for BN - therefore, GN can be considered to be a strong alternative to BN.\nThe validation errors (from the research paper) for various batch sizes are presented in table-1 below:\n\nWhile BN performs slightly better than GN for batch size 32, GN performs better for all lower batch sizes."
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#other-normalization-techniques",
    "href": "posts/2020-08-09-groupnorm.html#other-normalization-techniques",
    "title": "Group Normalization",
    "section": "2 Other Normalization Techniques",
    "text": "2 Other Normalization Techniques\nGroup Normalization isn’t the first technique that was proposed to overcome the drawback of BN. There are also several other techniques such as Layer Normalization, Instance Normalization and others mentioned in the references of this blog post.\nBut, GN is the first technique to achieve comparable validation error rates as compared to BN.\nIn this section we look at the most popular normalization tecniques namely - Layer Normalization (LN), Instance Normalization (IN), Batch Normalization (BN) and Group Normalization (GN).\n\n2.1 Group Normalization in detail and comparison to other normalization techniques\n\nThe above image presented in the research paper is one of the best ways to compare the various normalization techniques and get an intuitive understanding for GN.\nLet’s consider that we have a batch of dimension (N, C, H, W) that needs to be normalized.\nHere, - N: Batch Size - C: Number of Channels - H: Height of the feature map - W: Width of the feature map\nEssentially, in BN, the pixels sharing the same channel index are normalized together. That is, for each channel, BN computes the mean and std deviation along the (N, H, W) axes. As we can see, the group statistics depend on N, the batch size.\nIn LN, the mean and std deviation are computed for each sample along the (C, H, W) axes. Therefore, the calculations are independent of the batch size.\nIn IN, the mean and std deviation are computed for each sample and each channel along the (H, W) axes. Again, the calculations are independent of batch size.\n\n\n2.2 Group Normalization Explained\n\nFinally, for group norm, the batch is first divided into groups (32 by default, discussed later). The batch with dimension (N, C, W, H) is first reshaped to (N, G, C//G, H, W) dimensions where G represents the number of groups. Finally, the mean and std deviation are calculated along the groups, that is (H, W) and along C//G channels. This is also illustrated very well in fig-4.\nOne key thing to note here, if C == G, that is the number of groups are set to be equal to the number of channels (one channel per group), then GN becomes IN.\nAnd if, G == 1, that is number of groups is set to 1, GN becomes LN.\nI would like for the reader to take a minute here and make sure that he/she understands the differences between these normalization techniques mentioned above.\n\n\n2.3 Benefits of Group Normalization over other techniques\nAlso, it is important to note that GN is less restricted than LN, because in LN it is assumed that all channels in a layer make “equal contributions” whereas GN is more flexible because in GN, each group of channels (instead of all of them) are assumed to have shared mean and variance - the model still has flexibility of learning a different distribution for each group.\nAlso, GN is slightly better than IN because IN normalizes accross each sample for each channel, therefore, unlike GN, it misses the opportunity of exploiting the channel dependence.\n\nTherefore, due to the reasons discussed above, we can see that the validation and training errors for GN are lower than those for LN and IN."
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#number-of-groups-hyperparameter-in-group-normalization",
    "href": "posts/2020-08-09-groupnorm.html#number-of-groups-hyperparameter-in-group-normalization",
    "title": "Group Normalization",
    "section": "3 Number of Groups hyperparameter in Group Normalization",
    "text": "3 Number of Groups hyperparameter in Group Normalization\nOne key hyperparameter in Group Normalization is the number of groups to divide the channels into.\n\nThe authors of the research paper ran an experiment to train ResNet-50 model on Imagenet dataset using various number of groups.\nAs can be seen in table-2, setting number of groups to 32 achieves the lowest validation error.\nIn the bottom part of table-2, the authors set a fixed number of channels per group. Essentially, since each layer in a deep learning model can have various number of channels, this means there are varying number of groups per layer. Setting 16 channels per group achieved the lowest score.\n\n3.1 Group Division Experiments Explained\n\nLet’s understand what’s going on with help of VGGNet. As can be seen, there are varying number of channels in different layers of VGGNet (this is also the case for other deep learning models like ResNet, DenseNet etc). The authors essentially in the first experiment, divide each layer into 32 groups. Thus for layer 2 of VGGNet with 128 #channels, there are 128//32, that is, 4 channels per group if group number is set to 32. The authors ran this experiments for varying number of groups and found for number of groups set to 32 to have the lowest validtion error.\nFor the second experiment, the authors set the number of channels per group fixed. For example, if number of channels per group was set to 16, then the second layer with 128 channels had 128//16, that is, 8 groups and the third layer with 256 channels had 256//16, 16 groups and so on. The authors found setting 16 channels per group to have to have the lowest validation error."
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#effect-of-group-normalization-on-deeper-models",
    "href": "posts/2020-08-09-groupnorm.html#effect-of-group-normalization-on-deeper-models",
    "title": "Group Normalization",
    "section": "4 Effect of Group Normalization on deeper models",
    "text": "4 Effect of Group Normalization on deeper models\nThe authors also ran experiments and trained ResNet-101 architecture for batch size 32 and compared the validation errors with BN and GN implementation. The authors found the BN baseline to have 22.0% validation error and the GN counterpart to have 22.4% validation error. Also, for batch size 2, the authors found the GN error to be 23.0% which is still a very decent result considering the very small batch size.\n\nThus, I think from the results of this experiment, it is safe to say that GN with smaller batch sizes also works for larger models."
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#implementation-of-groupnorm",
    "href": "posts/2020-08-09-groupnorm.html#implementation-of-groupnorm",
    "title": "Group Normalization",
    "section": "5 Implementation of GroupNorm",
    "text": "5 Implementation of GroupNorm\nFinally, we are now ready to look at the implementation of GN.\nThe following snippet of code has been provided in the research paper:\ndef GroupNorm(x, gamma, beta, G, eps=1e−5): \n    # x: input features with shape [N,C,H,W] \n    # gamma, beta: scale and offset, with shape [1,C,1,1] \n    # G: number of groups for GN\n    N, C, H, W = x.shape \n    x = tf.reshape(x, [N, G, C // G, H, W])\n    mean, var = tf.nn.moments(x, [2, 3, 4], keep dims=True) \n    x = (x − mean) / tf.sqrt(var + eps)\n    x = tf.reshape(x, [N, C, H, W]) \n    return x ∗ gamma + beta\nEssentially, the authors reshape the batch and divide into groups with C // G channels per group where, - C: number of channels - G: number of groups\nFinally, as discussed in this section, the authors normalize along the (C//G, H, W) dimension and return the result after reshaping the batch back to (N, C, H, W).\nI hope that by this time, the implementation should be clear to the reader. If it isn’t, either I have not explained GN very well, or I kindly ask the reader to go back to Group Normalization Explained section and have a quick re-read.\nFinally, we could rewrite GN in PyTorch like so:\nimport torch\nimport torch.nn as nn\n\nclass GroupNorm(nn.Module):\n    def __init__(self, num_features, num_groups=32, eps=1e-5):\n        super(GroupNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(1,num_features,1,1))\n        self.beta = nn.Parameter(torch.zeros(1,num_features,1,1))\n        self.num_groups = num_groups\n        self.eps = eps\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = x.view(N, self.num_groups ,-1)\n        mean = x.mean(-1, keepdim=True)\n        var = x.var(-1, keepdim=True)\n\n        # normalize\n        x = (x-mean) / (var+self.eps).sqrt()\n        x = x.view(N,C,H,W)\n        return x * self.gamma + self.beta\nPyTorch also inherently supports GroupNorm and can be used by using nn.GroupNorm.\nHaving implemented GN in PyTorch and Tensorflow, we are now ready to run our own experiments and see the results for ourselves in the next section."
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#does-groupnorm-really-work-in-practice",
    "href": "posts/2020-08-09-groupnorm.html#does-groupnorm-really-work-in-practice",
    "title": "Group Normalization",
    "section": "6 Does GroupNorm really work in practice?",
    "text": "6 Does GroupNorm really work in practice?\nPersonally, I wanted to try a little experiment of my own to compare GN with BN and corroborate the findings in the GN research paper.\nYou can find the experiment in this notebook here.\nBasically, in the experiment, I trained two ResNet-34 architectures on the Pets dataset - one with BN and other with GN. To my surprise, I found that simply replacing BatchNorm with GroupNorm led to sub-optimal results and the model with GroupNorm used as the normalization layer performed much worse than the model normalized with BatchNorm layer even for a very small batch size of 4. This was very different to the results reported in fig-1.\nThanks to Sunil Kumar who pointed me to Big Transfer (BiT): General Visual Representation Learning research paper where I noticed that the researchers used a combination of Weight Standardization and GN to achieve SOTA results. So I tried this out with the implementation of Weight Standardization as in the official repository here and very quickly I was able to replicate the results with GN + WS performing significantly better than BN for batch size of 1 here."
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#conclusion",
    "href": "posts/2020-08-09-groupnorm.html#conclusion",
    "title": "Group Normalization",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nI hope that I have been clear in my explaination of Group Normalization, and also through my experiments, I have been able to provide a way for you to implement GN in PyTorch and Tensorflow and run experiments of your own.\nAs always, constructive feedback is always welcome at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#references",
    "href": "posts/2020-08-09-groupnorm.html#references",
    "title": "Group Normalization",
    "section": "8 References",
    "text": "8 References\n\nGroup Normalization by He et al\nBatch Normalization by Ioffe et al\nInstance Normalization: The Missing Ingredient for Fast Stylization\nLayer Normalization\nWeight Standardization\nImplementation of Weight Standardization from the official repository\nDeep Residual Learning for Image Recognition"
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#credits",
    "href": "posts/2020-08-09-groupnorm.html#credits",
    "title": "Group Normalization",
    "section": "9 Credits",
    "text": "9 Credits\nThanks to @AryMob for pointing out errata in this post."
  },
  {
    "objectID": "posts/2020-06-29-FocalLoss.html",
    "href": "posts/2020-06-29-FocalLoss.html",
    "title": "What is Focal Loss and when should you use it?",
    "section": "",
    "text": "In this blogpost we will understand what Focal Loss and when is it used. We will also take a dive into the math and implement it in PyTorch."
  },
  {
    "objectID": "posts/2020-06-29-FocalLoss.html#where-was-focal-loss-introduced-and-what-was-it-used-for",
    "href": "posts/2020-06-29-FocalLoss.html#where-was-focal-loss-introduced-and-what-was-it-used-for",
    "title": "What is Focal Loss and when should you use it?",
    "section": "1 Where was Focal Loss introduced and what was it used for?",
    "text": "1 Where was Focal Loss introduced and what was it used for?\nBefore understanding what Focal Loss is and all the details about it, let’s first quickly get an intuitive understanding of what Focal Loss actually does. Focal loss was implemented in Focal Loss for Dense Object Detection paper by He et al. \nFor years before this paper, Object Detection was actually considered a very difficult problem to solve and it was especially considered very hard to detect small size objects inside images. See example below where the model doesn’t predict anything for the motorbike which is of relatively smaller size compared to other images.\n\nThe reason why in the image above, the bike is not predicted by the model is because this model was trained using Binary Cross Entropy loss which really asks the model to be confident about what is predicting. Whereasm, what Focal Loss does is that it makes it easier for the model to predict things without being 80-100% sure that this object is “something”. In simple words, giving the model a bit more freedom to take some risk when making predictions. This is particularly important when dealing with highly imbalanced datasets because in some cases (such as cancer detection), we really need to model to take a risk and predict something even if the prediction turns out to be a False Positive.\nTherefore, Focal Loss is particularly useful in cases where there is a class imbalance. Another example, is in the case of Object Detection when most pixels are usually background and only very few pixels inside an image sometimes have the object of interest.\nOK - so focal loss was introduced in 2017, and is pretty helpful in dealing with class imbalance - great!\nBy the way, here are the predictions of the same model when trained with Focal Loss.\n\nThis might be a good time to actually analyse the two and observe the differences. This will help get an intuitive understanding about Focal Loss."
  },
  {
    "objectID": "posts/2020-06-29-FocalLoss.html#so-why-did-that-work-what-did-focal-loss-do-to-make-it-work",
    "href": "posts/2020-06-29-FocalLoss.html#so-why-did-that-work-what-did-focal-loss-do-to-make-it-work",
    "title": "What is Focal Loss and when should you use it?",
    "section": "2 So, why did that work? What did Focal Loss do to make it work?",
    "text": "2 So, why did that work? What did Focal Loss do to make it work?\nSo now that we have seen an example of what Focal Loss can do, let’s try and understand why that worked. The most important bit to understand about Focal Loss is the graph below:\n\nIn the graph above, the “blue” line represents the Cross Entropy Loss. The X-axis or ‘probability of ground truth class’ (let’s call it pt for simplicity) is the probability that the model predicts for the ground truth object. As an example, let’s say the model predicts that something is a bike with probability 0.6 and it actually is a bike. The in this case pt is 0.6. Also, consider the same example but this time the object is not a bike. Then pt is 0.4 because ground truth here is 0 and probability that the object is not a bike is 0.4 (1-0.6).\nThe Y-axis is simply the loss value given pt.\nAs can be seen from the image, when the model predicts the ground truth with a probability of 0.6, the Cross Entropy Loss is still somewhere around 0.5. Therefore, to reduce the loss, our model would have to predict the ground truth label with a much higher probability. In other words, Cross Entropy Loss asks the model to be very confident about the ground truth prediction.\nThis in turn can actually impact the performance negatively:\n\nThe Deep Learning model can actually become overconfident and therefore, the model wouldn’t generalize well.\n\nThis problem of overconfidence is also highlighted in this excellent paper Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration. Also, Label Smoothing which was introduced as part of Rethinking the Inception Architecture for Computer Vision is another way to deal with the problem.\nFocal Loss is different from the above mentioned solutions. As can be seen from the graph Compare FL with CE, using Focal Loss with γ>1 reduces the loss for “well-classified examples” or examples when the model predicts the right thing with probability > 0.5 whereas, it increases loss for “hard-to-classify examples” when the model predicts with probability < 0.5. Therefore, it turns the models attention towards the rare class in case of class imbalance.\nThe Focal Loss is mathematically defined as:\n\nScary? It’s rather quite intuitive - read on :)"
  },
  {
    "objectID": "posts/2020-06-29-FocalLoss.html#alpha-and-gamma",
    "href": "posts/2020-06-29-FocalLoss.html#alpha-and-gamma",
    "title": "What is Focal Loss and when should you use it?",
    "section": "3 Alpha and Gamma?",
    "text": "3 Alpha and Gamma?\nSo, what the hell are these alpha and gamma in Focal Loss? Also, we will now represent alpha as α and gamma as γ.\nHere is my understanding from fig-3: > γ controls the shape of the curve. The higher the value of γ, the lower the loss for well-classified examples, so we could turn the attention of the model more towards ’hard-to-classify examples. Having higher γ extends the range in which an example receives low loss.\nAlso, when γ=0, this equation is equivalent to Cross Entropy Loss. How? Well, for the mathematically inclined, Cross Entropy Loss is defined as:\n\nAfter some refactoring and defining pt as below:\n\nPutting eq-3 in eq-2, our Cross Entropy Loss therefore, becomes:\n\nTherefore, at γ=0, eq-1 becomes equivalent to eq-4 that is Focal Loss becomes equivalent to Cross Entropy Loss. Here is an excellent blogpost that explains Cross Entropy Loss.\nOk, great! So now we know what γ does, but, what does α do?\nAnother way, apart from Focal Loss, to deal with class imbalance is to introduce weights. Give high weights to the rare class and small weights to the dominating or common class. These weights are referred to as α.\n\nAdding these weights does help with class imbalance however, the focal loss paper reports: > The large class imbalance encountered during training of dense detectors overwhelms the cross entropy loss. Easily classified negatives comprise the majority of the loss and dominate the gradient. While α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples.\nWhat the authors are trying to explain is this: > Even when we add α, while it does add different weights to different classes, thereby balancing the importance of positive/negative examples - just doing this in most cases is not enough. What we also want to do is to reduce the loss of easily-classified examples because otherwise these easily-classified examples would dominate our training.\nSo, how does Focal Loss deal with this? It adds a multiplicative factor to Cross Entropy loss and this multiplicative factor is (1 − pt)**γ where pt as you remember is the probability of the ground truth label.\nFrom the paper for Focal Loss: > We propose to add a modulating factor (1 − pt)**γ to the cross entropy loss, with tunable focusing parameter γ ≥ 0.\nReally? Is that all that the authors have done? That is to add (1 − pt)**γ to Cross Entropy Loss? Yes!! Remember eq-4?"
  },
  {
    "objectID": "posts/2020-06-29-FocalLoss.html#how-to-implement-this-in-code",
    "href": "posts/2020-06-29-FocalLoss.html#how-to-implement-this-in-code",
    "title": "What is Focal Loss and when should you use it?",
    "section": "4 How to implement this in code?",
    "text": "4 How to implement this in code?\nWhile TensorFlow provides this loss function here, this is not inherently supported by PyTorch so we have to write a custom loss function.\nHere is the implementation of Focal Loss in PyTorch:\nclass WeightedFocalLoss(nn.Module):\n    \"Non weighted version of Focal Loss\"\n    def __init__(self, alpha=.25, gamma=2):\n        super(WeightedFocalLoss, self).__init__()\n        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        targets = targets.type(torch.long)\n        at = self.alpha.gather(0, targets.data.view(-1))\n        pt = torch.exp(-BCE_loss)\n        F_loss = at*(1-pt)**self.gamma * BCE_loss\n        return F_loss.mean()\nIf you’ve understood the meaning of alpha and gamma then this implementation should also make sense. Because, similar to the paper it is simply adding a factor of at*(1-pt)**self.gamma to the BCE_loss or Binary Cross Entropy Loss."
  },
  {
    "objectID": "posts/2020-06-29-FocalLoss.html#credits",
    "href": "posts/2020-06-29-FocalLoss.html#credits",
    "title": "What is Focal Loss and when should you use it?",
    "section": "5 Credits",
    "text": "5 Credits\nPlease feel free to let me know via twitter if you did end up trying Focal Loss after reading this and whether you did see an improvement in your results! Thanks for reading!\n\nThe implementation of Focal Loss has been adapted from here.\nfig-1 and fig-2 are from the Fastai 2018 course Lecture-09!"
  },
  {
    "objectID": "posts/2020-09-06-siimacr.html",
    "href": "posts/2020-09-06-siimacr.html",
    "title": "Top 100 solution - SIIM-ACR Pneumothorax Segmentation",
    "section": "",
    "text": "This week I spent most of my time implementing a solution for SIIM-ACR Pneumothorax Segmentation Kaggle competition and in today’s blog post, we will looking at how to work on Image Segmentation based problems in Pytorch with this competition serving as a useful example.\nThis blog post assumes the reader to have some idea about Image Segmentation, U-Net and PyTorch.\nThis post is based on a “code-first” approach with an aim to provide a reproducible training script that could then be used for other projects or Kaggle competitions. We train the model using pure PyTorch.\nFor a complete working notebook to follow along step-by-step refer here.\nFirst, we download the dataset from Kaggle and convert the .dcm files to .png images for both masks and radiography images. Next, we create a PyTorch Dataset that returns the image and mask as a dictionary. Finally, we create the model using the wonderful Pytorch Image Segmentation library by Pavel Yakubovskiy(https://github.com/qubvel). Once we have the model, we create our training and validation scripts and train it using ComboLoss that was also used by the competition winner - Anuar Aimoldin. Winning solution can be referenced here.\nWhile I found lots of blog posts showing Image Segmentation using the wonderful fastai library, there were fewer blog posts that attempted to do this task using pure Pytorch. I try the latter, because I have done the former many times and want to understand the minor details of training an Image Segmentation model in PyTorch. While it was much harder than training a model in fastai, getting stuck in problems helped me further expand my understanding on Image Segmentation.\nUsing pure PyTorch and some help from existing scripts, I was able to create a working solution that then landed in top-100 position on the private leaderboard with a dice score of 0.8421.\nWith that being said, let’s get started and look at the code pieces required to create such a solution step-by-step."
  },
  {
    "objectID": "posts/2020-09-06-siimacr.html#problem-statement",
    "href": "posts/2020-09-06-siimacr.html#problem-statement",
    "title": "Top 100 solution - SIIM-ACR Pneumothorax Segmentation",
    "section": "2 Problem Statement",
    "text": "2 Problem Statement\n\n2.1 What is Image Segmentation?\nBefore we look at what we are trying to solve in SIIM-ACR Competition, let’s first look at Image Segmentation. Image segmentation can also be thought of as pixel-wise classification where, for each pixel in the input image, we are predicting whether it is a part of the segmentation mask or not. We predict 1 if we think the pixel is a part of the segmentation mask and 0 otherwise.\n\nLet’s say we have a 8x8 image, therefore as shown in fig-1, the idea is to predict a label - 0 or 1 for each pixel in the input image. This is precisely what Image Segmentation is.\n\n\n2.2 SIIM-ACR Overview\nFor the SIIM-ACR Competition, the competitors were asked to build an image-segmentation model that can classify and segment pneumothorax. From the description on Kaggle for SIIM-ACR Competition:\n\nIn this competition, you’ll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images.\n\nSpecfically, given a chest radiographic image (fig-2 left), the idea is to predict a mask that shows the position of pneumothorax in an image (fig-2 right).\n\nNow that we have looked at what we are trying to solve, let’s start with building our solution."
  },
  {
    "objectID": "posts/2020-09-06-siimacr.html#preparing-the-dataset",
    "href": "posts/2020-09-06-siimacr.html#preparing-the-dataset",
    "title": "Top 100 solution - SIIM-ACR Pneumothorax Segmentation",
    "section": "3 Preparing the Dataset",
    "text": "3 Preparing the Dataset\nThe first step is almost always getting the data ready. In the competition, the data has been provided as .dcm files. .dcm is short for DICOM format. DICOM is standard image format for medical-imaging information and related data. It consists of the image and associated metadata of the patient such as sex, age etc as well. Don’t worry if you haven’t seen DICOM before, there’s always a first-time.\nI first learned about it through Kaggle too - specifically, RSNA Intracranial Hemorrhage Detection. There’s also this wonderful Some DICOM gotchas to be aware of (fastai) Kaggle notebook by none other than Jeremy Howard himself that serves as an excellent introduction to DICOM format.\nAlso, fastai library, has a medical submodule that makes it really easy to interact with .dcm files.\nFor us, we simply download the files from Kaggle first, and then re-use the prepare_png.py script by SIIM-ACR competition winner - Anuar Aimoldin. Let’s see how.\n\n3.1 Downloading the .dcm files and converting to .png\nThe following lines of code will first download the dataset from Kaggle - this includes all .dcm files for stage-two of the competition including train and test folders. Next, the prepare_png.py script extracts the pixel information (or simply said “the image”) from the .dcm file, and stores the image as .png file in the output directory.\nexport OUTPUT_DIR='<path_to_output_dir>'\nmkdir siim-acr && cd siim-acr \nmkdir data && cd data\nkaggle datasets download -d seesee/siim-train-test\nunzip siim-train-test.zip \nmv siim/* . \nrmdir siim\nmkdir ../src/ && cd ../src\ngit clone https://github.com/sneddy/pneumothorax-segmentation\npython pneumothorax-segmentation/unet_pipeline/utils/prepare_png.py -train_path ../data/dicom-images-train/ -test_path ../data/dicom-images-test/ -out_path $OUTPUT_DIR -img_size 512 -rle_path ../data/train-rle.csv\nRunning the above, should create a folder siim-acr in your current directory that has the following structure:\n├── data\n│   ├── dataset512\n│   ├── dicom-images-test\n│   └── dicom-images-train\n└── src\n    └── pneumothorax-segmentation\nThe dicom-images-train and dicom-images-test directory consist of all the .dcm files provided by Kaggle. The dataset512 consists of all the .png files that have been extracted from the .dcm files. This includes train image files and also label masks extracted as .png images. pneumothorax-segmentation is the GitHub repo that contains the prepare_png.py script.\nTherefore, now that our data has been downloaded and all files converted to .png images, we are ready to start building our Dataset class.\n\n\n3.2 PyTorch Dataset\nIn PyTorch, a Dataset class almost always returns the image and label. Since, this is an image segmentation task, the label, in this case is also an image - specifically the image of the mask as in fig-2 (right).\nOne thing I didn;t mention in the previous section, is that when we downloaded the files, we were also provided with a train-rle.csv file which consists of two columns - ImageId and EncodedPixels.\n\nEach row consists of an ImageId and EncodedPixels - which are the 512x512 masks as in fig-2 with run-length-encoding to keep the size of the .csv file small. You can read more about run-length-encoding here.\nSo, to create the dataset, we read this .csv file as rle_df, and store it to df attribute of the Dataset class. In order to get an item from the dataset, we could get the corresponding image_id based on index i. Once we have the image_id, we could get the img_path and mask_path too and return the image and mask after applying the augmentations if present. If the augmentations are present, this class returns two image and mask tensors of shape (3, 512, 512) and (1, 512, 512) respectively or numpy arrays of the same shape otherwise.\nclass Dataset():\n    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None):\n        self.df             = rle_df\n        self.image_base_dir = image_base_dir\n        self.masks_base_dir = masks_base_dir\n        self.image_ids      = rle_df.ImageId.values\n        self.augmentation   = augmentation\n    \n    def __getitem__(self, i):\n        image_id  = self.image_ids[i]\n        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n        mask_path = os.path.join(self.masks_base_dir, image_id+'.png')\n        image     = cv2.imread(img_path, 1)\n        mask      = cv2.imread(mask_path, 0)     \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = {\"image\": image, \"mask\": mask}\n            sample = self.augmentation(**sample)\n            image, mask = sample['image'], sample['mask']\n\n        return {\n            'image': image, \n            'mask' : mask\n        }\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n\n3.3 Five-fold splits\nOne thing we haven’t done yet is to create train-val splits. This is standard for training any model. For this competition, we will doing a five-fold stratified split using StratifiedKFold from sklearn.\nRLE_DF = pd.read_csv('<path_to_train_rle.csv'>)\nkf = StratifiedKFold()\n \nif CREATE_FIVE_FOLDS:\n    RLE_DF['has_mask'] = 0\n    RLE_DF.loc[RLE_DF.EncodedPixels!='-1', 'has_mask'] = 1\n    RLE_DF['kfold']=-1\n    for fold, (train_index, test_index) in enumerate(kf.split(X=RLE_DF.ImageId, y=RLE_DF.has_mask)):\n            RLE_DF.loc[test_index, 'kfold'] = fold\n    RLE_DF.to_csv('<path_to_target_file.csv>', index=False)\nRunning the above script creates a new .csv file which looks something like:\n\nAs can be seen, each image has been assigned to a specific fold. Thus, all our data has been divided into five folds. Once we have this, then getting the train and validation dataframes is as simple as running:\nTRAIN_DF = RLE_DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\nVAL_DF   = RLE_DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\nlen(TRAIN_DF), len(VAL_DF)\n\n>> (10364, 2590)\nTherefore, we have split the train-rle.csv into training and validation splits with 10,364 files in training split and 2590 files in validation split.\n\n\n3.4 Train and Val Datasets and Dataloaders\nNow that we have our train and validation splits, creating train and validation datasets and dataloaders is a piece of cake.\nTRAIN_BATCH_SIZE = 8 \nVALID_BATCH_SIZE = 16\n\n# datasets\ntrain_dataset = Dataset(TRAIN_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>') \nval_dataset   = Dataset(VAL_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>')\n# dataloaders\ntrain_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \n                              shuffle=True, num_workers=4)\nval_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=4)\nWe already have a Dataset class before, that accepts a DataFrame and returns image and mask numpy arrays or tensors based on augmentations. Since, we have not provided any augmentations so far, the returned values would be numpy arrays.\n\n\n3.5 Train and Valid Augmentations\nWe could also add image augmentations by simply passing augmentations to the Dataset class.\nLet’s first define train and validation augmentations using the brilliant albumentations library.\nimport albumentations as albu\n# Train transforms\nTRN_TFMS = albu.Compose([\n    albu.HorizontalFlip(),\n    albu.Rotate(10),\n    albu.Normalize(),\n    ToTensor(),\n])\n​\n# Test transforms\nVAL_TFMS = albu.Compose([\n    albu.Normalize(),\n    ToTensor(),\n])\nThe train transforms add random horizontal flipping, random rotation and also normalize the image and mask to have values between (0, 1) instead of (0, 255). We also convert the numpy arrays to tensor using ToTensor() method in albumentations.\nThe validation transform only normalizes and converts the numpy arrays to tensors.\nNow that we have defined the augmentations, we can create the datasets and dataloaders like so:\nTRAIN_BATCH_SIZE = 8 \nVALID_BATCH_SIZE = 16\n\n# datasets\ntrain_dataset = Dataset(TRAIN_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>', augmentation=TRN_TFMS) \nval_dataset   = Dataset(VAL_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>', augmentation=VAL_TFMS)\n# dataloaders\ntrain_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \n                              shuffle=True, num_workers=4)\nval_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=4)\nGreat! So far so good. Until now we have created the Dataset class that accepts a DataFrame and returns image and mask as a Python dictionary. We have also defined some train and validation augmentations and added them to training and validation datasets.\n\n\n3.6 Visualize\nLet’s visualize some images now. First, we create simple helper functions from torchvision examples like so:\ndef matplotlib_imshow(img, one_channel=False):\n    fig,ax = plt.subplots(figsize=(10,6))\n    ax.imshow(img.permute(1,2,0).numpy())\n\ndef visualize(**images):\n    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy \n    n = len(images)\n    plt.figure(figsize=(16, 8))\n    image, mask = images['image'], images['mask']\n    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n    if mask.max()>0:\n        plt.imshow(mask.squeeze(0), alpha=0.25)\n    plt.show()\nAnd now visualizing an image with mask is as simple as:\n# plot one image with mask \nvisualize(**train_dataset[1])\n\nAnd to visualize multiple train images, we could do something like:\nimages, masks = next(iter(train_dataloader))['image'], next(iter(train_dataloader))['mask']\nimg_grid = torchvision.utils.make_grid(images[:9], nrow=3, normalize=True)\nmatplotlib_imshow(img_grid)\n\nAs can be seen from fig-3, some images are rotated, some are flipped - this is due to the train image augmentations. This way, our model sees a slightly different version of the same image every time in an epoch and will therefore be able to generalize better.\nNow, that our dataset and dataloaders with image augmentations are done, we are ready to look into model building, training and validation."
  },
  {
    "objectID": "posts/2020-09-06-siimacr.html#model---training-and-validation",
    "href": "posts/2020-09-06-siimacr.html#model---training-and-validation",
    "title": "Top 100 solution - SIIM-ACR Pneumothorax Segmentation",
    "section": "4 Model - Training and Validation",
    "text": "4 Model - Training and Validation\nTo create our U-Net based model, we will be using the wonderful Segmentation Models library in PyTorch by Pavel Yakubovskiy. I promise to do a future blog post on creating a U-Net structure from scratch with a ResNet encoder branch, but for simplicity, for this introductory post, using the Segmentation Models library is a much easier option. If you’re keen to create a U-Net architecture from scratch, here is an excellent video by Abhishek Thakur.\nThanks to the library, creating our segmentation model architecture is as simple as:\nimport segmentation_models_pytorch as smp\n\nmodel = smp.Unet(\n    encoder_name='se_resnext50_32x4d', \n    encoder_weights='imagenet', \n    classes=1, \n    activation=None,\n)\nUsually, there is a Sigmoid activation U-Net, but in our case, we do not add any activation and set it to None because the Sigmoid activation is already a part of the loss function that we use to train the model.\nGiven an input batch of images of size (8, 3, 512, 512) the model outputs masks of batch dimensions (8, 1, 512, 512) which represents the predicted segmentation masks for each of the 8 images.\nUsually, one of the common loss functions for Image Segmentation is pixel wise Cross Entropy but for our introductory post today we will be using the same loss function as the winner of the competition.\n\n4.1 Loss Function\nimport sys; sys.path.append('<path to `pneumothorax-segmentation/unet_pipeline/`>')\nfrom Losses import ComboLoss\n\ncriterion = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\nThis loss function is a combination of Binary Cross Entropy, Dice Loss and Focal Loss. I have written about Focal Loss before here.\nFrom my understanding, we use a combination loss for stable training. Since the evaluation metric is dice, we are using dice loss here. We use bce for pixel wise comparison between predictions and ground truth mask, and focal loss due to the high class imbalance - the actual mask is only a tiny part of the whole image. I again promise to do a future blog post on the various loss functions, but for now, for simplicity we simply re-use the ComboLoss to train our model with (3, 1, 4) weights to stabilize training.\nLet’s also quickly define the optimizer and scheduler.\nimport torch\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)\nGreat, so far, we have our model, loss function and also the optimizer and scheduler defined. We are now ready to train our model.\n\n\n4.2 Model Training\nI reused the same training loop as in the ISIC competition below:\nclass AverageMeter:\n    \"keep record of batch scores and loss\"\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=1, device='cuda'):\n    losses = AverageMeter()\n    model = model.to(device)\n    model.train()\n    if accumulation_steps > 1: \n        optimizer.zero_grad()\n    tk0 = tqdm(train_loader, total=len(train_loader))\n    for b_idx, data in enumerate(tk0):\n        for key, value in data.items():\n            data[key] = value.to(device)\n        if accumulation_steps == 1 and b_idx == 0:\n            optimizer.zero_grad()\n        out  = model(data['image'])\n        loss = loss_fn(out, data['mask'])\n        with torch.set_grad_enabled(True):\n            loss.backward()\n            if (b_idx + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n        losses.update(loss.item(), train_loader.batch_size)\n        tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n    return losses.avg\nThe above training loop is a basic one with possible gradient accumulation due to the small batch size in Image Segmentation based problems. I personally trained my models without gradient accumulation, but you could very well train your models by passing in accumulation_steps=N where N is the number of times you want to accumulate the gradients, thereby, increasing the batch size.\nThe train_one_epoch function accepts a dataloader, model, optimizer and loss_fn. We first move the model to cuda and put the model in train mode. Next, we iterate over the dataloader to get images and masks and move them to cuda as well.\nFinally, we pass the batch of images to get predictions. Remember the output size is (8, 1, 512, 512) for three channel input images of height and width 512 and batch size 8. Once we have the predictions, as is standard for any training loop, we can calculate the loss and backpropogate to update the model weights. Finally, we also display the average loss in the training progress bar using tqdm.\nTo train the model we can simply run:\ntrain_loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n\n\n4.3 Model Validation\nHardly ever would one just be concerned with model training. It is imperative to have a good validation step to check model performance. In this step we create the model validation loop which is similar to the training loop. s\ndef evaluate(valid_loader, model, device='cuda', metric=dice_metric):\n    losses = AverageMeter()\n    model = model.to(device)\n    model.eval()\n    tk0 = tqdm(valid_loader, total=len(valid_loader))\n    with torch.no_grad():\n        for b_idx, data in enumerate(tk0):\n            for key, value in data.items():\n                data[key] = value.to(device)\n            out   = model(data['image'])\n            out   = torch.sigmoid(out)\n            dice  = metric(out, data['mask']).cpu()\n            losses.update(dice.mean().item(), valid_loader.batch_size)\n            tk0.set_postfix(dice_score=losses.avg)\n    return losses.avg\nThe above validation loop accepts a valid dataloader, trained model some metric to check model performance. It first moves the model to GPU, and puts the model in eval mode. Next we iterate over the dataloader to get the images and masks and pass the images through the model to get mask predictions. Finally, we take the sigmoid to convert raw logits to be in range (0, 1). Finally, we can calculate the dice score and display it in the validation progress bar with tqdm."
  },
  {
    "objectID": "posts/2020-09-06-siimacr.html#conclusion",
    "href": "posts/2020-09-06-siimacr.html#conclusion",
    "title": "Top 100 solution - SIIM-ACR Pneumothorax Segmentation",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nI hope that today, I was able to provide a good introductory and reporoducible script to train Image Segmentation based models. This was also my first experience with training image segmentation models and hope to write more about Unet architecture, loss functions and review image segmentation research papers in the future.\nFor a complete working notebook with a submission script to Kaggle and pretrained models, refer here. Running the script should take around 2 minutes on a P100.\nAs usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-09-06-siimacr.html#credits",
    "href": "posts/2020-09-06-siimacr.html#credits",
    "title": "Top 100 solution - SIIM-ACR Pneumothorax Segmentation",
    "section": "6 Credits",
    "text": "6 Credits\nWe made use of prepare_png.py script from Anuar Aymoldin’s 1st place solution and also directly used the loss function.\nThe model was created using segmentation_models.pytorch library."
  },
  {
    "objectID": "posts/2021-01-13-efficientdet-pytorch.html",
    "href": "posts/2021-01-13-efficientdet-pytorch.html",
    "title": "The EfficientDet Architecture in PyTorch",
    "section": "",
    "text": "This blog post is a direct continuation of my previous blog post explaining EfficientDets. In my previous post, we looked and understood what’s inside an EfficientDet and also read about the various components such as BiFPN and Compound Scaling that make an EfficientDet network so powerful.\nToday, our focus will be to build on top of that knowledge and showcase how to implement the network using PyTorch step-by-step. Throughout this blog post I have added some side notes to be able to explain things better.\nNOTE: The code implementations shared below are not my own. All code shown below has been directly copied from Ross Wightman’s wonderful repo efficientdet-pytorch. efficientdet-pytorch makes heavy use of timm to create the backbone network and also for several other operations.\nThere are few notable things in the architecture above that we must look at before starting with the implementation: 1. The BiFPN Layer only interacts with the feature maps at level 3-7 of the backbone network. 2. EfficientNets are used as the backbone network for EfficientDets. 3. There are bottom-up and top-down connections between the feature maps at different levels. Thus, we would need to be able to Upsample or Downsample the features. 4. The BiFPN Network consists of multiple BiFPN Layers and the number of BiFPN layers depends on the size of the EfficientDet (compound scaling). 5. The EfficientDet Architecture consists of two main components - Backbone + BiFPN network. 6. Each “Node” inside a BiFPN layer can accept either 2 or 3 inputs and it combines them to produce a single output.\nWe are going to be using a bottom-up approach in coding this time and build the EfficientDet together component by component."
  },
  {
    "objectID": "posts/2021-01-13-efficientdet-pytorch.html#resamplefeaturemap",
    "href": "posts/2021-01-13-efficientdet-pytorch.html#resamplefeaturemap",
    "title": "The EfficientDet Architecture in PyTorch",
    "section": "1 ResampleFeatureMap",
    "text": "1 ResampleFeatureMap\nSo, the first thing we are going to implement is a class called ResampleFeatureMap that is able to upsample or downsample an input feature map based on a parameter called reduction_ratio. This class typically represents the “arrows” in the architecture diagram above.\nclass ResampleFeatureMap(nn.Sequential):\n    def __init__(\n            self, in_channels, out_channels, reduction_ratio=1., pad_type='', downsample=None, upsample=None,\n            norm_layer=nn.BatchNorm2d, apply_bn=False, conv_after_downsample=False, redundant_bias=False):\n        super(ResampleFeatureMap, self).__init__()\n        downsample = downsample or 'max'\n        upsample = upsample or 'nearest'\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.reduction_ratio = reduction_ratio\n        self.conv_after_downsample = conv_after_downsample\n\n        conv = None\n        if in_channels != out_channels:\n            conv = ConvBnAct2d(\n                in_channels, out_channels, kernel_size=1, padding=pad_type,\n                norm_layer=norm_layer if apply_bn else None,\n                bias=not apply_bn or redundant_bias, act_layer=None)\n\n        if reduction_ratio > 1:\n            if conv is not None and not self.conv_after_downsample:\n                self.add_module('conv', conv)\n            if downsample in ('max', 'avg'):\n                stride_size = int(reduction_ratio)\n                downsample = create_pool2d(\n                     downsample, kernel_size=stride_size + 1, stride=stride_size, padding=pad_type)\n            else:\n                downsample = Interpolate2d(scale_factor=1./reduction_ratio, mode=downsample)\n            self.add_module('downsample', downsample)\n            if conv is not None and self.conv_after_downsample:\n                self.add_module('conv', conv)\n        else:\n            if conv is not None:\n                self.add_module('conv', conv)\n            if reduction_ratio < 1:\n                scale = int(1 // reduction_ratio)\n                self.add_module('upsample', Interpolate2d(scale_factor=scale, mode=upsample))\nHere is the general idea - if out_channels is not equal to in_channels, then use a 1x1 convolution operation to make them the same. Also, if the reduction ratio is not equal to 1, then either upsample or downsample the input feature map based on the requirements. If reduction_ratio<1 then, Upsample the input, otherwise if reduction_ratio>1 then, Downsample the input.\nUpsampling or Downsampling in simple terms refers to making the spatial dimensions of the input feature map larger or smaller. Upsampling is generally done using bilinear interpolation and downsampling is generally done using pooling.\nSo an example of using this class, assuming all imports work, would be:\n# downsampling\ninp = torch.randn(1, 40, 64, 64)\nresample = ResampleFeatureMap(in_channels=40, out_channels=112, reduction_ratio=2)\nout = resample(inp)\nprint(inp.shape, out.shape) \n\n>> torch.Size([1, 40, 64, 64]) torch.Size([1, 112, 32, 32])\n\n# upsampling\ninp = torch.randn(1, 40, 64, 64)\nresample = ResampleFeatureMap(in_channels=40, out_channels=112, reduction_ratio=0.5)\nout = resample(inp)\nprint(inp.shape, out.shape) \n\n>> torch.Size([1, 40, 64, 64]) torch.Size([1, 112, 128, 128])\nOne key thing that’s part of this class, is that the class does not have a forward method defined that is common to almost all layers in PyTorch. The reason is that this class inherits from nn.Sequential instead of nn.Module. This class does not need a forward method to be defined and automatically calls the modules defined in this class one by one. That is why we do things like self.add_module inside the ResampleFeatureMap class.\nAnother thing, the convolution operation inside this ResampleFeatureMap calls ConvBnAct2d and not nn.Conv2d. ConvBnAct2d as the name suggests is a Convolution operation followed by Batch Normalization and an Activation function.\nSo, here is the implementation of ConvBnAct2d:\nclass ConvBnAct2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, padding='', bias=False,\n                 norm_layer=nn.BatchNorm2d, act_layer=_ACT_LAYER):\n        super(ConvBnAct2d, self).__init__()\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, padding=padding, bias=bias)\n        self.bn = None if norm_layer is None else norm_layer(out_channels)\n        self.act = None if act_layer is None else act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.act is not None:\n            x = self.act(x)\n        return x\nThe create_conv2d is a function from timm, that creates a nn.Conv2d layer in our case. We won’t go into the source code of this function as it is part of the timm library, which we will look into a series of blog posts later.\nNow, let’s start to get into the tricky bits. Let’s see how could we implement a single BiFpnLayer."
  },
  {
    "objectID": "posts/2021-01-13-efficientdet-pytorch.html#bifpn-layer",
    "href": "posts/2021-01-13-efficientdet-pytorch.html#bifpn-layer",
    "title": "The EfficientDet Architecture in PyTorch",
    "section": "2 BiFPN Layer",
    "text": "2 BiFPN Layer\n\nLooking at the image above, we can see that a BiFPN Layer has Nodes. To be specific, each BiFPN Layer has 5 input Nodes (number 0-4) and 8 internal nodes (number 5-12). The input nodes for the first BiFPN Layer are feature outputs from the EfficientNet Backbone. For the subsequent BiFpn Layers, the feature outputs come from the previous BiFPN Layer.\nAlso, each arrow is the ResampleFeatureMap class where the blue arrows perform DownSampling and the red arrows perform Upsampling. From a code perspective, there are some things that we need to be able to implement the BiFPN Layer: 1. We need to be able to extract the feature maps from the EfficientNet Backbone. timm will do this for us. As you’ll notice later, we call timm.create_model method passing in a parameter called out_indices and also features_only=True. This tells timm to create a model that extracts the required feature maps for us at the correct level. 2. We need to be able to combine the features coming from different nodes at different levels. The class FpnCombine will take care of this for us which we will look at below. 3. We need to define the numbers and structures of the nodes in Python similar to the diagram. > For example, our implementation should know that Node-6 is the intermediate Node at level **P5** and it accepts the outputs of Node-5 and Node-2 as inputs. 4. Not all Nodes accept the same number of inputs. Some accept 2 inputs whereas some nodes (such as 9, 10, 11) accept 3 inputs as shown in the fig-2. We can clearly satisfy this requirement by passing the inputs as a List of tensors. 5. The output features from the EfficientNet-B0backbone at level P3-P5 have 40, 112, 320 number of channels respectively and each spatial dimension is half that of the previous level. > This is important to note: considering an input image of size [3, 512, 512], the size of feature maps at levels P3-P5 would be [40, 64, 64], [112, 32, 32], [320, 16, 16] respectively.\nWith this general understanding, let’s get to work.\nFirst things first, how could our implementation get to know about the numbers and structures of the nodes? There is a function called get_fpn_config that returns a Python dictionary like so:\nfpn_config = get_fpn_config()\nfpn_config \n\n>> {'nodes': [{'reduction': 64, 'inputs_offsets': [3, 4], 'weight_method': 'fastattn'}, {'reduction': 32, 'inputs_offsets': [2, 5], 'weight_method': 'fastattn'}, {'reduction': 16, 'inputs_offsets': [1, 6], 'weight_method': 'fastattn'}, {'reduction': 8, 'inputs_offsets': [0, 7], 'weight_method': 'fastattn'}, {'reduction': 16, 'inputs_offsets': [1, 7, 8], 'weight_method': 'fastattn'}, {'reduction': 32, 'inputs_offsets': [2, 6, 9], 'weight_method': 'fastattn'}, {'reduction': 64, 'inputs_offsets': [3, 5, 10], 'weight_method': 'fastattn'}, {'reduction': 128, 'inputs_offsets': [4, 11], 'weight_method': 'fastattn'}]}\n\nFor now, let’s not worry about where this function comes from. Let’s just consider we know that such a function exists which returns a dictionary output like above. And let’s assume that we need to build our BiFPN Layer using the config returned from the get_fpn_config().\n\nBy looking at the fpn_config, we can see that nodes is a List of Dicts and each item in the List represents a single Node. Specifically, the list represents Nodes 5-12. As can be seen and confirmed with the help of fig-2, Node-5 which is the first item in the List accepts the outputs from Nodes 3 & 4 as represented by {'reduction': 64, 'inputs_offsets': [3, 4], 'weight_method': 'fastattn'} in the List, Node-6 which is the second item in the List accepts the outputs from Nodes 2 & 5 as represented by {'reduction': 32, 'inputs_offsets': [2, 5], 'weight_method': 'fastattn'} in the List and so on..\n\nI repeat, let’s not worry about where the fpn_config comes from but let’s just say there is such a config that god created for us and we will use it to build the BiFPN Layer.\n\nYou might ask what’s this reduction inside the fpn_config? Can you see in fig-2 that there are somethings written like “input, P1/2, P2/4, P3/8…, well the denominator number is the reduction. For example, at level P5, where Node-5 exists, the reduction is 32. What this means is that the spatial dimensions of the feature map at this level are of size H/32 x W/32 where H and W are the original image Height and Width.\nGreat, good work so far! At least now we have a basic structure to build our BiFPN Layer on top of. Also, now we know which Nodes are linked to which other Nodes as defined inside the fpn_config.\nSo, for now, let’s move on to implementing Nodes without implementing the BiFPNLayer first.\n\n2.1 FNode\nNow inside a Node, we need to be able to accept some iputs, combine those together, perform some computation on this combined input and output a tensor.\nclass Fnode(nn.Module):\n    \"\"\" A simple wrapper used in place of nn.Sequential for torchscript typing\n    Handles input type List[Tensor] -> output type Tensor\n    \"\"\"\n    def __init__(self, combine: nn.Module, after_combine: nn.Module):\n        super(Fnode, self).__init__()\n        self.combine = combine\n        self.after_combine = after_combine\n\n    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:\n        return self.after_combine(self.combine(x))\nThis is exactly what this class Fnode does above. As can be seen in the forward method, it accepts List of Tensors, performs the combine to combine the inputs together and also performs the computations after_combine operation on them and returns the output.\nNow what are these combine and after_combine operations? The combine operation is the one that will make sure that the input tensors are changed to be of the same resolution and once they are of the same size, it will combine them together. Remember fig-2? A Node can accept inputs from various other Nodes that might not necessarily be at the same level. Thus there might be a need to do a resampling operation before we can combine the inputs to make them be of the same size and same number of channels. Only then can we sum the inputs up.\n\nIn the actual implementation, we do not actually sum the input tensors but rather do something called Fast normalized fusion that has been described in section 3.3 of the paper. But, it is completely okay if for now we assume that combine the inputs by simply summing them up once they are of the same size.\n\nNext, we still need to perform the “fusion”. Simply combining the inputs up might not be enough and we still need to do some more computation on top to get a good representation or do the actual “fusion” of the Node outputs. This “fusion” operation is a Depthwise Separable Convolution followed by a BatchNorm and activation layer.\nThis has been mentioned in the paper at the end of section-3 as well. > Notably, to further improve the efficiency, we use depthwise separable convolution for feature fusion, and add batch normalization and activation after each convolution.\nGreat, now that we have a general understanding of the combine and after_combine operations, let’s implement them below.\n\n2.1.1 The “combine” method - FpnCombine\nNote: This is the most complicated part of the code. So, please, bear with me. Re-read this section multiple times if needed.:)\nLet’s understand the general idea of this class before looking at the code.\n\nAssuming that the combine operation is simply a sum operation for now. As can be seen from the figure above, Node-5 accepts the inputs from Node-3 and Node-4. Now these feature maps are of different sizes and have different number of channels so we simply can’t sum them up. The feature map size at Node-4 is [64, 4, 4] whereas at Node-3 is [64, 8, 8]. So to be able to combine at Node-5, we will convert both feature maps to be of the size [64, 8, 8] cause that’s what the size of feature map at Node-5 should be.\nThis class, merely does this operation. It will first resample both feature maps to be of the same size as the required [64, 8, 8] and then it will combine them together.\nSo, now that we have some idea of what we want to accomplish, let’s look at the code implementation.\nclass FpnCombine(nn.Module):\n    def __init__(self, feature_info, fpn_config, fpn_channels, inputs_offsets, target_reduction, pad_type='',\n                 downsample=None, upsample=None, norm_layer=nn.BatchNorm2d, apply_resample_bn=False,\n                 conv_after_downsample=False, redundant_bias=False, weight_method='attn'):\n        super(FpnCombine, self).__init__()\n        self.inputs_offsets = inputs_offsets\n        self.weight_method = weight_method\n\n        self.resample = nn.ModuleDict()\n        for idx, offset in enumerate(inputs_offsets):\n            in_channels = fpn_channels\n            if offset < len(feature_info):\n                in_channels = feature_info[offset]['num_chs']\n                input_reduction = feature_info[offset]['reduction']\n            else:\n                node_idx = offset - len(feature_info)\n                input_reduction = fpn_config.nodes[node_idx]['reduction']\n            reduction_ratio = target_reduction / input_reduction\n            self.resample[str(offset)] = ResampleFeatureMap(\n                in_channels, fpn_channels, reduction_ratio=reduction_ratio, pad_type=pad_type,\n                downsample=downsample, upsample=upsample, norm_layer=norm_layer, apply_bn=apply_resample_bn,\n                conv_after_downsample=conv_after_downsample, redundant_bias=redundant_bias)\n\n        if weight_method == 'attn' or weight_method == 'fastattn':\n            self.edge_weights = nn.Parameter(torch.ones(len(inputs_offsets)), requires_grad=True)  # WSM\n        else:\n            self.edge_weights = None\n\n    def forward(self, x: List[torch.Tensor]):\n        dtype = x[0].dtype\n        nodes = []\n        for offset, resample in zip(self.inputs_offsets, self.resample.values()):\n            input_node = x[offset]\n            input_node = resample(input_node)\n            nodes.append(input_node)\n\n        if self.weight_method == 'attn':\n            normalized_weights = torch.softmax(self.edge_weights.to(dtype=dtype), dim=0)\n            out = torch.stack(nodes, dim=-1) * normalized_weights\n        elif self.weight_method == 'fastattn':\n            edge_weights = nn.functional.relu(self.edge_weights.to(dtype=dtype))\n            weights_sum = torch.sum(edge_weights)\n            out = torch.stack(\n                [(nodes[i] * edge_weights[i]) / (weights_sum + 0.0001) for i in range(len(nodes))], dim=-1)\n        elif self.weight_method == 'sum':\n            out = torch.stack(nodes, dim=-1)\n        else:\n            raise ValueError('unknown weight_method {}'.format(self.weight_method))\n        out = torch.sum(out, dim=-1)\n        return out\nThere’s actually quite a bit happening in this layer but hey, don’t be worried. Take a deep breath and read on! This layer will make sense. :)\nSomething new that we have encountered in this class is feature_info. What is it? It’s something that comes from timm. Do you remember that we are using the EfficientNet backbone? This backbone has something called a feature_info which we can see below.\n\nLet’s not worry about how this get_feature_info function is actually implemented. But, let’s just assume there is this beatiful function that gives us the desired outputs.\n\nbackbone = timm.create_model(\n        config.backbone_name, features_only=True, out_indices=(2, 3, 4),\n        pretrained=True, **config.backbone_args)\nfeature_info = get_feature_info(backbone)\nprint(feature_info, '\\n')\n\n>> [{'num_chs': 40, 'reduction': 8}, {'num_chs': 112, 'reduction': 16}, {'num_chs': 320, 'reduction': 32}]\nSo the feature_info is something that tells us how many channels are there at each reduction level. For example, the number of channels at level P5 or reduction 32 is 320 as shown in the feature_info dictionary. Note that this matches the number of channels shown in fig-2. Note that this feature_info is actually missing levels P6 and P7 where the reduction is 64 and 128 respectively. Let’s again assume there is some part of code that updates this feature_info so it actually looks something like below for the first BiFpnLayer:\n>> [{'num_chs': 40, 'reduction': 8}, {'num_chs': 112, 'reduction': 16}, {'num_chs': 320, 'reduction': 32}, {'num_chs': 64, 'reduction': 64}, {'num_chs': 64, 'reduction': 128}]\nGreat, now let’s dissect the __init__ method of this FpnCombine class.\ndef __init__(self, feature_info, fpn_config, fpn_channels, inputs_offsets, target_reduction, pad_type='',\n                downsample=None, upsample=None, norm_layer=nn.BatchNorm2d, apply_resample_bn=False,\n                conv_after_downsample=False, redundant_bias=False, weight_method='attn'):\n    super(FpnCombine, self).__init__()\n    self.inputs_offsets = inputs_offsets\n    self.weight_method = weight_method\n\n    self.resample = nn.ModuleDict()\n    for idx, offset in enumerate(inputs_offsets):\n        in_channels = fpn_channels\n        if offset < len(feature_info):\n            in_channels = feature_info[offset]['num_chs']\n            input_reduction = feature_info[offset]['reduction']\n        else:\n            node_idx = offset - len(feature_info)\n            input_reduction = fpn_config.nodes[node_idx]['reduction']\n        reduction_ratio = target_reduction / input_reduction\n        self.resample[str(offset)] = ResampleFeatureMap(\n            in_channels, fpn_channels, reduction_ratio=reduction_ratio, pad_type=pad_type,\n            downsample=downsample, upsample=upsample, norm_layer=norm_layer, apply_bn=apply_resample_bn,\n            conv_after_downsample=conv_after_downsample, redundant_bias=redundant_bias)\n\n    if weight_method == 'attn' or weight_method == 'fastattn':\n        self.edge_weights = nn.Parameter(torch.ones(len(inputs_offsets)), requires_grad=True)  # WSM\n    else:\n        self.edge_weights = None\nAs a general idea: > This FpnCombine layer accepts a list of nodes as input nodes. Then it calculates some parameters which are then passed to ResampleFeatureMap to make sure that we resample/resize the feature maps from the input nodes such that we can combine them.\nThe class accepts feature_info, fpn_config, fpn_channels, inputs_offsets and target_reduction as required inputs. We will focus just on these. We already know the values of feature_info and fpn_config. Let me share them below once again for reference:\n>> feature_info =  [{'num_chs': 40, 'reduction': 8}, {'num_chs': 112, 'reduction': 16}, {'num_chs': 320, 'reduction': 32}, {'num_chs': 64, 'reduction': 64}, {'num_chs': 64, 'reduction': 128}]\n\n>> fpn_config = {'nodes': [{'reduction': 64, 'inputs_offsets': [3, 4], 'weight_method': 'fastattn'}, {'reduction': 32, 'inputs_offsets': [2, 5], 'weight_method': 'fastattn'}, {'reduction': 16, 'inputs_offsets': [1, 6], 'weight_method': 'fastattn'}, {'reduction': 8, 'inputs_offsets': [0, 7], 'weight_method': 'fastattn'}, {'reduction': 16, 'inputs_offsets': [1, 7, 8], 'weight_method': 'fastattn'}, {'reduction': 32, 'inputs_offsets': [2, 6, 9], 'weight_method': 'fastattn'}, {'reduction': 64, 'inputs_offsets': [3, 5, 10], 'weight_method': 'fastattn'}, {'reduction': 128, 'inputs_offsets': [4, 11], 'weight_method': 'fastattn'}]}\nLet’s just assume that we are currently creating an instance of this class for Node-5 as an example.\nThe inputs_offsets would be a list of Node id’s such as [3,4] for Node-5. This states that Node-5 accepts the outputs of Node-3 and Node-4 as inputs and has to combine them.\nThe only variables that we do not know the values of are fpn_channels and target_reduction. The fpn_channels has a value of 64.Great, what about target_reduction? target_reduction just refers to the reduction value of the current Node for which we are creating this FpnCombine class. So, from the fpn_config we can see the reduction for Node-5 is 64. Thus target_reduction=64.\n\nNote that the value of target_reduction for Node-6 be 32, for Node-7 it will be 16 and so on..\n\nI leave it to the reader to see how the self.resample inside the FpnCombine is a list of ResampleFeatureMap that looks something like below for Node-5:\n>> self.resample\n\nModuleDict(\n  (3): ResampleFeatureMap: (inp_ch:64, out_ch:64, reduction:1.0)\n  (4): ResampleFeatureMap: (inp_ch:64, out_ch:64, reduction:0.5)\n)\nFor Node-6, this looks something like:\n>> ModuleDict(\n  (2): ResampleFeatureMap: (inp_ch:320, out_ch:64, reduction:1.0)\n  (5): ResampleFeatureMap: (inp_ch:64, out_ch:64, reduction:0.5)\n)\nAnd finally for Node-7 as an example, this looks like:\nModuleDict(\n  (1): ResampleFeatureMap: (inp_ch:112, out_ch:64, reduction:1.0)\n  (6): ResampleFeatureMap: (inp_ch:64, out_ch:64, reduction:0.5)\n)\n\nIf you follow along the for loop inside the __init__ method, what I have stated above will become pretty clear to you. It might be a good idea right now to take out a pen and paper, and actually try to guess the values that get passed to ResampleFeatureMap for each input offset. If you don’t get it, feel free to reach out to me and I’ll share the solution. Contact details have been provided at the last of this blog post.\n\nI hope that you’ve been able to trace the values of self.resample for the various Nodes. Now that we have already looked at __init__, the forward method is pretty straightforward:\n    def forward(self, x: List[torch.Tensor]):\n        dtype = x[0].dtype\n        nodes = []\n        for offset, resample in zip(self.inputs_offsets, self.resample.values()):\n            input_node = x[offset]\n            input_node = resample(input_node)\n            nodes.append(input_node)\n\n        if self.weight_method == 'attn':\n            normalized_weights = torch.softmax(self.edge_weights.to(dtype=dtype), dim=0)\n            out = torch.stack(nodes, dim=-1) * normalized_weights\n        elif self.weight_method == 'fastattn':\n            edge_weights = nn.functional.relu(self.edge_weights.to(dtype=dtype))\n            weights_sum = torch.sum(edge_weights)\n            out = torch.stack(\n                [(nodes[i] * edge_weights[i]) / (weights_sum + 0.0001) for i in range(len(nodes))], dim=-1)\n        elif self.weight_method == 'sum':\n            out = torch.stack(nodes, dim=-1)\n        else:\n            raise ValueError('unknown weight_method {}'.format(self.weight_method))\n        out = torch.sum(out, dim=-1)\n        return out\nThere isn’t a lot happening. We just go over the inputs one by one, perform the required resample operation as in self.resample to make the input feature maps to be of the required size and finally do the “combination” inside the forward.\n\n\n2.1.2 The after_combine method\nThe after_combine method is nothing but a** Depthwise Separable Convolution** that we will look at as part of the BiFpnLayer implementation. But for completeness of FNode, I state it below too:\nconv_kwargs = dict(\n                in_channels=fpn_channels, out_channels=fpn_channels, kernel_size=3, padding=pad_type,\n                bias=False, norm_layer=norm_layer, act_layer=act_layer)\nafter_combine = nn.Sequential()\nafter_combine.add_module(\n                'conv', SeparableConv2d(**conv_kwargs))"
  },
  {
    "objectID": "posts/2021-01-13-efficientdet-pytorch.html#bifpn-layer-implementation",
    "href": "posts/2021-01-13-efficientdet-pytorch.html#bifpn-layer-implementation",
    "title": "The EfficientDet Architecture in PyTorch",
    "section": "3 BiFPN Layer Implementation",
    "text": "3 BiFPN Layer Implementation\nFinally, we are ready to look at the implementation of the BiFPN Layer.\n\nIf you have understood the implementation of FpnCombine, and also the general idea so far, then you will find the implementation of BiFPN Layer as something that brings all the pieces together. It should be intuitive rather than complex.\n\nSo, let’s have a look at it.\nclass BiFpnLayer(nn.Module):\n    def __init__(self, feature_info, fpn_config, fpn_channels, num_levels=5, pad_type='',\n                 downsample=None, upsample=None, norm_layer=nn.BatchNorm2d, act_layer=_ACT_LAYER,\n                 apply_resample_bn=False, conv_after_downsample=True, conv_bn_relu_pattern=False,\n                 separable_conv=True, redundant_bias=False):\n        super(BiFpnLayer, self).__init__()\n        self.num_levels = num_levels\n        self.conv_bn_relu_pattern = False\n\n        self.feature_info = []\n        self.fnode = nn.ModuleList()\n        for i, fnode_cfg in enumerate(fpn_config.nodes):\n            logging.debug('fnode {} : {}'.format(i, fnode_cfg))\n            reduction = fnode_cfg['reduction']\n            combine = FpnCombine(\n                feature_info, fpn_config, fpn_channels, tuple(fnode_cfg['inputs_offsets']),\n                target_reduction=reduction, pad_type=pad_type, downsample=downsample, upsample=upsample,\n                norm_layer=norm_layer, apply_resample_bn=apply_resample_bn, conv_after_downsample=conv_after_downsample,\n                redundant_bias=redundant_bias, weight_method=fnode_cfg['weight_method'])\n\n            after_combine = nn.Sequential()\n            conv_kwargs = dict(\n                in_channels=fpn_channels, out_channels=fpn_channels, kernel_size=3, padding=pad_type,\n                bias=False, norm_layer=norm_layer, act_layer=act_layer)\n            if not conv_bn_relu_pattern:\n                conv_kwargs['bias'] = redundant_bias\n                conv_kwargs['act_layer'] = None\n                after_combine.add_module('act', act_layer(inplace=True))\n            after_combine.add_module(\n                'conv', SeparableConv2d(**conv_kwargs) if separable_conv else ConvBnAct2d(**conv_kwargs))\n\n            self.fnode.append(Fnode(combine=combine, after_combine=after_combine))\n            self.feature_info.append(dict(num_chs=fpn_channels, reduction=reduction))\n\n        self.feature_info = self.feature_info[-num_levels::]\n\n    def forward(self, x: List[torch.Tensor]):\n        for fn in self.fnode:\n            x.append(fn(x))\n        return x[-self.num_levels::]\nWe already know what the values of the required parameters are feature_info, fpn_config and fpn_channels. I share them for reference below once again:\nfeature_info = [{'num_chs': 40, 'reduction': 8}, {'num_chs': 112, 'reduction': 16}, {'num_chs': 320, 'reduction': 32}, {'num_chs': 64, 'reduction': 64}, {'num_chs': 64, 'reduction': 128}]\n\nfpn_config = {'nodes': [{'reduction': 64, 'inputs_offsets': [3, 4], 'weight_method': 'fastattn'}, {'reduction': 32, 'inputs_offsets': [2, 5], 'weight_method': 'fastattn'}, {'reduction': 16, 'inputs_offsets': [1, 6], 'weight_method': 'fastattn'}, {'reduction': 8, 'inputs_offsets': [0, 7], 'weight_method': 'fastattn'}, {'reduction': 16, 'inputs_offsets': [1, 7, 8], 'weight_method': 'fastattn'}, {'reduction': 32, 'inputs_offsets': [2, 6, 9], 'weight_method': 'fastattn'}, {'reduction': 64, 'inputs_offsets': [3, 5, 10], 'weight_method': 'fastattn'}, {'reduction': 128, 'inputs_offsets': [4, 11], 'weight_method': 'fastattn'}]}\n\nfpn_channels = 64\nThe BiFPN Layer implementation is the glue that binds the FNode, FpnCombine, ResampleFeatureMap classes together. Let’s see how?\nInside the __init__ method, we iterate over the Nodes. For each node, combine is an instance of FpnCombine and after_combine is a SeparableConv2d nn Module. Next, we create a FNode for each of the Nodes inside the fpn_config with each FNode having it’s own combine and after_combine values.\nFinally, in the forward method, where the input x is list of feature maps from levels P3-P7, is passed through to each node and we append the outputs. Finally, we return the last 5 outputs (feature maps), that are then passed on to another BiFpnLayer which does the same thing again.\n\nIf this sounds confusing, then please feel free to reach out to me. I will share the solution. But I really want you to think about what I have stated above. It’s better if you spend some time trying to think about it than just asking me for the solution that explains this."
  },
  {
    "objectID": "posts/2021-01-13-efficientdet-pytorch.html#bifpn",
    "href": "posts/2021-01-13-efficientdet-pytorch.html#bifpn",
    "title": "The EfficientDet Architecture in PyTorch",
    "section": "4 BiFPN",
    "text": "4 BiFPN\nHaving already understood how to implement the BiFPN Layer, let’s now look at how to implement the BiFPN network. It’s really a simple case of having multiple BiFPN layers inside a single BiFPN network.\nclass BiFpn(nn.Module):\n\n    def __init__(self, config, feature_info):\n        super(BiFpn, self).__init__()\n        self.num_levels = config.num_levels\n        norm_layer = config.norm_layer or nn.BatchNorm2d\n        if config.norm_kwargs:\n            norm_layer = partial(norm_layer, **config.norm_kwargs)\n        act_layer = get_act_layer(config.act_type) or _ACT_LAYER\n        fpn_config = config.fpn_config or get_fpn_config(\n            config.fpn_name, min_level=config.min_level, max_level=config.max_level)\n\n        self.resample = nn.ModuleDict()\n        for level in range(config.num_levels):\n            if level < len(feature_info):\n                in_chs = feature_info[level]['num_chs']\n                reduction = feature_info[level]['reduction']\n            else:\n                # Adds a coarser level by downsampling the last feature map\n                reduction_ratio = 2\n                self.resample[str(level)] = ResampleFeatureMap(\n                    in_channels=in_chs,\n                    out_channels=config.fpn_channels,\n                    pad_type=config.pad_type,\n                    downsample=config.downsample_type,\n                    upsample=config.upsample_type,\n                    norm_layer=norm_layer,\n                    reduction_ratio=reduction_ratio,\n                    apply_bn=config.apply_resample_bn,\n                    conv_after_downsample=config.conv_after_downsample,\n                    redundant_bias=config.redundant_bias,\n                )\n                in_chs = config.fpn_channels\n                reduction = int(reduction * reduction_ratio)\n                feature_info.append(dict(num_chs=in_chs, reduction=reduction))\n\n        self.cell = SequentialList()\n        for rep in range(config.fpn_cell_repeats):\n            logging.debug('building cell {}'.format(rep))\n            fpn_layer = BiFpnLayer(\n                feature_info=feature_info,\n                fpn_config=fpn_config,\n                fpn_channels=config.fpn_channels,\n                num_levels=config.num_levels,\n                pad_type=config.pad_type,\n                downsample=config.downsample_type,\n                upsample=config.upsample_type,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                separable_conv=config.separable_conv,\n                apply_resample_bn=config.apply_resample_bn,\n                conv_after_downsample=config.conv_after_downsample,\n                conv_bn_relu_pattern=config.conv_bn_relu_pattern,\n                redundant_bias=config.redundant_bias,\n            )\n            self.cell.add_module(str(rep), fpn_layer)\n            feature_info = fpn_layer.feature_info\n\n    def forward(self, x: List[torch.Tensor]):\n        for resample in self.resample.values():\n            x.append(resample(x[-1]))\n        x = self.cell(x)\n        return x\nSo let’s have a look at what is self.resample inside here? As you will see in the implementation of the EfficientDet Architecture in the next section, the backbone EfficientNet only returns a total of 3 feature maps for levels P3-P5. We still need to calculate feature maps for levels P6 & P7. This is what the self.resample layer does here.\nAs you can see in the forward method:\nfor resample in self.resample.values():\n            x.append(resample(x[-1]))\nAs I mentioned, x has total of 3 feature maps reprenting levels P3-P5. We call the resample method and append 2 more feature maps to x to make the total length 5 representing the feature maps for levels P3-P7.\nFinally, what is self.cell? It’s simply a repetition of BiFPN Layers to represent the BiFPN Network. Since the self.cell is a SequentialList, each BiFPNLayer is called one by one.:)"
  },
  {
    "objectID": "posts/2021-01-13-efficientdet-pytorch.html#efficientdet-architecture",
    "href": "posts/2021-01-13-efficientdet-pytorch.html#efficientdet-architecture",
    "title": "The EfficientDet Architecture in PyTorch",
    "section": "5 EfficientDet Architecture",
    "text": "5 EfficientDet Architecture\nThis class is the main one - or is it? Considering we have all the pieces already, this class merely puts them all together.\nclass EfficientDet(nn.Module):\n\n    def __init__(self, config, pretrained_backbone=True, alternate_init=False):\n        super(EfficientDet, self).__init__()\n        self.config = config\n        set_config_readonly(self.config)\n        self.backbone = create_model(\n            config.backbone_name, features_only=True, out_indices=(2, 3, 4),\n            pretrained=pretrained_backbone, **config.backbone_args)\n        feature_info = get_feature_info(self.backbone)\n        self.fpn = BiFpn(self.config, feature_info)\n        self.class_net = HeadNet(self.config, num_outputs=self.config.num_classes)\n        self.box_net = HeadNet(self.config, num_outputs=4)\n\n        for n, m in self.named_modules():\n            if 'backbone' not in n:\n                if alternate_init:\n                    _init_weight_alt(m, n)\n                else:\n                    _init_weight(m, n)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.fpn(x)\n        x_class = self.class_net(x)\n        x_box = self.box_net(x)\n        return x_class, x_box\nThe reader should by now be able to read this code and understand what’s going on. We have already done the bulk of the work together here. Though there are some notable things that I would like to mention: 1. self.backbone returns a total of 3 feature maps because we passed the out_indices=(2, 3, 4). As to how? This comes from timm. 2. I have not explained what a HeadNet is. It is basically a custom head that takes the final outputs of the BiFPN network and either returns a class or bounding box coordinates. The self.class_net and self.box_net together represent the Box Prediction Net as in fig-1."
  },
  {
    "objectID": "posts/2021-01-13-efficientdet-pytorch.html#conclusion",
    "href": "posts/2021-01-13-efficientdet-pytorch.html#conclusion",
    "title": "The EfficientDet Architecture in PyTorch",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nI truly believe that it wasn’t easy to understand how to implement EfficientDets in Tensorflow or PyTorch. The official implementation exists here and the PyTorch version is here.\nThanks to Ross Wightman for his wonderful work in providing us with a PyTorch implementation of the EfficientDet network. It really makes things super easy for the native PyTorch users.\nSomething I have realised during this exercise, is that, when we are doing something like:\nfrom effdet import get_efficientdet_config, EfficientDet\nconfig = get_efficientdet_config('efficientdet_d0')\nefficientdet = EfficientDet(config)\noutput = efficientdet(inputs)\nWhen we are merely using the library, it is hard to appreciate the hard work that the author has put into making things so simple for us. In doing this exercise of going through the source code and to try and explain it to everyone, I have noticed the minor details and have tremendous appreciation for Ross’s hard work. So, thanks so much Ross! And of course to the authors of the EfficientDet for open sourcing the implementation in Tensorflow.\nI hope that in today’s blog post I have been able to explain how to implement EfficientDets in Code and take away all the confusion and doubt in case you had any.\nAs always, constructive feedback is always welcome at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-09-13-unet.html",
    "href": "posts/2020-09-13-unet.html",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "",
    "text": "Today’s blog post is going to be short and sweet. Today, we will be looking at how to implement the U-Net architecture in PyTorch in 60 lines of code.\nThis blog is not an introduction to Image Segmentation or theoretical explanation of the U-Net architecture, for that, I would like to refer the reader to this wonderful article by Harshall Lamba. Rather, this blog post is a step-by-step explaination of how to implement U-Net from scratch in PyTorch.\nIn this blogpost - first, we will understand the U-Net architecture - specifically, the input and output shapes of each block. We look at the U-Net Architecture with a factory production line analogy to keep things simple and easy to digest. Next, we will translate our understanding of U-Net architecture to concise PyTorch code.\nI also share a working notebook to train this implementation of U-Net of SIIM ACR Pneumothorax Kaggle competition here. Should be as simple as to switch and model in to the implemention mentioned in this blog post.\nSo, let’s get started."
  },
  {
    "objectID": "posts/2020-09-13-unet.html#understanding-input-and-output-shapes-in-u-net",
    "href": "posts/2020-09-13-unet.html#understanding-input-and-output-shapes-in-u-net",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "2 Understanding Input and Output shapes in U-Net",
    "text": "2 Understanding Input and Output shapes in U-Net\n\nAs can be seen from fig-1, the architecture is “U-shaped”, hence the name “U-Net”. The complete architecture consists of two parts - the Encoder and the Decoder. The part on the left of fig-1 (yellow highlight in fig-2) is the Encoder whereas the part on the right is the Decoder (orange highlight in fig-2).\nFrom the paper: > The network architecture is illustrated in Fig-1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network.\nThe Encoder is like any standard CNN - such as ResNet, that extracts a meaningful feature map from an input image. As is standard practice for a CNN, the Encoder, doubles the number of channels at every step and halves the spatial dimension.\nNext, the Decoder actually upsamples the feature maps, where at every step, it doubles the spatial dimension and halves the number of channels (opposite to that of what an Encoder does)."
  },
  {
    "objectID": "posts/2020-09-13-unet.html#the-factory-production-line-analogy",
    "href": "posts/2020-09-13-unet.html#the-factory-production-line-analogy",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "3 The Factory Production Line Analogy",
    "text": "3 The Factory Production Line Analogy\nLet’s now look at the U-Net with a Factory Production Line analogy as in fig-2.\n\nWe can think of this whole architecture as a factory line where the Black dots represents assembly stations and the path itself is a conveyor belt where different actions take place to the Image on the conveyor belt depending on whether the conveyor belt is Yellow or Orange.\nIf it’s Yellow, we downsample the Image, using Max Pooling 2x2 operation that reduces both the Height and Width of the Image by half. If the conveyor belt is Orange, we apply an upsampling operation ConvTranspose2d which doubles the height and width of the image and reduces the number of channels by half. Therefore, the Orange conveyor built performs an operation that is opposite to that performed by Yellow belt.\nAlso, it is key to note that in the Decoder side, at every assembly station (Black dot), the outputs from the Encoder assembly stations are also concatenated to the inputs.\nLet’s start to now turn this simple understanding to PyTorch Code."
  },
  {
    "objectID": "posts/2020-09-13-unet.html#the-black-dots-block",
    "href": "posts/2020-09-13-unet.html#the-black-dots-block",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "4 The Black Dots / Block",
    "text": "4 The Black Dots / Block\nAll assembly stations - the black dots in fig-2 have two Conv2D operations with ReLU activation between them. The Convolution operations have kernel size of 3, and no padding. Therefore, the output feature map doesn’t have the same Height and Width as the input feature map.\nFrom the paper: > The contractive path consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.\nLet’s write Block in code.\nclass Block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n        self.relu  = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n    \n    def forward(self, x):\n        return self.relu(self.conv2(self.relu(self.conv1(x))))\nThat’s simple - two convolution operations, one that doubles the number of channels from in_ch to out_ch and another that goes from out_ch to out_ch. Both are 2D convolutions with kernel size 3 and no padding as mentioned in the paper followed by ReLU activation.\nLet’s make sure this works.\nenc_block = Block(1, 64)\nx         = torch.randn(1, 1, 572, 572)\nenc_block(x).shape\n\n>> torch.Size([1, 64, 568, 568])\nSo this is looking good, the output size matches that in fig-1 top-left. Given an input image with shape 1x572x572 the output is of shape 64x568x568."
  },
  {
    "objectID": "posts/2020-09-13-unet.html#the-encoder",
    "href": "posts/2020-09-13-unet.html#the-encoder",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "5 The Encoder",
    "text": "5 The Encoder\nNow that we have implemented the Blocks or the black dots in fig-2, we are ready to implement the Encoder. The Encoder is the contractive path of the U-Net Architecture.\nSo far we have implemented the convolution operations but not the downsampling part. As mentioned in the paper: > Each block is followed by a 2x2 max pooling operation with stride 2 for downsampling.\nSo that’s all we need to do, we need add MaxPooling operation (Yellow conveyor belt in fig-2) that is performed between two Block operations.\nclass Encoder(nn.Module):\n    def __init__(self, chs=(3,64,128,256,512,1024)):\n        super().__init__()\n        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n        self.pool       = nn.MaxPool2d(2)\n    \n    def forward(self, x):\n        ftrs = []\n        for block in self.enc_blocks:\n            x = block(x)\n            ftrs.append(x)\n            x = self.pool(x)\n        return ftrs\nThat’s all we have done. On the Encoder side, the encoder block or self.enc_blocks is a list of Block operations. Next, we perform the MaxPooling operation to the outputs of every block. Since, we also need to store the outputs of the block, we store them in a list called ftrs and return this list.\nLet’s make sure this implementation works.\nencoder = Encoder()\n# input image\nx    = torch.randn(1, 3, 572, 572)\nftrs = encoder(x)\nfor ftr in ftrs: print(ftr.shape)\n\n>> \ntorch.Size([1, 64, 568, 568])\ntorch.Size([1, 128, 280, 280])\ntorch.Size([1, 256, 136, 136])\ntorch.Size([1, 512, 64, 64])\ntorch.Size([1, 1024, 28, 28])\nThe output shapes exactly match the shapes mentioned in fig-1 - so far, so good. Having implemented the Encoder, we are now ready to move on the Decoder."
  },
  {
    "objectID": "posts/2020-09-13-unet.html#the-decoder",
    "href": "posts/2020-09-13-unet.html#the-decoder",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "6 The Decoder",
    "text": "6 The Decoder\nThe Decoder, is the expansive path of the U-Net Architecture.\nFrom the paper: > Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution.\nNote that we have already implemented the part where two 3x3 convolutions occur followed by ReLU activation in Block. All we need to do to implement the Decoder is to add the “up-convolution” (the Orange highlight in fig-2) and the feature concatenation with correspondingly cropped feature map from the contracting path (the gray arrows in fig-2).\nNote that in PyTorch, the ConvTranspose2d operation performs the “up-convolution”. It accepts parameters like in_channels, out_channels, kernel_size and stride amongst others.\nSince the in_channels and out_channels values are different in the Decoder depending on where this operation is performed, in the implementation, the “up-convolution” operations are also stored as a list. Stride and kernel size are always 2 as mentioned in the paper.\nNow, all we need is to perform feature concatenation. Let’s look at the implementation of the Decoder to understand how all this works more clearly -\nclass Decoder(nn.Module):\n    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n        super().__init__()\n        self.chs         = chs\n        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n        \n    def forward(self, x, encoder_features):\n        for i in range(len(self.chs)-1):\n            x        = self.upconvs[i](x)\n            enc_ftrs = self.crop(encoder_features[i], x)\n            x        = torch.cat([x, enc_ftrs], dim=1)\n            x        = self.dec_blocks[i](x)\n        return x\n    \n    def crop(self, enc_ftrs, x):\n        _, _, H, W = x.shape\n        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n        return enc_ftrs\nSo the self.dec_blocks is a list of Decoder Blocks that perform the two conv + ReLU operation as mentioned in the paper. The self.upconvs is a list of ConvTranspose2d operations that perform the “up-convolution” operations. And finally, in the forward function, the decoder accepts the encoder_features which were output by the Encoder to perform the concatenation operation before passing the result to Block.\nThat’s really all there is inside the Decoder of a U-Net. Let’s make sure this implementation works:\ndecoder = Decoder()\nx = torch.randn(1, 1024, 28, 28)\ndecoder(x, ftrs[::-1][1:]).shape\n\n>> (torch.Size([1, 64, 388, 388])\nAnd there it is, the final feature map is of size 64x388x388 which matches that of fig-1. We have just successfully implemented both the Encoder and the Decoder so far.\nYou might ask why do we do ftrs[::-1][1:]?\nDo you remember the shapes of the outputs of the Encoder? They were:\ntorch.Size([1, 64, 568, 568]) #0\ntorch.Size([1, 128, 280, 280]) #1\ntorch.Size([1, 256, 136, 136]) #2\ntorch.Size([1, 512, 64, 64]) #3\ntorch.Size([1, 1024, 28, 28]) #4\nNow, from fig-1, we can see that the feature map with shape torch.Size([1, 1024, 28, 28]) is never really concatenated but only a “up-convolution” operation is performed on it. Also, the 1st Decoder block in fig-1 accepts the inputs from the 3rd position Encoder block. Similarly, the 2nd Decoder block accepts the inputs from the 2nd position Encoder block and so on. Therefore, the encoder_features are reversed before passing them to the Decoder and since the feature map with shape torch.Size([1, 1024, 28, 28]) is not concatenated to the Decoder blocks, it is not passed.\nHence, the input to the decoder is ftrs[::-1][1:]."
  },
  {
    "objectID": "posts/2020-09-13-unet.html#u-net",
    "href": "posts/2020-09-13-unet.html#u-net",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "7 U-Net",
    "text": "7 U-Net\nGreat, we have so far implemented both the Encoder and the Decoder of U-Net architecture. Let’s put it all together to complete our implementation of U-Net.\nclass UNet(nn.Module):\n    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n        super().__init__()\n        self.encoder     = Encoder(enc_chs)\n        self.decoder     = Decoder(dec_chs)\n        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n        self.retain_dim  = retain_dim\n\n    def forward(self, x):\n        enc_ftrs = self.encoder(x)\n        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n        out      = self.head(out)\n        if self.retain_dim:\n            out = F.interpolate(out, out_sz)\n        return out\nLet’s make sure this implementation works:\nunet = UNet()\nx    = torch.randn(1, 3, 572, 572)\nunet(x).shape\n\n>> torch.Size([4, 1, 388, 388])\nThe output shape matches that of fig-1.\nAs mentioned before, since the convolution operations are 3x3 without padding, the output feature map size is not the same as the input feature map size. Also, as shown in fig-1 the final output is of shape 1x388x388 while the input Image had dimensions 572x572. This can create problems when calculating BCELoss in PyTorch as it expects the input and output feature maps to have the same shape.\nTherefore, if we want to retain_dim, I have added F.interpolate operation to the U-Net to make the output size same as the input Image size.\nGreat, that was all! We have just successfully implemented the U-Net architecture in PyTorch. Everything put together, this looks something like:\nclass Block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n        self.relu  = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n    \n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\n\n\nclass Encoder(nn.Module):\n    def __init__(self, chs=(3,64,128,256,512,1024)):\n        super().__init__()\n        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n        self.pool       = nn.MaxPool2d(2)\n    \n    def forward(self, x):\n        ftrs = []\n        for block in self.enc_blocks:\n            x = block(x)\n            ftrs.append(x)\n            x = self.pool(x)\n        return ftrs\n\n\nclass Decoder(nn.Module):\n    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n        super().__init__()\n        self.chs         = chs\n        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n        \n    def forward(self, x, encoder_features):\n        for i in range(len(self.chs)-1):\n            x        = self.upconvs[i](x)\n            enc_ftrs = self.crop(encoder_features[i], x)\n            x        = torch.cat([x, enc_ftrs], dim=1)\n            x        = self.dec_blocks[i](x)\n        return x\n    \n    def crop(self, enc_ftrs, x):\n        _, _, H, W = x.shape\n        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n        return enc_ftrs\n\n\nclass UNet(nn.Module):\n    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n        super().__init__()\n        self.encoder     = Encoder(enc_chs)\n        self.decoder     = Decoder(dec_chs)\n        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n        self.retain_dim  = retain_dim\n\n    def forward(self, x):\n        enc_ftrs = self.encoder(x)\n        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n        out      = self.head(out)\n        if self.retain_dim:\n            out = F.interpolate(out, out_sz)\n        return out"
  },
  {
    "objectID": "posts/2020-09-13-unet.html#conclusion",
    "href": "posts/2020-09-13-unet.html#conclusion",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nI hope that today, I was able to provide a concise and easy to digest implementation of the U-Net architecture with proper explanations of every line of code.\nFor a complete working notebook to train this implementation, refer here.\nAs usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html",
    "href": "posts/2020-07-18-label-smoothing.html",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "",
    "text": "In this blogpost, together, we: - Read and understand about Label Smoothing from Rethinking the Inception Architecture for Computer Vision research paper - Look at why we need Label Smoothing? - Re-implement Label Smoothing in Microsoft Excel step by step - Compare the results from our MS Excel implementation with Fastai/PyTorch versions of Label Smoothing\nWhy are we using Microsoft Excel?\nIt’s a valid question you might ask and I wasn’t a big fan of MS Excel either until I saw this video by Jeremy Howard about Cross Entropy Loss. In the video Jeremy explains Cross Entropy Loss using Microsoft Excel. It clicked and I understood it very well even with the fancy math in the cross entropy loss formula.\n\nAnd that is my hope here too! In this blogpost I hope that together we can see past the math and get the intuition for Label Smoothing and then later be able to implement it in a language/framework of our choice.\nSo, let’s get started!"
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#why-do-we-need-label-smoothing",
    "href": "posts/2020-07-18-label-smoothing.html#why-do-we-need-label-smoothing",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "2 Why do we need Label Smoothing?",
    "text": "2 Why do we need Label Smoothing?\nLet’s consider we are faced with a multi-class image classification problem. Someone presents to us five images with labels -\n\n\n\nImage Name\nLabel\n\n\n\n\nimg-1.jpg\nDog\n\n\nimg-2.jpg\nCat\n\n\nimg-3.jpg\nHorse\n\n\nimg-4.jpg\nBear\n\n\nimg-5.jpg\nKangaroo\n\n\n\nAs humans, we will quickly be able to assign labels to the image just by looking at them, for example we know that img-1.jpg is that of a dog, img-2.jpg is a cat and so on.\nLet’s one-hot encode the labels, so our labels get updated to:\n\n\n\nImage Name\nis_dog\nis_cat\nis_horse\nis_bear\nis_kroo\n\n\n\n\nimg-1.jpg\n1\n0\n0\n0\n0\n\n\nimg-2.jpg\n0\n1\n0\n0\n0\n\n\nimg-3.jpg\n0\n0\n1\n0\n0\n\n\nimg-4.jpg\n0\n0\n0\n1\n0\n\n\nimg-5.jpg\n0\n0\n0\n0\n1\n\n\n\nLet’s imagine that we used the above set of 5 images and the labels and trained a deep learning model which in it’s early stages learns to predict a set of logits for each class like so:\n\n\n\n\n\n\n\n\n\n\n\nImage Name\nis_dog\nis_cat\nis_horse\nis_bear\nis_kroo\n\n\n\n\nimg-1.jpg\n4.7\n-2.5\n0.6\n1.2\n0.4\n\n\nimg-2.jpg\n-1.2\n2.4\n2.6\n-0.6\n2.34\n\n\nimg-3.jpg\n-2.4\n1.2\n1.1\n0.8\n1.2\n\n\nimg-4.jpg\n1.2\n0.2\n0.8\n1.9\n-0.6\n\n\nimg-5.jpg\n-0.9\n-0.1\n-0.2\n-0.5\n1.6\n\n\n\nThis is pretty standard - right? This is what we do when we’re training an image classifier anyway. We pass a list of images and labels, make the model predict something, then calculate the cross-entropy loss and backpropogate to update the model’s parameters. And we keep doing this until the model learns to assign the correct labels to the corresponding images. So what’s the problem?\nHere’s the important part:\nFor the cross-Entropy loss to really be at a minimum, each logit corresponding to the correct class needs to be significantly higher than the rest. That is, for example for row-1, img-1.jpg the logit of 4.7 corresponding to is_dog needs to be significantly higher than the rest. This is also the case for all the other rows.\nA mathematical proof is presented here by Lei Mao where he explains why minimizing cross entropy loss is equivalent to do maximum likelihood estimation.\nThis case where, in order to minimise the cross-entropy loss, the logits corresponding to the true label need to be significantly higher than the rest can actually cause two problems.\nFrom the paper, > This, however, can cause two problems. First, it may result in over-fitting: if the model learns to assign full probability to the ground- truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient ∂ℓ/∂z,k , reduces the ability of the model to adapt.\nIn other words, our model could become overconfident of it’s predictions because to really minimise the loss, our model needs to be very sure of everything that it predicts. This is bad because it is then harder for the model to generalise and easier for it to overfit to the training data. We want the model to generalize and be able to look at other dogs, cats.. images that weren’t part of the training set and still be able to predict them well."
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#what-is-label-smoothing",
    "href": "posts/2020-07-18-label-smoothing.html#what-is-label-smoothing",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "3 What is Label Smoothing?",
    "text": "3 What is Label Smoothing?\nLabel Smoothing was first introduced in Rethinking the Inception Architecture for Computer Vision.\nFrom Section-7 - Model Regularization via Label Smoothing in the paper, > We propose a mechanism for encouraging the model to be less confident. While this may not be desired if the goal is to maximize the log-likelihood of training labels, it does regularize the model and makes it more adaptable. The method is very simple. Consider a distribution over labels u(k), independent of the training example x, and a smoothing parameter Є. For a training example with ground-truth label y, we replace the label distribution q(k/x) = δ(k,y) with\n\n\nwhich is a mixture of the original ground-truth distribution q(k|x) and the fixed distribution u(k), with weights 1 − Є. and Є, respectively. In our experiments, we used the uniform distribution u(k) = 1/K, so that\n\n\nIn other words, instead of using the hard labels or the one-hot encoded variables where the true label is 1, let’s replace them with (1-Є) * 1 where Є refers to the smoothing parameter. Once that’s done, we add some uniform noise 1/K to the labels where K: total number of labels.\nSo the updated distribution for the our examples with label smoothing factor Є = 0.1 becomes:\n\n\n\n\n\n\n\n\n\n\n\nImage Name\nis_dog\nis_cat\nis_horse\nis_bear\nis_kroo\n\n\n\n\nimg-1.jpg\n0.92\n0.02\n0.02\n0.02\n0.02\n\n\nimg-2.jpg\n0.02\n0.92\n0.02\n0.02\n0.02\n\n\nimg-3.jpg\n0.02\n0.02\n0.92\n0.02\n0.02\n\n\nimg-4.jpg\n0.02\n0.02\n0.02\n0.92\n0.02\n\n\nimg-5.jpg\n0.02\n0.02\n0.02\n0.02\n0.92\n\n\n\nWe get the updated distribution above because 1-Є = 0.9. So as a first step, we replace all the true labels with 0.9 instead of 1. Next, we add a uniform noise 1/K = 0.02 because in our case K equals 5. Finally we get the above update distribution with uniform noise.\nThe authors refer to the above change as label-smoothing regularization or LSR. And then we calculate the cross-entropy loss with the updated distribution LSR above.\nNow we train the model with the updated LSR instead and therefore, cross-entropy loss get’s updated to:\n\nBasically, the new loss H(q′, p) equals 1-Є times the old loss H(q, p) + Є times the cross entropy loss of the noisy labels H(u, p). This is key in understanding Label Smoothing - it is essentially the cross entropy loss with the noisy labels.\nLet’s now cut the math and implement this in Microsoft Excel step by step."
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#label-smoothing-in-microsoft-excel",
    "href": "posts/2020-07-18-label-smoothing.html#label-smoothing-in-microsoft-excel",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "4 Label Smoothing in Microsoft Excel",
    "text": "4 Label Smoothing in Microsoft Excel\nIn this section we implement label smoothing in Microsoft Excel. We know that cross-entropy loss equals:\n\nGreat, and from section-2, we also know that Label Smoothing loss is actually the cross entropy loss with the noisy labels.\nLet’s consider we have five images again, but this time of only cats and dogs.\n\nAt the moment, the labels are one-hot encoded. Let’s consider we are using a smoothing factor Є of 0.1. In this case, the updated labels become:\n\nWe get fig-2 by implementing eq-2 on fig-1. So, now we have our LSR labels. Next step is to simply calculate the cross-entropy loss. We will use the fastai implementation of cross-entropy loss in excel, and use it on our LSR labels to calculate the Label Smoothing Cross Entropy Loss.\nLet’s consider that our model learns to predict the following logits for each class like so:\n\nAlso, to calculate the cross-entropy loss, we first need to convert the logits to probabilities. The logits are the outputs from the last linear layer of our deep learning model. To convert them to probabilities, we generally have a softmax layer in the end. Jeremy explains how to implement Cross-Entropy loss in Microsoft Excel here including Softmax implementation.\nThis is the where you PAUSE, look at the video and understand how Jeremy implements Softmax and Cross-Entropy loss in Microsoft Excel. If you already know how, great, let’s move on.\nWe repeat the same process of applying Softmax operation to the logits to then get our probabilities like so:\n\nWhat we have essentially done, is that we take the exponential of the logits, to get exp (cat) and exp (dog) from logit (cat) and logit (dog). Next, we take get the sum (exp) by adding exp (cat) and exp (dog) along the rows. Finally, we get prob (cat) by dividing exp (cat) with sum (exp) and we get prob (dog) by sum (exp). This is how we implement Softmax operation in Microsoft Excel.\nSo, now that we have successfully converted logits to Probabilities for each image. The next step is simply to calculate the Cross-Entropy loss which from eq-4, is ∑q(x)log(p(x)) where p(x) refers to the predicted probability and q(x) refers to the ground truth label. In our case q(x) are the noisy labels, so, we get the LabelSmoothingCrossEntropy loss like so:\n\nBelieve it or not, we have just successfully implemented Label Smoothing Cross Entropy loss in Microsoft Excel."
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#fastaipytorch-implementation-of-label-smoothing-cross-entropy-loss",
    "href": "posts/2020-07-18-label-smoothing.html#fastaipytorch-implementation-of-label-smoothing-cross-entropy-loss",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "5 Fastai/PyTorch Implementation of Label Smoothing Cross Entropy loss",
    "text": "5 Fastai/PyTorch Implementation of Label Smoothing Cross Entropy loss\nThe Label Smoothing Cross Entropy loss has been implemented in the wonderful fastai library like so:\n# Helper functions from fastai\ndef reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n# Implementation from fastai https://github.com/fastai/fastai2/blob/master/fastai2/layers.py#L338\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, ε:float=0.1, reduction='mean'):\n        super().__init__()\n        self.ε,self.reduction = ε,reduction\n    \n    def forward(self, output, target):\n        # number of classes\n        c = output.size()[-1]\n        log_preds = F.log_softmax(output, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        # (1-ε)* H(q,p) + ε*H(u,p)\n        return (1-self.ε)*nll + self.ε*(loss/c) \nIn PyTorch, nn.CrossEntropyLoss() is the same as F.nll_loss(F.log_softmax(...)). Therefore, in the implementation above, nll equates to H(q,p) from eq-3. And then, the loss/c equates to H(u,p) from eq-3 as well where, c equals total number of classes.\nFor reference again, we know that eq-3 was:\n\nSo, the above implementation can directly be compared to eq-3 and the Label Smoothing Cross Entropy loss then becomes (1-self.ε)*nll + self.ε*(loss/c)."
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#comparing-microsoft-excel-results-with-pytorch",
    "href": "posts/2020-07-18-label-smoothing.html#comparing-microsoft-excel-results-with-pytorch",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "6 Comparing Microsoft Excel results with PyTorch",
    "text": "6 Comparing Microsoft Excel results with PyTorch\nGreat, now that we know how to implement Label Smoothing Cross Entropy loss in both Microsoft Excel and PyTorch, let’s compare the results. We take the same example as fig-3, and assume that our model in PyTorch predicts the same logits.\n# X: model logits or outputs, y: true labels\nX = torch.tensor([\n    [4.2, -2.4], \n    [1.6, -0.6], \n    [3.6, 1.2], \n    [-0.5, 0.5], \n    [-0.25, 1.7]\n])\ny = torch.tensor([0,1,1,0,0])\n\nprint(X, '\\n\\n', y)\n\n>> #out\ntensor([[ 4.2000, -2.4000],\n        [ 1.6000, -0.6000],\n        [ 3.6000,  1.2000],\n        [-0.5000,  0.5000],\n        [-0.2500,  1.7000]]) \n\n tensor([0, 1, 1, 0, 0])\nThis is the same as Microsoft Excel and label 0 corresponds to is_cat and label 1 corresponds to is_dog. Let’s now calculate the Label Smoothing Cross Entropy loss.\nLabelSmoothingCrossEntropy(ε=0.1, reduction='none')(X,y)\n\n>> #out\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\nThe results match our Microsoft Excel LS X-entropy results from fig-5."
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#conclusion",
    "href": "posts/2020-07-18-label-smoothing.html#conclusion",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nI hope that through this blog post, I have been able to help you get a thorough understanding of Label Smoothing. By implementing Label Smoothing Cross Entropy loss in Microsoft Excel, step by step, I also hope that I’ve been clear in my attempt to explain everything that goes on behind the scenes. Please feel free to reach out to me via Twitter at @amaarora - constructive feedback is always welcome."
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#references",
    "href": "posts/2020-07-18-label-smoothing.html#references",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "8 References",
    "text": "8 References\n\nA Simple Guide to the Versions of the Inception Network by Bharat Raj\nWhen does label smoothing help by Hinton et al\nOn Calibration of Modern Neural Networks aka Temperature Scaling by Pleiss et al\nMathematical explainations and proofs for label smoothing by Lei Mao\nLabel Smoothing + Mixup by Jeremy Howard\nCross Entropy Loss in Microsoft Excel by Jeremy Howard"
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#credits",
    "href": "posts/2020-07-18-label-smoothing.html#credits",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "9 Credits",
    "text": "9 Credits\nThis blogpost wouldn’t have been possible without the help of my very talented friend Atmadeep Banerjee. Atmadeep, is currently interning and researching about Instance Segmentation at Harvard! You can find some of his very cool projects at his GitHub here.\nAtmadeep was very kind to jump on a call with me for over an hour, when I was unable to replicate the results in Excel and help me find my mistake - LOG function in excel has base 10 whereas in numpy and pytorch it’s LOG to the base e! In MS Excel, LOG to the base e is referred to as LN.\nIt was really funny to have spent the day reading numerous blog posts, few research papers and source code for PyTorch and then finding out that MS Excel implements LOG function differently than numpy and pytorch. But hey, lesson learnt, when in doubt, contact @Atmadeep Banerjee - he has an eye for detail."
  },
  {
    "objectID": "posts/2020-08-30-gempool.html",
    "href": "posts/2020-08-30-gempool.html",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "",
    "text": "As mentioned in the title, today we will be looking at GeM pooling and also look at the research paper Fine-tuning CNN Image Retrieval with No Human Annotation. Recently, I started to participate in Google Landmark Recognition 2020 Kaggle competition and as a result I have been reading lots of research papers related to large scale image retrieval and recognition. GeM Pooling featured in almost all of them [1], [2], [3]. Therefore, today we will look at GeM Pooling and understand how it is different from the common Max and Average pooling techniques.\nWe will also look at the Fine-tuning CNN Image Retrieval with No Human Annotation paper briefly and understand about some common ways of doing large scale image retrieval and recognition.\nFinally, we also run our own little experiment to compare the performance of GeM Pooling with Average Pooling on PETs Dataset. In the experiment, we find that GeM Pooling doesn’t necessarily lead to performance gains on a classification task and a working notebook can be found here.\nIn this blog post, we touch upon the basics of image retrieval, and also understand GeM Pooling.\nSo, let’s get started."
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#image-retrieval-overview",
    "href": "posts/2020-08-30-gempool.html#image-retrieval-overview",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "2 Image Retrieval Overview",
    "text": "2 Image Retrieval Overview\nYou might want to ask what is Image Retrieval? Why is it relevant and how is this of any importance?\n\nUsually, when we do google image search, or we want to find similar images - this warrants a need to find similar images related to the query image. This task of finding similar images from a large scale unordered database of images is referred to as Image Retrieval. In fact Google Landmark Retrieval 2020 Kaggle competition is specific to the image retrieval task.\n\n2.1 How does it work?\nThe idea is to extract global and local features from the image and perform matching/image similarity based on these extracted vectors. Typically, representing an image as a vector is as simple as extracting the output from a CNN just before the pooling layer. We do this for each image in the database and the query image. Finally, to get image similarity is as simple as performing a dot product between these vector representations and similar images have higher dot product value whereas non similar images have smaller dot-product values. Thus, given a query image, it is possible to find similar images from a large scale unordered database.\nThe complexity lies in having suitable/accurate vector representations for each image. To do this, it is possible to train a CNN to do image classification and then extract the image representations from this trained model.\nTo have appropriate image representations, a trained model must be able to distinguish similar images of the same class from non-similar images. Such a model could be trained as a Siamese network where it could be fed similar images by finding images with the same label class in the dataset and non-similar images by finding images with a different label.\nFor example, let’s say we have a dataset consisting of dogs, cats and horses with 10 images per label. A model could be trained by providing matching or non-matching image pairs. If two dog images are provided to the model, it should say output matching, whereas if an image of a dog and a cat are provided to the model, then it must be able to distinguish between the two and should say non-matching pair. Such a model can be trained as a Siamese network using Contrastive loss. (more about this later in the blogpost)\nBut, getting labelled data is costly. Could there be a workaround this?"
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#unsupervised-matching-and-non-matching-pairs",
    "href": "posts/2020-08-30-gempool.html#unsupervised-matching-and-non-matching-pairs",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "3 Unsupervised matching and non-matching pairs",
    "text": "3 Unsupervised matching and non-matching pairs\nIn the Fine-tuning CNN Image Retrieval with No Human Annotation research paper, the authors Radenovic et al dispense with the need for manually annotated data or any assumptions on the training dataset.\nBasically, there is no longer a need for image labels to find similar images. Previously, we would look at image labels and feed two dog labelled images to the model and train it to say that these two images are a matching pair. And we would also feed the model with a dog and cat labelled image and train it to say that these two images are a non-matching pair.\nRadenovic et al take away the need for image labels to find similar or non-similar images. They do this by using SfM pipeline [4].\nFrom the paper, > We achieve this by exploiting the geometry and the camera positions from 3D models reconstructed automatically by a structure-from-motion (SfM) pipeline. The state- of-the-art retrieval-SfM pipeline takes an unordered image collection as input and attempts to build all possible 3D models.\n\nFrom the image above, the SfM Pipeline can construct 3D models given an unordered database of images. A typical reconstruction process looks something like below:\n\nAs seen in the image above, it is possible to reconstruct detailed 3D models from unordered photo collections. This is done using local spatial verification and more details can be found in [4].\nThus, using this technique, one can find similar images and non-similar images without the need for image labels. Now, once we have matching and non-matching pairs, one can train a model using Contrastive Loss such that a trained model can separate similar images from non-similar ones, thus providing accurate vector representations for each image."
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#image-retrieval-architecture",
    "href": "posts/2020-08-30-gempool.html#image-retrieval-architecture",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "4 Image Retrieval Architecture",
    "text": "4 Image Retrieval Architecture\nIn this section, we will look at the network architecture presented in Fine-tuning CNN Image Retrieval with No Human Annotation for image retrieval and also look at proposed GeM Pooling layer in detail.\n\nAs mentioned before, it is possible to first get a vector representation of an image by getting the output from a CNN just before the pooling layer. Next, the authors perform the proposed GeM Pooling operation to reduce the dimensionality, perform normalization and finally the final output is the image descriptor.\nIt is then possible to train a network to get accurate image descriptors through Siamese Learning and Contrastive Loss by training a network with positive pairs (matching pairs) and negative pairs (non-matching pairs).\nOne could wonder what possible advantages could GeM Pooling have over Max/Average Pooling?\n\nAs can be seen in the image above, the authors report that by training a network using Siamese Learning and GeM Pooling, the implicit correspondences between images are improved after fine-tuning. Moreover, the CNN fires less on ImageNet classes, e.g. cars and bicycles and is better able to find similar images, thus improving image retrieval process."
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#gem-pooling",
    "href": "posts/2020-08-30-gempool.html#gem-pooling",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "5 GeM Pooling",
    "text": "5 GeM Pooling\nHaving looked at an overview of the Image Retrieval process, let’s now look at the proposed GeM Pooling operation in detail. In this section, we also look a code-level implementation of the GeM Pooling layer in PyTorch.\nGiven an input image, the output from a CNN is a 3D tensor of shape K x H x W where, K is the number of channels, H refers to feature map height and W refers to feature map width. If Xk represents the H x W spatial feature map activation, then the network consists of K such feature maps.\nFor Max Pooling, \nIn other words, for each feature map Xk, we take the maximum value to get a K length long vector representation of the image.\nFor Average pooling, \nIn other words, for each feature map Xk, we take the average value to get a K length long vector representation of the image.\nFinally, for GeM Pooling: \nFrom the paper, > Pooling methods (eq-1) and (eq-2) are special cases of GeM pool- ing given in (eq-3), i.e., max pooling when pk→∞ and average pooling for pk = 1. The feature vector finally consists of a single value per feature map, i.e. the generalized-mean activation, and its dimensionality is equal to K. For many popular networks this is equal to 256, 512 or 2048, making it a compact image representation. The pooling parameter pk can be manually set or learned since this operation is differentiable and can be part of the back-propagation.\nThus, GeM Pooling layer is trainable. One can either fix the hyperparameter pk or train it using back propagation as part of the standard model training process.\n\n5.1 PyTorch Implementation\nHopefully, by looking at the code implementation, GeM Pooling process will become clearer to the reader.\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\nWe initialize a trainable Parameter- p, and replicate eq-3 in self.gem function call. As can be seen, this pooling operation is different from Average Pooling and Max Pooling operations."
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#siamese-learning-and-loss-function",
    "href": "posts/2020-08-30-gempool.html#siamese-learning-and-loss-function",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "6 Siamese Learning and Loss Function",
    "text": "6 Siamese Learning and Loss Function\nNow that we have looked at GeM Pooling, let’s understand how do we train a network that can perform image retrieval task. The answer lies in Siamese Learning as mentioned before.\nThe training input consists of image pairs (i, j) and labels Y(i, j) ∈ {0, 1} declaring whether a pair is non-matching (label 0) or matching (label 1). Therefore, it two images of two dog images are fed to the network, the label is 1 corresponding to matching pair and if dog and cat images are fed to the network, then the label is 0 corresponding to non-matching pair.\nThen, we can train the network using Contrastive Loss:\n\nHere f(i) is the L2-normalized GeM vector of image i and τ is a margin parameter defining when non-matching pairs have large enough distance in order to be ignored by the loss. The network is usually trained using a large number of training pairs created automatically using the SfM-pipeline discussed before. The authors found that the contrastive loss generalizes better and converges at higher performance than the triplet loss."
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#image-representation-and-search",
    "href": "posts/2020-08-30-gempool.html#image-representation-and-search",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "7 Image Representation and Search",
    "text": "7 Image Representation and Search\nNow that we have looked at the training process using Siamese Learning let’s look at how one can find similar images. Once the training is finished, an image is fed to the net- work shown in fig-4. We get the Descriptor as output at a single scale. Since it is possibe that the object of interest in the query image is either zoomed in or zoomed out, the query image is fed to the network at multiple scales. The resulting descriptors are then finally pooled and re-normalized thus, we get a scale invariance representation of the query image.\nThen, image retrieval is simply performed by exhaustive Euclidean search over database descriptors w.r.t. the query descriptor. This is equivalent to the inner product evaluationof L2 normalized vectors, i.e. vector-to-matrix multiplication, and sorting. Images with highest scores are the most similar and the lowest scores are the least similar.\nThus, we have successfully performed the task of image retrieval and sorted the images by similarity."
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#conclusion",
    "href": "posts/2020-08-30-gempool.html#conclusion",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nIn this blog post, I hope I have been able to provide the reader with an introduction to image retrieval tasks in Computer Vision and also introduce the GeM Pooling layer. We also looked at how to implement GeM Pooling in PyTorch and performed a simple experiment to compare GeM Pooling with Average Pooling for PETs classification task and found GeM Pooling and Average Pooling to have comparable accuracy. The working notebook can be found here.\nThe authors Radenovic et al won 6th palce in Google Landmark Image Retrieval 2018 and 9th place in Google Landmark Image Retrieval 2019. Thus, we can safely say that GeM Pooling works particularly for image retrieval based tasks.\nAs usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#references",
    "href": "posts/2020-08-30-gempool.html#references",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "9 References",
    "text": "9 References\n\nUnifying Deep Local and Global Features for Image Search\n6th Place Solution: VRG Prague\n9th Place Solution: VRG Prague\nFrom Single Image Query to Detailed 3D Reconstruction"
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#credits",
    "href": "posts/2020-08-30-gempool.html#credits",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "10 Credits",
    "text": "10 Credits\nThe code implementation of GeM Pooling has been copied and replicated from the official implementation in PyTorch CNN Image Retrieval toolbox."
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html",
    "href": "posts/2021-03-13-optimizers.html",
    "title": "Adam and friends",
    "section": "",
    "text": "Adam is an Optimizer. He has many friends but his dearest are SGD, Momentum & RMSprop.\nEach of Adam’s friends has contributed to Adam’s personality. So to get to know Adam very well, we should first meet the friends. We start out with SGD first, then meet Momentum, RMSprop and finally Adam.\n\nIn this blog post we are going to re-implement SGD, SGD with Momentum, RMSprop & Adam. The major contribution of this blog post is to help the reader re-implement these algorithms keeping the implementations simple & by using minimal lines of code. We try to understand these algoirthms from a code perspective rather than from a mathematical perspective. I would also like to refer the reader to Sebastian Ruder’s blog on Optimizers here for a more theoretical introduction. We also compare the implementations with PyTorch’s implementations to check accuracy.\n\nThis blog post has been structured the following way:"
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#introduction",
    "href": "posts/2021-03-13-optimizers.html#introduction",
    "title": "Adam and friends",
    "section": "2 Introduction",
    "text": "2 Introduction\nIn this blog post we are going to re-implement SGD, Momentum, RMSprop and Adam from scratch.\nIn this blog post, the code for the Optimizers has been mostly copied from PyTorch but follows a different structure to keep the code implementations to a minimum. The implementations for these various Optimizers in this blog post are “much shorter” than those in PyTorch.\nI also compared the re-implementations with PyTorch’s implementations and excited to share results below! SGD, SGD_with_momentum, RMSprop and Adam are from Pytorch whereas SGDOptimizer, SGDOptimizer_with_momentum, RMSPropOptimizer and AdamOptimizer are our own re-implementations. As shown in the fig-1 below, results are comparable!\n\nRefer here for a complete working notebook to reproduce fig-1."
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#resourcescredits",
    "href": "posts/2021-03-13-optimizers.html#resourcescredits",
    "title": "Adam and friends",
    "section": "3 Resources/Credits",
    "text": "3 Resources/Credits\nGenerally the resources section is at the last but I’d like to share some wonderful resources right at the start that have helped shape this blog post in it’s current form.\n\nIntroduction to SGD by Jeremy Howard. In lesson-2 of Course 19 fast.ai, Jeremy re-implements SGD from scratch using Python!\nIntroduction to Optimizers by Jeremy Howard. In lesson-5 of Course 19 fast.ai, Jeremy re-implements SGD, Momentum, RMSprop & Adam in Microsoft Excel! This is a great resource to learn about these algorithms. I started here too and then re-implemented the algorithms in PyTorch that has led to this blog post.\nGeneric Optimizer by Jeremy Howard. In Lesson-11 of Course 19 fast.ai, Jeremy creates a Generic Optimizer. Some of the code in this blog post has been inspired from here, but majorly we follow the code implementations as in PyTorch.\nCS231n Introduction to Optimizers. This is another great resource from Stanford that introduces Optimization and is a great resource to get an intuition for SGD. It also showcases how to compute the gradients from scratch without using torch.autograd. In our blog post, we use torch.autograd instead to compute the gradients.\nAn overview of gradient descent optimization algorithms by Sebastian Ruder is an excellent blog post by one of my favorite researchers and presents the various Optimization algorithms such as Adagrad, Adadelta, AdaMax, Nadam, AMSGrad and more in an easy to understand manner!\nWhy Momentum Really Works from distil.pub. If you haven’t heard of distil.pub, stop what you’re doing and visit this wonderful website that distils research using visual explainations that are easy to understand!\n\nHaving mentioned these resources, we are now ready to start on our journey of re-implementing SGD, Momentum, RMSprop and Adam from scratch. We first start out with SGD below:"
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#stochastic-gradient-descent",
    "href": "posts/2021-03-13-optimizers.html#stochastic-gradient-descent",
    "title": "Adam and friends",
    "section": "4 Stochastic Gradient Descent",
    "text": "4 Stochastic Gradient Descent\nIn this section we will first introduce what is Stochastic Gradient Descent and then based on our understanding, implement it in PyTorch from scratch.\n\n4.1 What is Stochastic Gradient Descent?\nFor an intuitive understanding, refer fig-2 below:\n\nLet’s say we are standing at a certain point A of a parabolic hill as shown in fig-2 and we wish to find the lowest point on this curve. Can you think of some ways to do this? Well, we could try going in a random direction, calculate the value of the function and if it’s lower than the previous value, we could take a step in that direction. But this process is slow. With some mathematical magic, we can make this process faster. In fact, the fastest way down a function or the sleepest way down the hill is the one in the opposite direction of the gradient. Gradient at point A is the slope of the parabolic function, and by calculating the gradients, we can find the steepest direction in which to move to minimise the value of the function. This is referred to as Gradient Descent. Ofcourse in a high dimensional space, calculating the gradients is a little bit more complicated than in fig-2 but the idea remains the same. We take a step from point A directed by the gradients to follow the steepest path downwards to point B to find the lowest value of the curve. The step-size is governed by a parameter called learning rate. The new position B then can be defined as B = A - lr * A.grad where A.grad represents the slope/gradients of the curve at point A.\nThe stochasticity in Stochastic Gradient Descent arises when we compute the batch gradients. This has been explained below through pseudo-code in Vanilla Stochastic Gradient Descent.\nFrom the Introduction to SGD by Jeremy Howard, and from fig-2, we already know that to perform Gradient Descent, we need to be able to calculate the gradients of some function that we wish to minimise with respect to the parameters.\nWe don’t need to manually calculate the gradients and as mentioned in this video by Jeremy, PyTorch can already do this for us using torch.autorgrad.\nSo now that we know that we can compute the gradients, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent. Its vanilla version looks as follows:\n# Vanilla Gradient Descent\n\nfor epoch in range(num_epochs):\n    predictions = model(training_data)\n    loss = loss_function(predictions, ground_truth)\n    weights_grad = evaluate_gradient(loss) # using torch by calling `loss.backward()`\n    weights -= learning_rate * weights_grad # perform parameter update\nIn Vanilla Gradient Descent, we first get the predictions on the whole training data, then we calculate the loss using some loss function. Finally, we update the weights in the direction of the gradients to minimise the loss. We do this repeatedly for some predefined number of epochs.\n\nCan you think of possible problems with this approach? Can you think of why this approach could be computationally expensive?\n\nIn large-scale applications, the training data can have on order of millions of examples. Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over batches of the training data. This approach is reffered to as Stochastic Gradient Descent:\n# Vanilla Stochastic Gradient Descent\nfor epoch in range(num_epochs):\n    for input_data, labels in training_dataloader:\n        preds = model(input_data)\n        loss  = loss_function(preds, labels)\n        weights_grad = evaluate_gradient(loss) # using torch by calling `loss.backward()`\n        weights -= learning_rate * weights_grad # perform parameter update\nIn Stochastic Gradient Descent, we divide our training data into sets of batches. This is essentially what the DataLoader does, it divides the complete training set into batches of some predefined batch_size.\nSo let’s keep the key things in our mind before we set out to implement SGD: 1. Divide the training data into batches, PyTorch DataLoaders can do this for us. 2. For each mini-batch: - Make some predictions on the input data and calculate the loss. - Calculate the gradients using torch.autograd based on the loss. - Take a step in the opposite direction of gradients to minimise the loss.\nWe follow a similar code implementation to PyTorch. In PyTorch as mentioned here, there is a base class for all optimizers called torch.optim.Optimizer. It has some key functions methods like zero_grad, step etc. Remember from our general understanding of SGD, we wish to be able to update the parameters (that we want to optimize) by taking a step in the opposite direction of the gradients to minimise the loss function.\nThus, from a code implementation perspective, we would need to be able to iterate through all the parameters and do p = p - lr * p.grad, where p refers to parameters and lr refers to learning rate.\nWith this basic understanding let’s implement an Optimizer class below:\nclass Optimizer(object):\n    def __init__(self, params, **defaults):\n        self.params = list(params)\n        self.defaults = defaults\n    \n    def grad_params(self):\n        return [p for p in self.params if p.grad is not None]\n    \n    def step(self): \n        raise NotImplementedError\n    \n    def zero_grad(self):\n        for p in self.grad_params():\n            p.grad.zero_()\nThe Optimizer class above implements two main methods - grad_params and zero_grad. Doing something like self.grad_params() grabs all those parameters as a list whose gradients are not None. Also, calling the zero_grad() method would zero out the gradients as explained in this video.\n\n\nAt this stage you might ask, what are these parametrs? In PyTorch calling model.parameters() returns a generator through which we can iterate through all parameters of our model. A typical training loop as you might have seen in PyTorch looks something like:\n\nmodel = create_model()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\nfor input_data, labels in train_dataloader:\n    preds = model(input_data)\n    loss  = loss_fn(preds, labels)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\nIn the training loop above we first create an optimizer by passing in model.parameters() which represents the parameters that we wish to optimize. We also pass in a learning rate that represents the step size. In PyTorch, calling loss.backward() is what appends an attribute .grad to each of the parameters in model.parameters(). Therefore, in our implementation, we can grab all those parameters whose gradients are not None by doing something like [p for p in self.params if p.grad is not None].\nNow to implement SGD optimizer, we just need to create a method called step that does the optimization step and updates the value of the model parameters based on the gradients.\nclass SGDOptimizer(Optimizer):\n    def __init__(self, params, **defaults):\n        super().__init__(params, **defaults)\n    \n    def step(self):\n        for p in self.grad_params():\n            p.data.add_(p.grad.data, alpha=-self.defaults['lr'])\nThis line p.data.add_(p.grad.data, alpha=-self.defaults['lr']) essentially does p = p - lr * p.grad which is the SGD step for each mini-batch. Thus, we have successfully re-implemented SGD Optimizer."
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#sgd-with-momentum",
    "href": "posts/2021-03-13-optimizers.html#sgd-with-momentum",
    "title": "Adam and friends",
    "section": "5 SGD with Momentum",
    "text": "5 SGD with Momentum\nClassical Momentum as described in this paper can be defined as:\n\nHere µ represents the momentum factor, typically 0.9. Δ𝑓(θt) represents the gradients of parameters θ at time t. And ε represents the learning rate.\nAs can be seen from eq-1, essentially we add a factor µ times the value of the previous step to the current step. Thus instead of going p = p - lr * p.grad, the new step value becomes new_step = µ * previous_step + lr * p.grad whereas previously for SGD, the step value was lr * p.grad.\nWhat is momentum you might ask? Why does it work?\nFrom Why Momentum Really Works from distil.pub -\n\nHere’s a popular story about momentum - gradient descent is a man walking down a hill. He follows the steepest path downwards; his progress is slow, but steady. Momentum is a heavy ball rolling down the same hill. The added inertia acts both as a smoother and an accelerator, dampening oscillations and causing us to barrel through narrow valleys, small humps and local minima. It is simple — when optimizing a smooth function f, we make a small step in the gradient -\n\n\n\nFor a step-size small enough, gradient descent makes a monotonic improvement at every iteration. It always converges, albeit to a local minimum. Things often begin quite well — with an impressive, almost immediate decrease in the loss. But as the iterations progress, things start to slow down. You start to get a nagging feeling you’re not making as much progress as you should be. What has gone wrong?\n\n\nThe landscapes are often described as valleys, trenches, canals and ravines. The iterates either jump between valleys, or approach the optimum in small, timid steps. Progress along certain directions grind to a halt. In these unfortunate regions, gradient descent fumbles.\n\nMomentum proposes the following tweak to gradient descent. We give gradient descent a short-term memory -\n\n\nThe change is innocent, and costs almost nothing. When = 0β=0 , we recover gradient descent. But for = 0.99β=0.99 (sometimes 0.9990.999, if things are really bad), this appears to be the boost we need. Our iterations regain that speed and boldness it lost, speeding to the optimum with a renewed energy.\n\n\nThus, essentially, with Momentum, if the momentum factor as in eq-3 is β, then compared to SGD, instead of the new step just being guided by the gradients, is also guided by β times the old step size. Thus, to implement momentum, we would need to keep a track of the previous steps. We do this by storing moment_buffer inside a param_state for each parameter as in the implementation below:\nclass SGDOptimizer(Optimizer):\n    def __init__(self, params, **defaults):\n        super().__init__(params, **defaults)\n        self.lr = defaults['lr']\n        self.µ  = defaults['momentum']\n        self.state = defaultdict(dict)\n    \n    def step(self):\n        for p in self.grad_params():\n            param_state = self.state[p]\n            \n            d_p = p.grad.data            \n            if 'moment_buffer' not in param_state:\n                buf = param_state['`moment_buffer`'] = torch.clone(d_p).detach()\n            else:\n                buf = param_state['moment_buffer']\n            \n            buf.mul_(self.µ).add_(d_p)\n            \n            p.data.add_(buf, alpha=-self.lr)\nFrom Sebastian Ruder’s blog: > At the core of Momentum is this idea - why don’t we keep going in the same direction as last time? If the loss can be interpreted as the height of a hilly terrain, then the optimization process can then be seen as equivalent to the process of simulating the parameter vector as rolling on the landscape. Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way. The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence.\nFrom a code implementation perspective, for each parameter inside self.grad_params(), we store a state called momentum_buffer that is initialized with the first value of p.grad. For every subsequent update, we do buf.mul_(self.µ).add_(d_p) which represents buf = buf * µ + p.grad. And finally, the parameter updates become p.data.add_(buf, alpha=-self.lr) which is essentially p = p - lr * buf.\nThus, we have successfully re-implemented eq-1."
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#rmsprop",
    "href": "posts/2021-03-13-optimizers.html#rmsprop",
    "title": "Adam and friends",
    "section": "6 RMSprop",
    "text": "6 RMSprop\nRMSprop Optimizer brings to us an idea that why should all parameters have the step-size when clearly some parameters should move faster? It’s great that RMSprop was actually introduced as part of a MOOC by Geoffrey Hinton in his course.\nFrom the PyTorch docs: > The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus α/(sqrt(v) + ϵ) where α is the scheduled learning rate and v is the weighted moving average of the squared gradient.\nThe update step for RMSprop looks like:\n\nEssentially, for every parameter we keep a moving average of the Mean Square of the gradients. Next, we update the parameters similar to SGD but instead by doing something like p = p - lr * p.grad, we instead update the parameters by doing p(t) = p(t) - (lr / MeanSquare(p, t)) * p(t).grad.\nHere, p(t) represents the value of the parameter at time t, lr represents learning rate and MeanSquare(p, t) represents the moving average of the Mean Square Weights of parameter p at time t.\n\nKey takeaway to be able to implement RMSprop - we need to able to store the exponentially weighted moving average of the mean square weights of the gradients.\n\nTherefore, we can update the implementation of SGD with momentum to instead implement RMSprop like so:\nclass RMSPropOptimizer(Optimizer):\n    def __init__(self, params, **defaults):\n        super().__init__(params, **defaults)\n        self.lr  = defaults['lr']\n        self.α   = defaults['alpha']\n        self.eps = defaults['epsilon']\n        self.state = defaultdict(dict)\n    \n    def step(self):\n        for p in self.grad_params():\n            param_state = self.state[p]\n            \n            d_p = p.grad.data   \n            if 'exp_avg_sq' not in param_state:\n                exp_avg_sq = param_state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            else:\n                exp_avg_sq = param_state['exp_avg_sq']\n            \n            exp_avg_sq.mul_(self.α).addcmul_(d_p, d_p, value=1-self.α)\n            denom = exp_avg_sq.sqrt().add_(self.eps)\n            \n            p.data.addcdiv_(d_p, denom, value=-self.lr)\nAs can be seen inside the step method, we iterate through the parameters with gradients, and store the initial value of the gradients inside the a variable called d_p which represents derivative of parameter p.\nNext, we initialize the exponential moving average of the square of the gradients exp_avg_sq as an empty array filled with zeros of the same shape as d_p. For every next step, this exp_avg_sq is updated by this line of code: exp_avg_sq.mul_(self.α).addcmul_(d_p, d_p, value=1-self.α). This equates to exp_avg_sq = (self.α * exp_avg_sq)  + (1 - self.α * (d_p**2)).\nTherefore, we are keeping an exponentially weighted moving average of the square of the gradients. But as can be seen in eq-2, the update step of RMSprop actually divides by the sqrt of this exp_avg_sq. So our denominator denom becomes exp_avg_sq.sqrt().add_(self.eps). eps is added for numerical stability.\nFinally, we do our update step p.data.addcdiv_(d_p, denom, value=-self.lr) which equates to p = p - (self.lr * d_p)/denom thus performing the RMSprop update step as in eq-2.\nTherefore, we have successfully re-implemented RMSprop from scratch."
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#adam",
    "href": "posts/2021-03-13-optimizers.html#adam",
    "title": "Adam and friends",
    "section": "7 Adam",
    "text": "7 Adam\nFrom the paper:\n\nThe method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation.\n\nIn this section we are going to discuss Adam. Adam’s algorithm as defined in the paper is shown below:\n\nWe are going to re-implement this algorithm using PyTorch.\n\nI have long tried to understand all the math behind Adam, reading the paper multiple times. But I believe that I can at best contribute towards helping the reader re-implement Adam in PyTorch and not in explaining all the math behind the algorithm. In various papers and algorithms such as RMSprop, it is mentioned that dividing by the sqrt of second moments of the gradients, we can achieve better stability. As to why? I am not sure. Having said that, it is best to assume that this given algorithm works and try to re-implement in PyTorch.\n\nAs can be seen from fig-2, to re-implement Adam, we need to be able to keep a moving average of the first and second moments of the gradients. Finally, based on the bias correction term 1 - β1t for the first moment estimate and 1 - β2t for the second moment estimate, we compute the biased corrected version and first and second raw moment estimates.\nFinally, the update step for the parameters at time t becomes:\nθt = θt-1 - α * m_hatt / (sqrt( v_hatt) + ε)\nWhere, θt - Parameter vector at time t α - Learning rate m_hatt - Bias corrected first moment estimate v_hatt - Bias corrected second moment estimate\nReplicating this algorithm in PyTorch is fairly straightforward as shown in the code implementation below:\nclass AdamOptimizer(Optimizer):\n    def __init__(self, params, **defaults):\n        super().__init__(params, **defaults)\n        self.lr   = defaults['lr']\n        self.ß1   = defaults['beta1']\n        self.ß2   = defaults['beta2']\n        self.eps = defaults['epsilon']\n        self.state = defaultdict(dict)\n        self.state_step = 0\n    \n    def step(self):\n        for p in self.grad_params():\n            self.state_step+=1\n            param_state = self.state[p]\n            \n            d_p = p.grad.data   \n            \n            if 'exp_avg' not in param_state:\n                exp_avg = param_state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            else:\n                exp_avg = param_state['exp_avg']\n                \n            if 'exp_avg_sq' not in param_state:\n                exp_avg_sq = param_state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            else:\n                exp_avg_sq = param_state['exp_avg_sq']\n            \n            exp_avg.mul_(self.ß1).add_(d_p, alpha=1-self.ß1)\n            exp_avg_sq.mul_(self.ß2).addcmul_(d_p, d_p, value=1-self.ß2)\n\n            bias_correction_1 = 1 - self.ß1**self.state_step\n            bias_correction_2 = 1 - self.ß2**self.state_step\n            \n            unbiased_exp_avg = exp_avg/bias_correction_1\n            unbiased_exp_avg_sq = exp_avg_sq/bias_correction_2\n            \n            denom = unbiased_exp_avg_sq.sqrt().add_(self.eps)\n            \n            step_size = self.lr / bias_correction_1\n            \n            p.data.addcdiv_(unbiased_exp_avg, denom, value=-step_size)\nLooking at the step method of the above implementation, we can directly relate the implementation to the Adam algorithm as in fig-2. We store the gradients of the paramter p in a variable called d_p. Next, for each parameter we store a state referred to as param_state.\nFrom the algorithm, we know that we need to store both first and second moments of the gradients. Therefore, if both exp_avg (first moment) and exp_avg_sq (second moment) are null, we initialize them as zeroes with the same shape as p.\nOnce initialized, then for every subsequent step we grab the first and second moments and update them based on the update rule as in the Adam algorithm.\nFor first moment exp_avg, we do exp_avg.mul_(self.ß1).add_(d_p, alpha=1-self.ß1) which equates to exp_avg = self.ß1 * exp_avg + (1 - self.ß1) * d_p. This is the same as the Update biased first moment step in the algorithm. exp_avg is equivalent to mt in the algorithm.\nFor the second moment exp_avg_sq, we do exp_avg_sq.mul_(self.ß2).addcmul_(d_p, d_p, value=1-self.ß2) which equates to exp_avg_sq = self.ß2 * exp_avg_sq + (1 - self.ß2) * (d_p**2). This is the same as the update biased second raw moment estimate step in the algorithm. exp_avg_sq is equivalent to vt in the algorithm.\nFinally, we calculate the bias correction terms as mentioned in the algorithm and calculate the unbiased_exp_avg which equates to m_hatt in the algorithm. We also calculate the unbiased_exp_avg_sq after bias correction and unbiased_exp_avg_sq equates to v_hatt in the algorithm.\nWe calulate the denominator denom as in the algorithm denom = unbiased_exp_avg_sq.sqrt().add_(self.eps). Finally, we perform the parameter update step p.data.addcdiv_(unbiased_exp_avg, denom, value=-step_size) which equates to p = p - unbiased_exp_avg * step_size / denom that is equivalent to the last step in the algorithm.\nThus, we have successfully re-implemented Adam."
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#working-notebook",
    "href": "posts/2021-03-13-optimizers.html#working-notebook",
    "title": "Adam and friends",
    "section": "8 Working notebook",
    "text": "8 Working notebook\n\nRefer here for a complete working notebook to reproduce fig-1."
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#conclusion",
    "href": "posts/2021-03-13-optimizers.html#conclusion",
    "title": "Adam and friends",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nI hope that through this blog, I have been able to explain all the magic that goes on inside the various optimizers such as SGD, Momentum, RMSprop and Adam!\nAs usual, in case we have missed anything or to provide feedback, please feel free to reach out to me at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "posts/2020-08-13-efficientnet.html",
    "href": "posts/2020-08-13-efficientnet.html",
    "title": "EfficientNet",
    "section": "",
    "text": "It brings me great pleasure as I begin writing about EfficientNets for two reasons: 1. At the time of writing, Fixing the train-test resolution discrepancy: FixEfficientNet (family of EfficientNet) is the current State of Art on ImageNet with 88.5% top-1 accuracy and 98.7% top-5 accuracy. 2. This blog post also sets up the base for future blog posts on Self-training with Noisy Student improves ImageNet classification, Fixing the train-test resolution discrepancy and Fixing the train-test resolution discrepancy: FixEfficientNet.\nIn this blog post, in The “Why” section, we take a look at the superior performance of EfficientNets compared to their counterparts and understand why we are looking into EfficientNets and “why” they are totally worth your time.\nNext, in “The How” section, we start to unravel the magic inside EfficientNets. Particularly, we look at two main contributions from the research paper: 1. Compound Scaling 2. The EfficientNet Architecture (developed using Neural Architecture Search)\nHaving introduced the two contributions in The “How”, we the compare the conventional methods of scaling with Compound Scaling approach in Comparing Conventional Methods with Compound Scaling.\nFinally we look at the details of the EfficientNet Architecture in The EfficientNet Architecture using NAS and learn how the authors used Nerual Architecture Search (NAS) to get EfficientNet-B0 architecture and scaled it using Compound Scaling technique to get EfficientNet B1-B7.\nSo, let’s get started!"
  },
  {
    "objectID": "posts/2020-08-13-efficientnet.html#the-why",
    "href": "posts/2020-08-13-efficientnet.html#the-why",
    "title": "EfficientNet",
    "section": "2 The “WHY”?",
    "text": "2 The “WHY”?\nIn this section we understand “why” EfficientNets are totally worth your time.\nfig-1 below summarizes “why” we could a learn a lot by understanding the EfficientNet Architecture.\n\nAs we can see from fig-1, EfficientNets significantly outperform other ConvNets. In fact, EfficientNet-B7 achieved new state of art with 84.4% top-1 accuracy outperforming the previous SOTA GPipe but being 8.4 times smaller and 6.1 times faster.\nFrom the paper, > EfficientNet-B7 achieves state- of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.\nThe great thing about EfficientNets is that not only do they have better accuracies compared to their counterparts, they are also lightweight and thus, faster to run.\nHaving looked at their superior accuracies and faster runtimes, let’s start to unravel the magic step-by-step."
  },
  {
    "objectID": "posts/2020-08-13-efficientnet.html#the-how",
    "href": "posts/2020-08-13-efficientnet.html#the-how",
    "title": "EfficientNet",
    "section": "3 The “HOW”?",
    "text": "3 The “HOW”?\nSo “how” did the authors Mingxing Tan and Quoc V. Le make EfficientNets perform so well and efficiently?\nIn this section we will understand the main idea introduced in the research paper - Compound Scaling.\n\n3.1 Compound Scaling\nBefore the EfficientNets came along, the most common way to scale up ConvNets was either by one of three dimensions - depth (number of layers), width (number of channels) or image resolution (image size).\nEfficientNets on the other hand perform Compound Scaling - that is, scale all three dimensions while mantaining a balance between all dimensions of the network.\nFrom the paper: > In this paper, we want to study and rethink the process of scaling up ConvNets. In particular, we investigate the central question: is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency? Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. Based on this observation, we propose a simple yet effective compound scaling method. Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.\nThis main difference between the scaling methods has also been illustrated in fig-2 below.\n\nIn fig-2 above, (b)-(d) are conventional scaling that only increases one dimension of network width, depth, or resolution. (e) is the proposed compound scaling method that uniformly scales all three dimensions with a fixed ratio.\nThis main idea of Compound Scaling really set apart EfficientNets from its predecessors. And intuitively, this idea of compound scaling makes sense too because if the input image is bigger (input resolution), then the network needs more layers (depth) and more channels (width) to capture more fine-grained patterns on the bigger image.\nIn fact, this idea of Compound Scaling also works on existing MobileNet and ResNet architectures.\nFrom table-1 below, we can clearly see, that the versions of MobileNet and ResNet architectures scaled using the Compound Scaling approach perform better than their baselines or also those architectures that were scaled using conventional methods - (b)-(d) in fig-2.\n\nThus, it is safe to summarize - Compound Scaling works! But, we’re not done yet, there’s more magic to be unraveled.\n\n\n3.2 Neural Architecture Search\nSince we are looking at the “how” - while so far we know Compound Scaling was the main idea introduced - the authors found that having a good baseline network is also critical.\nIt wasn’t enough to achieve such great performance by picking up any existing architecture and applying Compound Scaling to it. While the authors evaluated the scaling method using existing ConvNets (for example - ResNets and MobileNets in table-1 before), in order to better demonstrate the effectiveness of this scaling method, they also developed a new mobile-size baseline, called EfficientNet using Neural Architecture Search.\nWe understand how they did this is in a lot more detail in a later section of this blog post.\n\n\n3.3 Main Contributions - CS & NAS\nTherefore, to summarize the two main contributions of this research paper were the idea of Compound Scaling and using Neural Architecture Search to define a new mobile-size baseline called EfficientNet. We look at both model scaling and the EfficientNet architecture in a lot more detail in the following sections."
  },
  {
    "objectID": "posts/2020-08-13-efficientnet.html#comparing-conventional-methods-with-compound-scaling",
    "href": "posts/2020-08-13-efficientnet.html#comparing-conventional-methods-with-compound-scaling",
    "title": "EfficientNet",
    "section": "4 Comparing Conventional Methods with Compound Scaling",
    "text": "4 Comparing Conventional Methods with Compound Scaling\nIn this section we look at various ways of scaling neural networks in a lot more detail and compare then with the Compound Scaling approach.\nBasically, the authors of EfficientNet architecture ran a lot of experiments scaling depth, width and image resolution and made two main observations:\n\n\nScaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.\nIn order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.\n\n\n\nThese two observations can also be seen in fig-3. Now, let’s look at the effects of scaling single dimensions on a ConvNet in more detail below.\n\n4.1 Depth\nScaling network depth (number of layers), is the most common way used by many ConvNets.\nWith the advancements in deep learning (particularly thanks to Residual Connections, BatchNorm), it has now been possible to train deeper neural networks that generally have higher accuracy than their shallower counterparts. The intuition is that deeper ConvNet can capture richer and more complex features, and generalize well on new tasks. However, deeper networks are also more difficult to train due to the vanishing gradient problem. Although residual connections and batchnorm help alleviate this problem, the accuracy gain of very deep networks diminishes. For example, ResNet-1000 has similar accuracy as ResNet-101 even though it has much more layers.\nIn fig-3 (middle), we can also see that ImageNet Top-1 Accuracy saturates at d=6.0 and no further improvement can be seen after.\n\n\n4.2 Width\nScaling network width - that is, increasing the number of channels in Convolution layers - is most commonly used for smaller sized models. We have seen applications of wider networks previously in MobileNets, MNasNet.\nWhile wider networks tend to be able to capture more fine-grained features and are easier to train, extremely wide but shallow networks tend to have difficul- ties in capturing higher level features.\nAlso, as can be seen in fig-3 (left), accuracy quickly saturates when networks become much wider with larger w.\n\n\n4.3 Resolution\nFrom the paper: > With higher resolution input images, ConvNets can potentially capture more fine-grained patterns. Starting from 224x224 in early ConvNets, modern ConvNets tend to use 299x299 (Szegedy et al., 2016) or 331x331 (Zoph et al., 2018) for better accuracy. Recently, GPipe (Huang et al., 2018) achieves state-of-the-art ImageNet accuracy with 480x480 resolution. Higher resolutions, such as 600x600, are also widely used in object detection ConvNets (He et al., 2017; Lin et al., 2017).\nIncreasing image resolution to help improve the accuracy of ConvNets is not new - This has been termed as Progressive Resizing in fast.ai course. (explained here).\nIt is also beneficial to ensemble models trained on different input resolution as explained by Chris Deotte here.\nfig-3 (right), we can see that accuracy increases with an increase in input image size.\nBy studying the indivdiual effects of scaling depth, width and resolution, this brings us to the first observation which I post here again for reference:\n\nScaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.\n\n\n\n4.4 Compound Scaling\n\nEach dot in a line in fig-4 above denotes a model with different width(w). We can see that the best accuracy gains can be obvserved by increasing depth, resolution and width. r=1.0 represents 224x224 resolution whereas r=1.3 represents 299x299 resolution.\nTherefore, with deeper (d=2.0) and higher resolution (r=2.0), width scaling achieves much better accuracy under the same FLOPS cost.\nThis brings to the second observation:\n\nIn order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.\n\nHaving looked at Compound Scaling, we will now look at how the authors used neural Architecture Search to get mobile-size network that they named EfficientNet."
  },
  {
    "objectID": "posts/2020-08-13-efficientnet.html#the-efficientnet-architecture-using-nas",
    "href": "posts/2020-08-13-efficientnet.html#the-efficientnet-architecture-using-nas",
    "title": "EfficientNet",
    "section": "5 The EfficientNet Architecture using NAS",
    "text": "5 The EfficientNet Architecture using NAS\nThe authors used Neural Architecture Search approach similar to MNasNet research paper. This is a reinforcement learning based approach where the authors developed a baseline neural architecture Efficient-B0 by leveraging a multi-objective search that optimizes for both Accuracy and FLOPS. From the paper:\n\nSpecifically, we use the same search space as (Tan et al., 2019), and use ACC(m)×[FLOPS(m)/T]w as the optimization goal, where ACC(m) and FLOPS(m) denote the accuracy and FLOPS of model m, T is the target FLOPS and w=-0.07 is a hyperparameter for controlling the trade-off between accuracy and FLOPS. Unlike (Tan et al., 2019; Cai et al., 2019), here we optimize FLOPS rather than latency since we are not targeting any specific hardware device.\n\nThe EfficientNet-B0 architecture has been summarized in table-2 below:\n\nThe MBConv layer above is nothing but an inverted bottleneck block with squeeze and excitation connection added to it. We will learn more about this layer in this section of the blog post.\nStarting from this baseline architecture, the authors scaled the EfficientNet-B0 using Compound Scaling to obtain EfficientNet B1-B7.\n\n5.1 MnasNet Approach\nBefore we understand how was the EfficientNet-B0 architecture developed, let’s first look into the MnasNet Architecture and the main idea behind the research paper.\n\nFrom the paper: > The search framework consists of three components: a recurrent neural network (RNN) based controller, a trainer to obtain the model accuracy, and a mobile phone based inference engine for measuring the latency.\nFor MNasNet, the authors used model accuracy (on ImageNet) and latency as model objectives to find the best architecture.\nEssentially, the Controller finds a model architecture, this model architecture is then used to train on ImageNet, it’s accuracy and latency values are calculated. Then, reward function is calculated and feedback is sent back to controller. We repeat this process a few times until the optimum architecture is achieved such that it’s accuracy is maximum given latency is lower than certain specified value.\nThe objective function can formally be defined as:\n\nUsing the above as reward function, the authors were able to find the MNasNet architecture that achieved 75.2% top-1 accuracy and 78ms latency. More about this approach has been explained here.\n\n\n5.2 Neural Architecture Search for EfficientNets\nThe authors of the EfficientNet research paper used the similar approach as explained above to then find an optimal neural network architecture that maximises ACC(m)×[FLOPS(m)/T]w. Note that for EfficientNets, the authors used FLOPS instead of latency in the objective function since the authors were not targeting specific hardware as opposed to MNasNet architecture.\nFrom the paper: > Our search produces an efficient network, which we name EfficientNet-B0. Since we use the same search space as (Tan et al., 2019), the architecture is similar to MnasNett, except our EfficientNet-B0 is slightly bigger due to the larger FLOPS target (our FLOPS target is 400M).\nThe authors named this architecture as EfficientNet-B0 and it is defined in table-2 shown below again for reference:\n\nSince, the authors of EfficientNets used the same approach and similar neural network search space as MNasNet, the two architectures are very similar.\nSo, the key question now is - what’s this MBConv layer? As I have mentioned before, it is nothing but an inverted residual bottleneck.\nThis has been explained further in the next section.\n\n\n5.3 Inverted Bottleneck MBConv\n\nAs in the case of Bottleneck layers that were introduced in the InceptionV2 architecture, the key idea is to first use a 1x1 convolution to bring down the number of channels and apply the 3x3 or 5x5 convolution operation to the reduced number of channels to get output features. Finally, use another 1x1 convolution operation to again increase the number of channels to the initial value. Bottleneck design used in ResNets has been shown below.\n\nThe inverted bottleneck as in MBConv does the reverse - instead of reducing the number of channels, the first 1x1 conv layer increases the number of channels to 3 times the initial.\nNote that using a standard convolution operation here would be computationally expensive, so a Depthwise Convolution is used to get the output feature map. Finally, the second 1x1 conv layer downsamples the number of channels to the initial value. This has been illustrated in fig-7.\nNow you might ask what’s a Depthwise Convolution? It has been explained very well here.\nSo to summarize, the EfficientNet-B0 architecture uses this inverted bottleneck with Depthwise Convolution operation. But, to this, they also add squeeze and excitation operation which have been explained in my previous blog post here.\nFrom the paper: > The main building block of EfficientNet-B0 is mobile inverted bottleneck MBConv (Sandler et al., 2018; Tan et al., 2019), to which we also add squeeze-and-excitation optimization (Hu et al., 2018).\nThat’s all the magic - explained.\n\n\n5.4 Scaling Efficient-B0 to get B1-B7\nThis is the last section explaining the EffcientNet Architecture. In this section, we look into the details as to how the authors scaled EfficientNet-B0 to get EfficientNet B1-B7.\nLet the network depth(d), widt(w) and input image resolution(r) be:\n\nIntuitively, φ is a user-defined coeffecient that determines how much extra resources are available. The constants α, β, γ determine how to distribute these extra resources accross networks depth(d), width(w) and input resolution(r).\nGiven that we have some extra resources α, β, γ can be determined using a small grid search and thus we can scale networks depth, width and input resolution to get a bigger network.\nFrom the paper: > Starting from the baseline EfficientNet-B0, we apply our compound scaling method to scale it up with two steps: > - STEP 1: we first fix φ = 1, assuming twice more resources available, and do a small grid search of α, β, γ. In particular, we find the best values for EfficientNet-B0 are α = 1.2, β = 1.1, γ = 1.15, under constraint of α * β2 * γ2 ≈ 2. > - STEP 2: we then fix α, β, γ as constants and scale up baseline network with different φ, to obtain EfficientNet-B1 to B7."
  },
  {
    "objectID": "posts/2020-08-13-efficientnet.html#conclusion",
    "href": "posts/2020-08-13-efficientnet.html#conclusion",
    "title": "EfficientNet",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nFirst, we looked at the idea of compound scaling depth, width and image resolution all at the same time instead of the conventional method of scaling only one of the three. Next, we also looked at the various experiments on model scaling and also at the effects of scaling each dimension on model accuracy. We realized, Compound Scaling as a technique works best compared to other conventional methods.\nWe also realized that the baseline network to which Compound Scaling is applied also matters a lot. It is not enough to pick up any existing architecture and scale depth, width and image resolution. The authors therefore, used Nueral Architecture Search to get a mobile-size network that’s very similar to MNasNet and they named it EfficientNet. Particularly, the baseline network is termed Efficient-B0.\nNext, the authors scaled this baseline network using Compound Scaling technique as explained in this section to scale depth(d), width(w) and resolution(r) to get Efficient B1-B7. This process has also been summarized in the image below.\n\nI hope through this post I have been able to explain all the magic behind EfficientNets.\nAs always, constructive feedback is always welcome at @amaarora.\nAlso, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading!"
  },
  {
    "objectID": "nbs/Training.html",
    "href": "nbs/Training.html",
    "title": "",
    "section": "",
    "text": "BLOG POST: Top 100 solution - SIIM-ACR Pneumothorax Segmentation\nBefore running this notebook, run the following script:\nexport OUTPUT_DIR='../data/dataset512'\nmkdir siim-acr && cd siim-acr \nmkdir data && cd data\nkaggle datasets download -d seesee/siim-train-test\nunzip siim-train-test.zip \nmv siim/* . \nrmdir siim\nmkdir ../src/ && cd ../src\ngit clone https://github.com/sneddy/pneumothorax-segmentation\npython pneumothorax-segmentation/unet_pipeline/utils/prepare_png.py -train_path ../data/dicom-images-train/ -test_path ../data/dicom-images-test/ -out_path $OUTPUT_DIR -img_size 512 -rle_path ../data/train-rle.csv\nWe first download the dataset from Kaggle and then convert all the .dcm files to .png files using prepare_png.py script by Anuar Aimoldin - winner of SIIM-ACR competition.\nRunning the above script should create folder structure that looks like:\n├── data\n│   ├── dataset512\n│   ├── dicom-images-test\n│   └── dicom-images-train\n└── src\n    └── pneumothorax-segmentation \nAll .dcm training and test files are in dicom-images-train and dicom-images-test folders and the converted .png images are in the dataset512 folder.\n\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport segmentation_models_pytorch as smp\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom collections import defaultdict\nimport torchvision\nimport albumentations as albu\nfrom albumentations.pytorch.transforms import ToTensor\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data.sampler import Sampler\nimport sys; sys.path.append('../pneumothorax-segmentation/unet_pipeline/')\nfrom Losses import ComboLoss, dice_metric\n\n\n#import tqdm\ntry:\n    get_ipython().__class_._name__\n    from tqdm.notebook import tqdm\nexcept:\n    from tqdm import tqdm\n\n\n\nThe ComboLoss function used in CRITERION below also comes from the winning solution by Anuar.\n\nIMG_SIZE         = 512\nDATA_DIR         = Path(f'../data/dataset{IMG_SIZE}/')\nTRAIN_IMG_DIR    = DATA_DIR/'train'\nTRAIN_LBL_DIR    = DATA_DIR/'mask'\nRLE_DF           = pd.read_csv('../data/train-rle.csv', names=['ImageId', 'EncodedPixels'], skiprows=1)\nKFOLD_PATH       = '../data/RLE_kfold.csv'\nTRAIN_BATCH_SIZE = 14\nVALID_BATCH_SIZE = 14\nUSE_SAMPLER      = False\nPOSTIVE_PERC     = 0.8\nENCODER          = 'se_resnext50_32x4d'\nENCODER_WEIGHTS  = 'imagenet'\nCLASSES          = ['mask']\nACTIVATION       = None \nDEVICE           = 'cuda'\nPRETRAINED_PATH  = '../data/bst_model512_fold2_0.9565.bin'\nPRETRAINED       = True\nLEARNING_RATE    = 2e-5\nEPOCHS           = 5\nLOSS_FN          = 'mixed'\nCRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\nUSE_CRIT         = True\nTRAIN_MODEL      = False\nEVALUATE         = False\nFOLD_ID          = 4\nSUBMIT_TO_KAGGLE = False\n\nAll pretrained models have been provided in a Kaggle dataset here.\n\n\n\nGeneral utility functions provided by Kaggle to perform perform run length encoding and decoding.\n\ndef run_length_decode(rle, height=1024, width=1024, fill_value=1):\n    component = np.zeros((height, width), np.float32)\n    component = component.reshape(-1)\n    rle = np.array([int(s) for s in rle.strip().split(' ')])\n    rle = rle.reshape(-1, 2)\n    start = 0\n    for index, length in rle:\n        start = start+index\n        end = start+length\n        component[start: end] = fill_value\n        start = end\n    component = component.reshape(width, height).T\n    return component\n\n\ndef run_length_encode(component):\n    component = component.T.flatten()\n    start = np.where(component[1:] > component[:-1])[0]+1\n    end = np.where(component[:-1] > component[1:])[0]+1\n    length = end-start\n    rle = []\n    for i in range(len(length)):\n        if i == 0:\n            rle.extend([start[0], length[0]])\n        else:\n            rle.extend([start[i]-end[i-1], length[i]])\n    rle = ' '.join([str(r) for r in rle])\n    return rle\n\n\ndef matplotlib_imshow(img, one_channel=False):\n    fig,ax = plt.subplots(figsize=(10,6))\n    ax.imshow(img.permute(1,2,0).numpy())\n\n\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy \n    n = len(images)\n    plt.figure(figsize=(16, 8))\n    image, mask = images['image'], images['mask']\n    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n    if mask.max()>0:\n        plt.imshow(mask.squeeze(0), alpha=0.25)\n    plt.show()\n\n\n\n\n\n\n\n# create 5 folds train file if it doesn't exist already\nif not os.path.exists(KFOLD_PATH):\n    RLE_DF['has_mask'] = 0\n    RLE_DF.loc[RLE_DF.EncodedPixels!='-1', 'has_mask'] = 1\n    kf = StratifiedKFold()\n    RLE_DF['kfold']=-1\n    for fold, (train_index, test_index) in enumerate(kf.split(X=RLE_DF.ImageId, y=RLE_DF.has_mask)):\n            RLE_DF.loc[test_index, 'kfold'] = fold\n    RLE_DF.to_csv('../data/RLE_kfold.csv', index=False)\n\n\n# single fold training for now, rerun notebook to train for multi-fold\nDF       = pd.read_csv(KFOLD_PATH)\nTRAIN_DF = DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\nVAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\nlen(TRAIN_DF), len(VAL_DF)\n\n(10364, 2590)\n\n\n\n\n\n\nclass Dataset():\n    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None):\n        self.df             = rle_df\n        self.image_base_dir = image_base_dir\n        self.masks_base_dir = masks_base_dir\n        self.image_ids      = rle_df.ImageId.values\n        self.augmentation   = augmentation\n    \n    def __getitem__(self, i):\n        image_id  = self.image_ids[i]\n        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n        mask_path = os.path.join(self.masks_base_dir, image_id+'.png')\n        image     = cv2.imread(img_path, 1)\n        mask      = cv2.imread(mask_path, 0)     \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = {\"image\": image, \"mask\": mask}\n            sample = self.augmentation(**sample)\n            image, mask = sample['image'], sample['mask']\n\n        return {\n            'image': image, \n            'mask' : mask\n        }\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n\n# Train transforms\nTFMS = albu.Compose([\n    albu.HorizontalFlip(),\n    albu.Rotate(10),\n    albu.Normalize(),\n    ToTensor(),\n])\n\n# Test transforms\nTEST_TFMS = albu.Compose([\n    albu.Normalize(),\n    ToTensor(),\n])\n\n\n# train dataset\ntrain_dataset = Dataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS) \nval_dataset   = Dataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS) \n\n\n# sanity check\nimage, mask = train_dataset[1237]['image'], train_dataset[1237]['mask']\nimage.shape, mask.shape\n\n(torch.Size([3, 512, 512]), torch.Size([1, 512, 512]))\n\n\n\n# plot one with mask \nvisualize(**train_dataset[1])\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\nA sampler similar to the winning solution used by Anuar Aimoldin to help with easy convergence. (not used as part of this solution, but provided for the reader to experiment)\n\nclass PneumoSampler(Sampler):\n    def __init__(self, train_df, positive_perc=0.8):\n        assert positive_perc > 0, 'percentage of positive pneumothorax images must be greater then zero'\n        self.train_df = train_df\n        self.positive_perc = positive_perc\n        self.positive_idxs = self.train_df.query('has_mask==1').index.values\n        self.negative_idxs = self.train_df.query('has_mask!=1').index.values\n        self.n_positive = len(self.positive_idxs)\n        self.n_negative = int(self.n_positive * (1 - self.positive_perc) / self.positive_perc)\n        \n    def __iter__(self):\n        negative_sample = np.random.choice(self.negative_idxs, size=self.n_negative)\n        shuffled = np.random.permutation(np.hstack((negative_sample, self.positive_idxs)))\n        return iter(shuffled.tolist())\n\n    def __len__(self):\n        return self.n_positive + self.n_negative\n\n\nSAMPLER = PneumoSampler(TRAIN_DF, positive_perc=POSTIVE_PERC)\n\n\n\n\n\n# dataloaders\ntrain_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \n                              shuffle=True if not USE_SAMPLER else False, \n                              num_workers=4, \n                              sampler=SAMPLER if USE_SAMPLER else None)\nval_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=4)\n\n\n# sanity check\nimages, masks = next(iter(train_dataloader))['image'], next(iter(train_dataloader))['mask']\nimages.shape, masks.shape\n\n(torch.Size([14, 3, 512, 512]), torch.Size([14, 1, 512, 512]))\n\n\n\n# plot train images\nimg_grid = torchvision.utils.make_grid(images[:9], nrow=3, normalize=True)\nmatplotlib_imshow(img_grid)\n\n\n\n\n\n\n\n\n\n\nIn this script we use ComboLoss from winning solution but it is also possible to use MixedLoss shared below if USE_CRIT is set to False in CONFIG.\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\ndef dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\nclass MixedLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n\n    def forward(self, input, target):\n        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n        return loss.mean()\n\n\n\n\nFrom PyTorch Segmentation Models:\n\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\n\n\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(\n                \"EarlyStopping counter: {} out of {}\".format(\n                    self.counter, self.patience\n                )\n            )\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        model_path = Path(model_path)\n        parent = model_path.parent\n        os.makedirs(parent, exist_ok=True)\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print(\n                \"Validation score improved ({} --> {}). Model saved at at {}!\".format(\n                    self.val_score, epoch_score, model_path\n                )\n            )\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\n\n\n\nclass AverageMeter:\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=1, device='cuda'):\n    losses = AverageMeter()\n    model = model.to(device)\n    model.train()\n    if accumulation_steps > 1: \n        optimizer.zero_grad()\n    tk0 = tqdm(train_loader, total=len(train_loader))\n    for b_idx, data in enumerate(tk0):\n        for key, value in data.items():\n            data[key] = value.to(device)\n        if accumulation_steps == 1 and b_idx == 0:\n            optimizer.zero_grad()\n        out  = model(data['image'])\n        loss = loss_fn(out, data['mask'])\n        with torch.set_grad_enabled(True):\n            loss.backward()\n            if (b_idx + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n        losses.update(loss.item(), train_loader.batch_size)\n        tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n    return losses.avg\n\n\n\n\n\n# pixel-wise accuracy\ndef acc_metric(input, target):\n    inp = torch.where(input>0.5, torch.tensor(1, device='cuda'), torch.tensor(0, device='cuda'))\n    acc = (inp.squeeze(1) == target).float().mean()\n    return acc\n\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\ndef metric(probability, truth, threshold=0.5, reduction='none'):\n    batch_size = len(truth)\n    with torch.no_grad():\n        probability = probability.view(batch_size, -1)\n        truth = truth.view(batch_size, -1)\n        assert(probability.shape == truth.shape)\n\n        p = (probability > threshold).float()\n        t = (truth > 0.5).float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n        neg_index = torch.nonzero(t_sum == 0)\n        pos_index = torch.nonzero(t_sum >= 1)\n\n        dice_neg = (p_sum == 0).float()\n        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n\n        dice_neg = dice_neg[neg_index]\n        dice_pos = dice_pos[pos_index]\n        dice = torch.cat([dice_pos, dice_neg])\n\n        num_neg = len(neg_index)\n        num_pos = len(pos_index)\n\n    return dice\n\n\ndef evaluate(valid_loader, model, device='cuda', metric=dice_metric):\n    losses = AverageMeter()\n    model = model.to(device)\n    model.eval()\n    tk0 = tqdm(valid_loader, total=len(valid_loader))\n    with torch.no_grad():\n        for b_idx, data in enumerate(tk0):\n            for key, value in data.items():\n                data[key] = value.to(device)\n            out   = model(data['image'])\n            out   = torch.sigmoid(out)\n            dice  = metric(out, data['mask']).cpu()\n            losses.update(dice.mean().item(), valid_loader.batch_size)\n            tk0.set_postfix(dice_score=losses.avg)\n    return losses.avg\n\n\nif PRETRAINED: \n    model.load_state_dict(torch.load(PRETRAINED_PATH))\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)\n\n\n\n\n\ncriterion = MixedLoss(10.0, 2.0) if not USE_CRIT else CRITERION \nes = EarlyStopping(patience=10, mode='max')\n\n\ncriterion\n\nComboLoss(\n  (bce): StableBCELoss()\n  (dice): DiceLoss()\n  (jaccard): JaccardLoss()\n  (lovasz): LovaszLoss()\n  (lovasz_sigmoid): LovaszLossSigmoid()\n  (focal): FocalLoss2d()\n)\n\n\n\nif TRAIN_MODEL:\n    for epoch in range(EPOCHS):\n        loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n        dice = evaluate(val_dataloader, model, metric=metric)\n        scheduler.step()\n        print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL DICE: {dice}\")\n        es(dice, model, model_path=f\"../data/bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(dice,4)}.bin\")\n        best_model = f\"../data/bst_model{IMG_SIZE}__fold{FOLD_ID}_{np.round(es.best_score,4)}.bin\"\n        if es.early_stop:\n            print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n            break\nif EVALUATE:\n    valid_score = evaluate(val_dataloader, model, metric=metric)\n    print(f\"Valid dice score: {valid_score}\")\n\n\n\n\n\n\nclass TestDataset():\n    def __init__(self, sample_sub, image_base_dir, augmentation=None):\n        self.image_base_dir = image_base_dir\n        self.image_ids      = sample_sub.ImageId.values\n        self.augmentation   = augmentation\n    \n    def __getitem__(self, i):\n        image_id  = self.image_ids[i]\n        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n        image     = cv2.imread(img_path)\n        image     = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image)\n            image  = sample['image']\n\n        return {\n            'image': image, \n        }\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n\nSAMPLE_SUBMISSION = pd.read_csv('/home/ubuntu/repos/kaggle/acr_Pneumothorax/data/stage-2/stage_2_sample_submission.csv')\nTEST_IMG_DIR = DATA_DIR/'test'\n\nlen(SAMPLE_SUBMISSION)\n\n3205\n\n\n\ntest_dataset    = TestDataset(SAMPLE_SUBMISSION, TEST_IMG_DIR, TEST_TFMS)\ntest_dataloader = DataLoader(test_dataset, 16, shuffle=False, num_workers=6)\n\n\nmodel.load_state_dict(torch.load('<path_to_bst_model>'))\n\n<All keys matched successfully>\n\n\n\nmodel = model.to('cuda')\n\n\ndef post_process(probability, threshold, min_size):\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((1024, 1024), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num\n\n\ndef predict(test_dataloader, model, device='cuda'):\n    encoded_pixels = []\n    losses = AverageMeter()\n    model = model.to(device)\n    model.eval()\n    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n    for b_idx, data in enumerate(tk0):\n        data['image'] = data['image'].to(device)\n        out   = model(data['image'])\n        out   = out.detach().cpu().numpy()[:, 0, :, :]\n        for out_ in out:\n            import pdb;pdb.set_trace()\n            if out_.shape != (1024, 1024):\n                out_ = cv2.resize(out_, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n            predict, num_predict = post_process(out_, 0.5, 3500)\n            if num_predict == 0:\n                encoded_pixels.append('-1')\n            else:\n                r = run_length_encode(predict)\n                encoded_pixels.append(r)\n    return encoded_pixels\n\n\nif SUBMIT_TO_KAGGLE:    \n    enc_pixels = predict(test_dataloader, model)\n\n    SAMPLE_SUBMISSION['EncodedPixels'] = enc_pixels\n    SAMPLE_SUBMISSION.to_csv('./submission.csv', index=False)\n\n    !kaggle competitions submit -c siim-acr-pneumothorax-segmentation -f submission.csv -m \"512x512 fold-2\"\n\n\n\n\n\nclass TestDataset():\n    def __init__(self, sample_sub, image_base_dir, augmentation=None):\n        self.image_base_dir = image_base_dir\n        self.image_ids      = sample_sub.ImageId.values\n        self.augmentation   = augmentation\n    \n    def __getitem__(self, i):\n        image_id  = self.image_ids[i]\n        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n        image     = cv2.imread(img_path)\n        image     = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image)\n            image  = sample['image']\n\n        return {\n            'image_id': image_id,\n            'image': image\n        }\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n\ntest_dataset    = TestDataset(SAMPLE_SUBMISSION, TEST_IMG_DIR, TEST_TFMS)\ntest_dataloader = DataLoader(test_dataset, 16, shuffle=False, num_workers=6)\n\n\ndef inference_image(model, images, device='cuda'):\n    images = images.to(device)\n    predicted = model(images)\n    masks = torch.sigmoid(predicted)\n    masks = masks.squeeze(1).cpu().detach().numpy()\n    return masks\n\n\ndef inference_model(model, loader, device, use_flip=False):\n    mask_dict = {}\n    for data in tqdm(loader):\n        image_ids = data['image_id']\n        images    = data['image']\n        masks = inference_image(model, images, device)\n        for name, mask in zip(image_ids, masks):\n            mask_dict[name] = mask.astype(np.float32)\n    return mask_dict\n\n\n# Path to pretrained models from https://www.kaggle.com/aroraaman/siimacr-pretrained/\ncheckpoints_list=['../data/bst_model512_fold1_0.9492.bin', \n                  '../data/bst_model512_fold2_0.9565.bin', \n                  '../data/bst_model512_fold3_0.9608.bin', \n                  '../data/bst_model512_fold4_0.9604.bin']\n\n\nmask_dict = defaultdict(int)\nfor pred_idx, checkpoint_path in enumerate(checkpoints_list):\n    model.load_state_dict(torch.load(checkpoint_path))\n    model.eval()\n    current_mask_dict = inference_model(model, test_dataloader, device='cuda')\n    for name, mask in current_mask_dict.items():\n        mask_dict[name] = (mask_dict[name] * pred_idx + mask) / (pred_idx + 1)\n\n100%|██████████| 201/201 [00:37<00:00,  5.35it/s]\n100%|██████████| 201/201 [00:37<00:00,  5.33it/s]\n100%|██████████| 201/201 [00:38<00:00,  5.20it/s]\n100%|██████████| 201/201 [00:38<00:00,  5.18it/s]\n\n\n\nlen(mask_dict)\n\n3205\n\n\n\nencoded_pixels = []\nfor image_name, out_ in tqdm(mask_dict.items()):\n    if out_.shape != (1024, 1024):\n        out_ = cv2.resize(out_, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n    predict, num_predict = post_process(out_, 0.275, 3500)\n    if num_predict == 0:\n        encoded_pixels.append('-1')\n    else:\n        r = run_length_encode(predict)\n        encoded_pixels.append(r)\n\n100%|██████████| 3205/3205 [00:09<00:00, 344.91it/s]\n\n\n\nSAMPLE_SUBMISSION['EncodedPixels'] = encoded_pixels\nSAMPLE_SUBMISSION.to_csv('./submission.csv', index=False)\n\n!kaggle competitions submit -c siim-acr-pneumothorax-segmentation -f submission.csv -m \"512x512 0-4 fold\"\n\n100%|█████████████████████████████████████████| 908k/908k [00:07<00:00, 119kB/s]\nSuccessfully submitted to SIIM-ACR Pneumothorax Segmentation\n\n\n\n\n\n\n../data/bst_model512_0.8193.bin (fold-0)\n../data/bst_model512_fold1_0.9492.bin (fold-1)\n../data/bst_model512_fold2_0.9565.bin(fold-2)\n../data/bst_model512_fold3_0.9608.bin (fold-3)\nsubscribe.html"
  },
  {
    "objectID": "nbs/Understanding Metric Learning.html",
    "href": "nbs/Understanding Metric Learning.html",
    "title": "",
    "section": "",
    "text": "In this notebook, we will specifically be looking at Metric Learning. A complete blog post is available here.\nWith metric learning, the main idea is to increase the inter class discriminations and the intra class compactness.\n\nfrom pathlib import Path\nimport math\nimport pandas as pd \nimport numpy as np \nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport torch\nimport albumentations\nfrom torchvision import models\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport seaborn as sns\nimport torch.nn.functional as F\nfrom torchvision.models.utils import load_state_dict_from_url\n\n\nDATA_PATH = Path('/home/ubuntu/.fastai/data/mnist_png/')\nEPOCHS    = 5\n\n\n\n\nimg_paths = list(DATA_PATH.rglob('*.png'))\nlen(img_paths), img_paths[:2]\n\n(70000,\n [PosixPath('/home/ubuntu/.fastai/data/mnist_png/training/9/48159.png'),\n  PosixPath('/home/ubuntu/.fastai/data/mnist_png/training/9/1695.png')])\n\n\n\ndef extract_label(o):\n     return o.parent.stem\n\n\ndf = pd.DataFrame({'img_paths': img_paths})\ndf['labels'] = df.img_paths.map(extract_label)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      img_paths\n      labels\n    \n  \n  \n    \n      0\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n    \n    \n      1\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n    \n    \n      2\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n    \n    \n      3\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n    \n    \n      4\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n    \n  \n\n\n\n\n\ndef is_val(o):\n    return 'training' not in str(o)\n\n\ndf['is_val'] = -1\ndf['is_val'] = df.img_paths.map(is_val)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      img_paths\n      labels\n      is_val\n    \n  \n  \n    \n      0\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n      False\n    \n    \n      1\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n      False\n    \n    \n      2\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n      False\n    \n    \n      3\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n      False\n    \n    \n      4\n      /home/ubuntu/.fastai/data/mnist_png/training/9...\n      9\n      False\n    \n  \n\n\n\n\n\ntrn = df.query(\"is_val==False\")\nval = df.query(\"is_val==True\")\n\nlen(trn), len(val)\n\n(60000, 10000)\n\n\n\n\n\n\nclass MNIST_Dataset():\n    def __init__(self, df, augmentations=None):\n        self.df            = df\n        self.img_paths     = df.img_paths.values\n        self.labels        = df.labels.values\n        self.augmentations = augmentations\n        \n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        label    = self.labels[idx]        \n        \n        img = Image.open(img_path)\n        img = img.convert('RGB')\n        img = np.array(img)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=img)\n            img       = augmented['image']\n        \n        img = np.transpose(img, (2, 0, 1)).astype(np.float32)    \n        \n        return {\n            'image': torch.tensor(img, dtype=torch.float), \n            'label': torch.tensor(int(label), dtype=torch.long)\n        }\n    \n    def __len__(self): \n        return len(self.df)\n\n\ntrain_aug = albumentations.Compose([\n    albumentations.Normalize()\n])\n\nval_aug = albumentations.Compose([\n    albumentations.Normalize()\n\n])\n\n\ntrain_dataset = MNIST_Dataset(trn, augmentations=train_aug)\nval_dataset   = MNIST_Dataset(val, augmentations=val_aug)\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_dataloader   = torch.utils.data.DataLoader(val_dataset, batch_size=64, num_workers=4, shuffle=False)\n\n\nnext(iter(train_dataloader))['image'].shape\n\ntorch.Size([64, 3, 28, 28])\n\n\n\n\n\n\nclass Resnet(nn.Module):\n    def __init__(self, pretrained=True, arch_name='resnet34', n_out=10):\n        super(Resnet, self).__init__()\n        self.pretrained = pretrained\n        self.base_model = getattr(models, arch_name)()\n        nftrs           = self.base_model.fc.in_features\n        self.base_model.fc = nn.Linear(nftrs, 2)\n        self.fc         = nn.Linear(2, n_out)\n\n\n    def forward(self, image, **kwargs):\n        o1 = self.base_model(image)  \n        o2 = self.fc(o1)\n        return o1, o2\n\n\n# project logits to 2 dim\nmodel = Resnet()\nmodel = model.to('cuda')\n\n\n\n\n\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef train_one_epoch(train_loader, model, optimizer, loss_fn=nn.CrossEntropyLoss(), device='cuda'):\n    losses = AverageMeter()\n    model.train()\n    tk0 = tqdm(train_loader, total=len(train_loader))\n    for b_idx, data in enumerate(tk0):\n        data['image']  = data['image'].to(device)\n        data['label']  = data['label'].to(device)\n        \n        optimizer.zero_grad()\n        o1, o2 = model(**data)\n        \n        loss = loss_fn(o2, data['label'])\n\n        with torch.set_grad_enabled(True):\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        losses.update(loss.item(), train_loader.batch_size)\n        tk0.set_postfix(loss=losses.avg)\n    return losses.avg\n\n\nparams = list(model.parameters()) #+list(model.base_model.parameters()) + \noptimizer = torch.optim.Adam(params, lr=1e-4)\n\n\ndef evaluate(valid_loader, model, device='cuda'):\n    losses = AverageMeter()\n    final_preds = []\n    model.eval()\n    preds_  = torch.zeros(1,2)\n    labels_ = np.array([])\n    with torch.no_grad():\n        tk0 = tqdm(valid_loader, total=len(valid_loader))\n        for data in tk0:\n            for key, value in data.items():\n                data[key] = value.to(device)\n            o1, o2 = model(**data)\n            preds_ = torch.cat([preds_, o1.cpu()])\n            labels_ = np.append(labels_, data['label'].cpu().numpy())\n            loss   = nn.CrossEntropyLoss()(o2, data['label'])\n            preds  = o2.argmax(1)\n            acc    = (preds == data['label']).float().mean()\n            losses.update(acc.item(), valid_loader.batch_size) \n            preds = preds.cpu().numpy()\n            final_preds.extend(preds)\n            tk0.set_postfix(loss=losses.avg)\n    return losses.avg, preds_[1:], labels_\n\n\n\n\n# TRAINING using Tesla T4\nfor i in range(5):\n    train_one_epoch(train_dataloader, model, optimizer)\n\n100%|██████████| 938/938 [01:02<00:00, 14.99it/s, loss=0.862]\n100%|██████████| 938/938 [01:02<00:00, 14.94it/s, loss=0.368]\n100%|██████████| 938/938 [01:03<00:00, 14.83it/s, loss=0.179]\n100%|██████████| 938/938 [01:03<00:00, 14.79it/s, loss=0.134]\n100%|██████████| 938/938 [01:03<00:00, 14.75it/s, loss=0.104] \n\n\n\nloss, preds, labels = evaluate(val_dataloader, model)\n\n100%|██████████| 157/157 [00:02<00:00, 62.75it/s, loss=0.983]\n\n\n\npreds = pd.DataFrame(preds.numpy())\npreds['label'] = labels\npreds.head(2)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      label\n    \n  \n  \n    \n      0\n      38.511963\n      16.713697\n      9.0\n    \n    \n      1\n      64.323685\n      28.391975\n      9.0\n    \n  \n\n\n\n\n\nsns.scatterplot(data=preds, x=0, y=1, hue='label', palette='Set2', s=10);\nplt.title(\"MNIST: Test set distribution\");\n\n\n\n\n\n\n\n\n# credits: https://github.com/KaiyangZhou/pytorch-center-loss/blob/master/center_loss.py\n\nclass CenterLoss(nn.Module):\n    \"\"\"Center loss.\n    \n    Reference:\n    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n    \n    Args:\n        num_classes (int): number of classes.\n        feat_dim (int): feature dimension.\n    \"\"\"\n    def __init__(self, num_classes=10, feat_dim=2, use_gpu=True):\n        super(CenterLoss, self).__init__()\n        self.num_classes = num_classes\n        self.feat_dim = feat_dim\n        self.use_gpu = use_gpu\n\n        if self.use_gpu:\n            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n        else:\n            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n\n    def forward(self, x, labels):\n        \"\"\"\n        Args:\n            x: feature matrix with shape (batch_size, feat_dim).\n            labels: ground truth labels with shape (batch_size).\n        \"\"\"\n        batch_size = x.size(0)\n        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n        distmat.addmm_(1, -2, x, self.centers.t())\n\n        classes = torch.arange(self.num_classes).long()\n        if self.use_gpu: classes = classes.cuda()\n        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n\n        dist = distmat * mask.float()\n        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n\n        return loss\n\n\n# TRAINING using Tesla T4\nfor i in range(3):\n    train_one_epoch(train_dataloader, model, optimizer, loss_fn=CenterLoss(10,10,True))\n\n100%|██████████| 938/938 [01:02<00:00, 14.89it/s, loss=9.15]\n100%|██████████| 938/938 [01:03<00:00, 14.82it/s, loss=8.34]\n100%|██████████| 938/938 [01:03<00:00, 14.67it/s, loss=8.35]\n\n\n\nloss, preds, labels = evaluate(val_dataloader, model)\n\n100%|██████████| 157/157 [00:02<00:00, 63.02it/s, loss=0.103]  \n\n\n\npreds = pd.DataFrame(preds.numpy())\npreds['label'] = labels\npreds.head(2)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      label\n    \n  \n  \n    \n      0\n      0.203384\n      -0.598501\n      9.0\n    \n    \n      1\n      0.238211\n      -0.528210\n      9.0\n    \n  \n\n\n\n\n\nsns.scatterplot(data=preds, x=0, y=1, hue='label', palette='Set2', s=10);\nplt.title(\"MNIST: Test set distribution\");\nsubscribe.html"
  },
  {
    "objectID": "nbs/Group Normalization.html",
    "href": "nbs/Group Normalization.html",
    "title": "",
    "section": "",
    "text": "from fastai2.vision.all import *\nfrom nbdev.showdoc import *\nimport glob\nimport albumentations\nfrom torchvision import models\nfrom albumentations.pytorch.transforms import ToTensorV2\nset_seed(2)\nsubscribe.html"
  },
  {
    "objectID": "nbs/Group Normalization.html#dataset",
    "href": "nbs/Group Normalization.html#dataset",
    "title": "",
    "section": "Dataset",
    "text": "Dataset\n\nclass PetsDataset:\n    def __init__(self, paths, transforms=None):\n        self.image_paths = paths\n        self.transforms = transforms\n        \n    def __len__(self): \n        return len(self.image_paths)\n    \n    def setup(self, pat=r'(.+)_\\d+.jpg$', label2int=None):\n        \"adds a label dictionary to `self`\"\n        self.pat = re.compile(pat)\n        if label2int is not None:\n            self.label2int = label2int\n            self.int2label = {v:i for i,v in self.label2int.items()}\n        else:\n            labels = [os.path.basename(self.pat.search(str(p)).group(1))\n                  for p in self.image_paths]\n            self.labels = set(labels)\n            self.label2int = {label:i for i,label in enumerate(self.labels)}\n            self.int2label = {v:i for i,v in self.label2int.items()}\n\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path)\n        img = np.array(img)\n        \n        target = os.path.basename(self.pat.search(str(img_path)).group(1))\n        target = self.label2int[target]\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']      \n            \n        return img, torch.tensor(target, dtype=torch.long)\n\n\nimage_paths = get_image_files(path/'images')\nimage_paths\n\n(#7378) [Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg')...]\n\n\n\nfrom tqdm.notebook import tqdm\nrun_remove = False\ndef remove(o):\n    img = Image.open(o)\n    img = np.array(img)\n    if img.shape[2] != 3:\n        os.remove(o)\nif run_remove:\n    for o in tqdm(image_paths): remove(o)\n\n\nimage_paths = get_image_files(path/'images')\nimage_paths\n\n(#7378) [Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg')...]\n\n\n\nsz = 224\ntfms = albumentations.Compose([\n    albumentations.Resize(sz, sz) if sz else albumentations.NoOp(),\n    albumentations.OneOf(\n        [albumentations.Cutout(random.randint(1,8), 16, 16),\n         albumentations.CoarseDropout(random.randint(1,8), 16, 16)]\n    ),\n    albumentations.Normalize(always_apply=True),\n    ToTensorV2()\n])\n\n\ndataset = PetsDataset(image_paths, tfms)\n\n\ndataset.setup()\n\n\ndataset[0]\n\n(tensor([[[ 0.8618,  0.1597,  0.4166,  ..., -0.6452, -0.3198, -0.2171],\n          [ 1.1872,  0.3481,  0.4166,  ..., -0.3027,  0.0912,  0.3138],\n          [ 0.8104,  0.6049,  0.0227,  ..., -0.3712, -0.1657, -0.1828],\n          ...,\n          [ 1.2385,  0.4851,  0.0227,  ...,  0.8789,  1.2214,  0.8961],\n          [ 0.7077,  0.9474, -0.6965,  ...,  0.1254,  1.5297,  1.6667],\n          [ 0.1083, -0.0801,  0.3652,  ...,  0.2111,  0.5193,  0.6734]],\n \n         [[ 0.9230,  0.4328,  0.4503,  ..., -0.2850, -0.0224, -0.0399],\n          [ 1.3256,  0.7304,  0.4678,  ..., -0.0399,  0.1527,  0.3277],\n          [ 0.8354,  0.8354,  0.3102,  ..., -0.2500, -0.1975, -0.3200],\n          ...,\n          [ 1.3606,  1.3431,  0.6078,  ...,  0.9755,  1.3957,  1.1331],\n          [ 0.7654,  1.0455, -0.0574,  ...,  0.7654,  1.6232,  1.7458],\n          [ 0.4153,  0.5903,  0.9230,  ...,  0.7654,  0.8529,  1.0980]],\n \n         [[ 0.3393, -0.3578, -0.4275,  ..., -0.7936, -0.4624, -0.3578],\n          [ 0.6531, -0.2358, -0.4973,  ..., -0.3753, -0.0615,  0.1128],\n          [ 0.0431,  0.1128, -1.0201,  ..., -0.4101, -0.2707, -0.3578],\n          ...,\n          [ 0.7228,  0.3219, -0.5321,  ...,  0.4439,  1.0017,  0.7576],\n          [ 0.2173,  0.4265, -1.1247,  ..., -0.0790,  1.1411,  1.2457],\n          [-0.4450, -0.2881,  0.1302,  ...,  0.0082,  0.2696,  0.4439]]]),\n tensor(36))\n\n\n\ndataset[0][0].shape\n\ntorch.Size([3, 224, 224])"
  },
  {
    "objectID": "nbs/Group Normalization.html#dataloaders",
    "href": "nbs/Group Normalization.html#dataloaders",
    "title": "",
    "section": "DataLoaders",
    "text": "DataLoaders\n\nnval = int(len(image_paths)*0.2)\nnval\n\n1475\n\n\n\ntrn_img_paths = image_paths[:-nval]\nval_img_paths = image_paths[-nval:]\nassert len(trn_img_paths) + len(val_img_paths) == len(image_paths)\nlen(trn_img_paths), len(val_img_paths)\n\n(5903, 1475)\n\n\n\ntrn_dataset = PetsDataset(trn_img_paths, transforms=tfms)\nval_dataset = PetsDataset(val_img_paths, transforms=tfms)\n\n\ntrn_dataset.setup(label2int=dataset.label2int)\nval_dataset.setup(label2int=dataset.label2int)\n\n\n# bs = 64\n\n\ntrn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=bs, num_workers=4, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs, num_workers=4, shuffle=False)\n\n\nnext(iter(trn_loader))[0].shape, next(iter(val_loader))[0].shape\n\n(torch.Size([4, 3, 224, 224]), torch.Size([4, 3, 224, 224]))"
  },
  {
    "objectID": "nbs/Group Normalization.html#model",
    "href": "nbs/Group Normalization.html#model",
    "title": "",
    "section": "Model",
    "text": "Model\n\nresnet34_bn = models.resnet34(num_classes=len(trn_dataset.label2int))\nresnet34_bn\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=37, bias=True)\n)\n\n\n\nclass GroupNorm_32(torch.nn.GroupNorm):\n    def __init__(self, num_channels, num_groups=32, **kwargs):\n        super().__init__(num_groups, num_channels, **kwargs)\n\n\nresnet34_gn = models.resnet34(norm_layer=GroupNorm_32, num_classes=len(trn_dataset.label2int))\nresnet34_gn\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=37, bias=True)\n)\n\n\n\nresnet34_gn(next(iter(trn_loader))[0]).shape\n\ntorch.Size([4, 37])"
  },
  {
    "objectID": "nbs/Group Normalization.html#training-using-pytorchlightning",
    "href": "nbs/Group Normalization.html#training-using-pytorchlightning",
    "title": "",
    "section": "Training using PytorchLightning",
    "text": "Training using PytorchLightning\n\nfrom pytorch_lightning import LightningModule, Trainer\n\n\nclass Model(LightningModule):\n    def __init__(self, base):\n        super().__init__()\n        self.base = base\n\n    def forward(self, x):\n        return self.base(x)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\n    def step(self, batch):\n        x, y  = batch\n        y_hat = self(x)\n        loss  = nn.CrossEntropyLoss()(y_hat, y)\n        return loss, y, y_hat\n\n    def training_step(self, batch, batch_nb):\n        loss, _, _ = self.step(batch)\n        return {'loss': loss}\n\n    def validation_step(self, batch, batch_nb):\n        loss, y, y_hat = self.step(batch)\n        return {'loss': loss, 'y': y.detach(), 'y_hat': y_hat.detach()}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        acc = self.get_accuracy(outputs)\n        print(f\"Epoch:{self.current_epoch} | Loss:{avg_loss} | Accuracy:{acc}\")\n        return {'loss': avg_loss}\n    \n    def get_accuracy(self, outputs):\n        from sklearn.metrics import accuracy_score\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        preds = y_hat.argmax(1)\n        return accuracy_score(y.cpu().numpy(), preds.cpu().numpy())\n\n\nmodel_bn = Model(resnet34_bn)\nmodel_gn = Model(resnet34_gn)\n\n\ndebug = False\ngpus = torch.cuda.device_count()\ntrainer = Trainer(gpus=gpus, max_epochs=50, \n                  num_sanity_val_steps=1 if debug else 0)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_bn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:3.8165385723114014 | Accuracy:0.05694915254237288\n\n\n\n\n\nEpoch:1 | Loss:3.4834210872650146 | Accuracy:0.07186440677966102\n\n\n\n\n\nEpoch:2 | Loss:3.6691737174987793 | Accuracy:0.07254237288135593\n\n\n\n\n\nEpoch:3 | Loss:3.4139177799224854 | Accuracy:0.08\n\n\n\n\n\nEpoch:4 | Loss:3.2902932167053223 | Accuracy:0.09627118644067796\n\n\n\n\n\nEpoch:5 | Loss:3.308424711227417 | Accuracy:0.10847457627118644\n\n\n\n\n\nEpoch:6 | Loss:3.1262896060943604 | Accuracy:0.13559322033898305\n\n\n\n\n\nEpoch:7 | Loss:3.1067755222320557 | Accuracy:0.14576271186440679\n\n\n\n\n\nEpoch:8 | Loss:3.039379596710205 | Accuracy:0.14983050847457627\n\n\n\n\n\nEpoch:9 | Loss:2.8480300903320312 | Accuracy:0.18508474576271186\n\n\n\n\n\nEpoch:10 | Loss:2.803644895553589 | Accuracy:0.20813559322033898\n\n\n\n\n\nEpoch:11 | Loss:2.7595109939575195 | Accuracy:0.23457627118644067\n\n\n\n\n\nEpoch:12 | Loss:2.5479202270507812 | Accuracy:0.28271186440677964\n\n\n\n\n\nEpoch:13 | Loss:2.571686029434204 | Accuracy:0.2983050847457627\n\n\n\n\n\nEpoch:14 | Loss:2.500197649002075 | Accuracy:0.3077966101694915\n\n\n\n\n\nEpoch:15 | Loss:2.504734992980957 | Accuracy:0.3233898305084746\n\n\n\n\n\nEpoch:16 | Loss:2.4901981353759766 | Accuracy:0.3396610169491525\n\n\n\n\n\nEpoch:17 | Loss:2.4564080238342285 | Accuracy:0.36542372881355933\n\n\n\n\n\nEpoch:18 | Loss:2.6735761165618896 | Accuracy:0.34983050847457625\n\n\n\n\n\nEpoch:19 | Loss:2.7455430030822754 | Accuracy:0.36\n\n\n\n\n\nEpoch:20 | Loss:3.1568517684936523 | Accuracy:0.3376271186440678\n\n\n\n\n\nEpoch:21 | Loss:3.229506254196167 | Accuracy:0.3464406779661017\n\n\n\n\n\nEpoch:22 | Loss:3.116837501525879 | Accuracy:0.3559322033898305\n\n\n\n\n\nEpoch:23 | Loss:3.3821210861206055 | Accuracy:0.34983050847457625\n\n\n\n\n\nEpoch:24 | Loss:3.667248249053955 | Accuracy:0.328135593220339\n\n\n\n1\n\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_gn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:3.604952096939087 | Accuracy:0.031186440677966103\n\n\n\n\n\nEpoch:1 | Loss:3.51139760017395 | Accuracy:0.04745762711864407\n\n\n\n\n\nEpoch:2 | Loss:3.5002284049987793 | Accuracy:0.06305084745762712\n\n\n\n\n\nEpoch:3 | Loss:3.4050915241241455 | Accuracy:0.0711864406779661\n\n\n\n\n\nEpoch:4 | Loss:3.424589157104492 | Accuracy:0.07389830508474576\n\n\n\n\n\nEpoch:5 | Loss:3.395509958267212 | Accuracy:0.07254237288135593\n\n\n\n\n\nEpoch:6 | Loss:3.3599977493286133 | Accuracy:0.08542372881355932\n\n\n\n\n\nEpoch:7 | Loss:3.3448259830474854 | Accuracy:0.0759322033898305\n\n\n\n\n\nEpoch:8 | Loss:3.2987756729125977 | Accuracy:0.08542372881355932\n\n\n\n\n\nEpoch:9 | Loss:3.3236124515533447 | Accuracy:0.09423728813559322\n\n\n\n\n\nEpoch:10 | Loss:3.2912700176239014 | Accuracy:0.09830508474576272\n\n\n\n\n\nEpoch:11 | Loss:3.2386019229888916 | Accuracy:0.11322033898305085\n\n\n\n\n\nEpoch:12 | Loss:3.2127442359924316 | Accuracy:0.10847457627118644\n\n\n\n\n\nEpoch:13 | Loss:3.282404661178589 | Accuracy:0.10915254237288136\n\n\n\n\n\nEpoch:14 | Loss:3.2815961837768555 | Accuracy:0.12271186440677966\n\n\n\n\n\nEpoch:15 | Loss:3.3165860176086426 | Accuracy:0.1423728813559322\n\n\n\n\n\nEpoch:16 | Loss:3.4761595726013184 | Accuracy:0.13288135593220338\n\n\n\n\n\nEpoch:17 | Loss:3.692209482192993 | Accuracy:0.12406779661016949\n\n\n\n\n\nEpoch:18 | Loss:4.045893669128418 | Accuracy:0.13898305084745763\n\n\n\n\n\nEpoch:19 | Loss:4.537643909454346 | Accuracy:0.14508474576271185\n\n\n\n\n\nEpoch:20 | Loss:5.022345066070557 | Accuracy:0.13966101694915253\n\n\n\n\n\nEpoch:21 | Loss:5.505904674530029 | Accuracy:0.14779661016949153\n\n\n\n\n\nEpoch:22 | Loss:5.6631903648376465 | Accuracy:0.14915254237288136\n\n\n\n\n\nEpoch:23 | Loss:5.990276336669922 | Accuracy:0.12271186440677966\n\n\n\n\n\nEpoch:24 | Loss:6.015255928039551 | Accuracy:0.13559322033898305\n\n\n\n1\n\n\n\ntrn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=64, num_workers=4, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, num_workers=4, shuffle=False)\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_bn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:3.54060697555542 | Accuracy:0.3735593220338983\n\n\n\n\n\nEpoch:1 | Loss:3.6123523712158203 | Accuracy:0.3776271186440678\n\n\n\n\n\nEpoch:2 | Loss:3.6704928874969482 | Accuracy:0.3905084745762712\n\n\n\n\n\nEpoch:3 | Loss:3.7412772178649902 | Accuracy:0.3898305084745763\n\n\n\n\n\nEpoch:4 | Loss:3.6899402141571045 | Accuracy:0.3891525423728814\n\n\n\n\n\nEpoch:5 | Loss:3.7951247692108154 | Accuracy:0.3959322033898305\n\n\n\n\n\nEpoch:6 | Loss:3.8599658012390137 | Accuracy:0.3898305084745763\n\n\n\n\n\nEpoch:7 | Loss:3.8910741806030273 | Accuracy:0.39389830508474577\n\n\n\n\n\nEpoch:8 | Loss:4.046212196350098 | Accuracy:0.3871186440677966\n\n\n\n\n\nEpoch:9 | Loss:4.152932167053223 | Accuracy:0.37966101694915255\n\n\n\n\n\nEpoch:10 | Loss:4.072637557983398 | Accuracy:0.38305084745762713\n\n\n\n\n\nEpoch:11 | Loss:4.0407257080078125 | Accuracy:0.39322033898305087\n\n\n\n\n\nEpoch:12 | Loss:4.077968597412109 | Accuracy:0.38169491525423727\n\n\n\n\n\nEpoch:13 | Loss:4.097992897033691 | Accuracy:0.3911864406779661\n\n\n\n\n\nEpoch:14 | Loss:4.074225425720215 | Accuracy:0.3945762711864407\n\n\n\n\n\nEpoch:15 | Loss:4.119948863983154 | Accuracy:0.3905084745762712\n\n\n\n\n\nEpoch:16 | Loss:4.232583045959473 | Accuracy:0.38508474576271184\n\n\n\n\n\nEpoch:17 | Loss:4.135111331939697 | Accuracy:0.3905084745762712\n\n\n\n\n\nEpoch:18 | Loss:5.55678653717041 | Accuracy:0.288135593220339\n\n\n\n\n\nEpoch:19 | Loss:4.31779670715332 | Accuracy:0.3708474576271186\n\n\n\n\n\nEpoch:20 | Loss:3.84808349609375 | Accuracy:0.38033898305084746\n\n\n\n\n\nEpoch:21 | Loss:3.9929111003875732 | Accuracy:0.39796610169491525\n\n\n\n\n\nEpoch:22 | Loss:3.9850997924804688 | Accuracy:0.3891525423728814\n\n\n\n\n\nEpoch:23 | Loss:4.030226230621338 | Accuracy:0.3905084745762712\n\n\n\n\n\nEpoch:24 | Loss:4.062991142272949 | Accuracy:0.39864406779661016\n\n\n\n1\n\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_gn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:6.449962615966797 | Accuracy:0.1464406779661017\n\n\n\n\n\nEpoch:1 | Loss:6.826666831970215 | Accuracy:0.14101694915254237\n\n\n\n\n\nEpoch:2 | Loss:6.971066474914551 | Accuracy:0.14983050847457627\n\n\n\n\n\nEpoch:3 | Loss:7.010488510131836 | Accuracy:0.15457627118644068\n\n\n\n\n\nEpoch:4 | Loss:7.043515205383301 | Accuracy:0.1464406779661017\n\n\n\n\n\nEpoch:5 | Loss:6.943863868713379 | Accuracy:0.14372881355932204\n\n\n\n\n\nEpoch:6 | Loss:7.146998405456543 | Accuracy:0.15389830508474575\n\n\n\n\n\nEpoch:7 | Loss:7.152970790863037 | Accuracy:0.14169491525423727\n\n\n\n\n\nEpoch:8 | Loss:7.2448859214782715 | Accuracy:0.14101694915254237\n\n\n\n\n\nEpoch:9 | Loss:7.3982672691345215 | Accuracy:0.1369491525423729\n\n\n\n\n\nEpoch:10 | Loss:7.3345160484313965 | Accuracy:0.14508474576271185\n\n\n\n\n\nEpoch:11 | Loss:7.635211944580078 | Accuracy:0.14576271186440679\n\n\n\n\n\nEpoch:12 | Loss:7.198373317718506 | Accuracy:0.1335593220338983\n\n\n\n\n\nEpoch:13 | Loss:7.670504570007324 | Accuracy:0.13627118644067795\n\n\n\n\n\nEpoch:14 | Loss:7.509610652923584 | Accuracy:0.14372881355932204\n\n\n\n\n\nEpoch:15 | Loss:7.9462714195251465 | Accuracy:0.14033898305084747\n\n\n\n\n\nEpoch:16 | Loss:7.655241966247559 | Accuracy:0.1369491525423729\n\n\n\n\n\nEpoch:17 | Loss:7.840564727783203 | Accuracy:0.1288135593220339\n\n\n\n\n\nEpoch:18 | Loss:7.267488479614258 | Accuracy:0.1430508474576271\n\n\n\n\n\nEpoch:19 | Loss:7.464041709899902 | Accuracy:0.14101694915254237\n\n\n\n\n\nEpoch:20 | Loss:7.323596000671387 | Accuracy:0.1423728813559322\n\n\n\n\n\nEpoch:21 | Loss:7.24222469329834 | Accuracy:0.14847457627118643\n\n\n\n\n\nEpoch:22 | Loss:7.451413631439209 | Accuracy:0.14508474576271185\n\n\n\n\n\nEpoch:23 | Loss:7.660017967224121 | Accuracy:0.14101694915254237\n\n\n\n\n\nEpoch:24 | Loss:7.5283966064453125 | Accuracy:0.14033898305084747\n\n\n\n1"
  },
  {
    "objectID": "nbs/Group Normalization WS.html",
    "href": "nbs/Group Normalization WS.html",
    "title": "",
    "section": "",
    "text": "In this notebook, we implement GroupNorm with Weight Standardization and compare the results with BatchNorm. Simply replacing BN with GN lead to sub-optimal results.\n\nfrom fastai2.vision.all import *\nfrom nbdev.showdoc import *\nimport glob\nimport albumentations\nfrom torchvision import models\nfrom albumentations.pytorch.transforms import ToTensorV2\nset_s`eed(2)\nsubscribe.html"
  },
  {
    "objectID": "nbs/Group Normalization WS.html#dataset",
    "href": "nbs/Group Normalization WS.html#dataset",
    "title": "",
    "section": "Dataset",
    "text": "Dataset\nThe implementation of the PetsDataset has been heavily inspired and partially copied (regex part) from fastai2 repo here.\n\nclass PetsDataset:\n    def __init__(self, paths, transforms=None):\n        self.image_paths = paths\n        self.transforms = transforms\n        \n    def __len__(self): \n        return len(self.image_paths)\n    \n    def setup(self, pat=r'(.+)_\\d+.jpg$', label2int=None):\n        \"adds a label dictionary to `self`\"\n        self.pat = re.compile(pat)\n        if label2int is not None:\n            self.label2int = label2int\n            self.int2label = {v:i for i,v in self.label2int.items()}\n        else:\n            labels = [os.path.basename(self.pat.search(str(p)).group(1))\n                  for p in self.image_paths]\n            self.labels = set(labels)\n            self.label2int = {label:i for i,label in enumerate(self.labels)}\n            self.int2label = {v:i for i,v in self.label2int.items()}\n\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path)\n        img = np.array(img)\n        \n        target = os.path.basename(self.pat.search(str(img_path)).group(1))\n        target = self.label2int[target]\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']      \n            \n        return img, torch.tensor(target, dtype=torch.long)\n\n\nimage_paths = get_image_files(path/'images')\nimage_paths\n\n(#7378) [Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg')...]\n\n\n\n# remove those images that are not 3 channel\nfrom tqdm.notebook import tqdm\nrun_remove = False\ndef remove(o):\n    img = Image.open(o)\n    img = np.array(img)\n    if img.shape[2] != 3:\n        os.remove(o)\nif run_remove:\n    for o in tqdm(image_paths): remove(o)\n\n\nimage_paths = get_image_files(path/'images')\nimage_paths\n\n(#7378) [Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg')...]\n\n\n\n# augmentations using `albumentations` library\nsz = 224\ntfms = albumentations.Compose([\n    albumentations.Resize(sz, sz) if sz else albumentations.NoOp(),\n    albumentations.OneOf(\n        [albumentations.Cutout(random.randint(1,8), 16, 16),\n         albumentations.CoarseDropout(random.randint(1,8), 16, 16)]\n    ),\n    albumentations.Normalize(always_apply=True),\n    ToTensorV2()\n])\n\n\ndataset = PetsDataset(image_paths, tfms)\n\n\n# to setup the `label2int` dictionary\ndataset.setup()\n\n\ndataset[0]\n\n(tensor([[[ 0.8618,  0.1597,  0.4166,  ..., -0.6452, -0.3198, -0.2171],\n          [ 1.1872,  0.3481,  0.4166,  ..., -0.3027,  0.0912,  0.3138],\n          [ 0.8104,  0.6049,  0.0227,  ..., -0.3712, -0.1657, -0.1828],\n          ...,\n          [ 1.2385,  0.4851,  0.0227,  ...,  0.8789,  1.2214,  0.8961],\n          [ 0.7077,  0.9474, -0.6965,  ...,  0.1254,  1.5297,  1.6667],\n          [ 0.1083, -0.0801,  0.3652,  ...,  0.2111,  0.5193,  0.6734]],\n \n         [[ 0.9230,  0.4328,  0.4503,  ..., -0.2850, -0.0224, -0.0399],\n          [ 1.3256,  0.7304,  0.4678,  ..., -0.0399,  0.1527,  0.3277],\n          [ 0.8354,  0.8354,  0.3102,  ..., -0.2500, -0.1975, -0.3200],\n          ...,\n          [ 1.3606,  1.3431,  0.6078,  ...,  0.9755,  1.3957,  1.1331],\n          [ 0.7654,  1.0455, -0.0574,  ...,  0.7654,  1.6232,  1.7458],\n          [ 0.4153,  0.5903,  0.9230,  ...,  0.7654,  0.8529,  1.0980]],\n \n         [[ 0.3393, -0.3578, -0.4275,  ..., -0.7936, -0.4624, -0.3578],\n          [ 0.6531, -0.2358, -0.4973,  ..., -0.3753, -0.0615,  0.1128],\n          [ 0.0431,  0.1128, -1.0201,  ..., -0.4101, -0.2707, -0.3578],\n          ...,\n          [ 0.7228,  0.3219, -0.5321,  ...,  0.4439,  1.0017,  0.7576],\n          [ 0.2173,  0.4265, -1.1247,  ..., -0.0790,  1.1411,  1.2457],\n          [-0.4450, -0.2881,  0.1302,  ...,  0.0082,  0.2696,  0.4439]]]),\n tensor(24))\n\n\n\ndataset[0][0].shape\n\ntorch.Size([3, 224, 224])"
  },
  {
    "objectID": "nbs/Group Normalization WS.html#dataloaders",
    "href": "nbs/Group Normalization WS.html#dataloaders",
    "title": "",
    "section": "DataLoaders",
    "text": "DataLoaders\nWe divide the image_paths into train and validation with 20% split.\n\nnval = int(len(image_paths)*0.2)\nnval\n\n1475\n\n\n\ntrn_img_paths = image_paths[:-nval]\nval_img_paths = image_paths[-nval:]\nassert len(trn_img_paths) + len(val_img_paths) == len(image_paths)\nlen(trn_img_paths), len(val_img_paths)\n\n(5903, 1475)\n\n\n\ntrn_dataset = PetsDataset(trn_img_paths, transforms=tfms)\nval_dataset = PetsDataset(val_img_paths, transforms=tfms)\n\n\n# use same `label2int` dictionary as in `dataset` for consistency across train and val\ntrn_dataset.setup(label2int=dataset.label2int)\nval_dataset.setup(label2int=dataset.label2int)\n\n\ntrn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=bs, num_workers=4, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs, num_workers=4, shuffle=False)\n\n\n# make sure eveyrthing works so far\nnext(iter(trn_loader))[0].shape, next(iter(val_loader))[0].shape\n\n(torch.Size([4, 3, 224, 224]), torch.Size([4, 3, 224, 224]))"
  },
  {
    "objectID": "nbs/Group Normalization WS.html#model",
    "href": "nbs/Group Normalization WS.html#model",
    "title": "",
    "section": "Model",
    "text": "Model\nNow, we define the resnet34 from the torchvision repo with pretrained=False as we do not have pretrained weights for the GroupNorm layer.\n\n# Vanilla resnet with `BatchNorm`\nresnet34_bn = models.resnet34(num_classes=len(trn_dataset.label2int), pretrained=False)\nresnet34_bn\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=37, bias=True)\n)\n\n\nNext, we define GroupNorm_32 class with default 32 groups as in the Group Normalization research paper here.\n\nclass GroupNorm_32(torch.nn.GroupNorm):\n    def __init__(self, num_channels, num_groups=32, **kwargs):\n        super().__init__(num_groups, num_channels, **kwargs)\n\n\n# resnet34 with `GroupNorm` and `Standardized Weights`\n# `conv2d` replaced with `Conv2d_WS` and `BatchNorm` replaced with `GroupNorm`\nresnet34_gn = resnet34(norm_layer=GroupNorm_32, num_classes=len(trn_dataset.label2int))\nresnet34_gn\n\nResNet(\n  (conv1): Conv2d_WS(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d_WS(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d_WS(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d_WS(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d_WS(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (downsample): Sequential(\n        (0): Conv2d_WS(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d_WS(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d_WS(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d_WS(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d_WS(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (downsample): Sequential(\n        (0): Conv2d_WS(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d_WS(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (downsample): Sequential(\n        (0): Conv2d_WS(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d_WS(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d_WS(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d_WS(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=37, bias=True)\n)\n\n\n\n# make sure we are able to make forward pass\nresnet34_gn(next(iter(trn_loader))[0]).shape\n\ntorch.Size([4, 37])"
  },
  {
    "objectID": "nbs/Group Normalization WS.html#training-using-pytorchlightning",
    "href": "nbs/Group Normalization WS.html#training-using-pytorchlightning",
    "title": "",
    "section": "Training using PytorchLightning",
    "text": "Training using PytorchLightning\nFinally, we use PytorchLightning for training the model.\n\nfrom pytorch_lightning import LightningModule, Trainer\n\n\nclass Model(LightningModule):\n    def __init__(self, base):\n        super().__init__()\n        self.base = base\n\n    def forward(self, x):\n        return self.base(x)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\n    def step(self, batch):\n        x, y  = batch\n        y_hat = self(x)\n        loss  = nn.CrossEntropyLoss()(y_hat, y)\n        return loss, y, y_hat\n\n    def training_step(self, batch, batch_nb):\n        loss, _, _ = self.step(batch)\n        return {'loss': loss}\n\n    def validation_step(self, batch, batch_nb):\n        loss, y, y_hat = self.step(batch)\n        return {'loss': loss, 'y': y.detach(), 'y_hat': y_hat.detach()}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        acc = self.get_accuracy(outputs)\n        print(f\"Epoch:{self.current_epoch} | Loss:{avg_loss} | Accuracy:{acc}\")\n        return {'loss': avg_loss}\n    \n    def get_accuracy(self, outputs):\n        from sklearn.metrics import accuracy_score\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        preds = y_hat.argmax(1)\n        return accuracy_score(y.cpu().numpy(), preds.cpu().numpy())\n\n\n# define PL versions \nmodel_bn = Model(resnet34_bn)\nmodel_gn = Model(resnet34_gn)\n\n\ndebug = False\ngpus = torch.cuda.device_count()\ntrainer = Trainer(gpus=gpus, max_epochs=50, \n                  num_sanity_val_steps=1 if debug else 0)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n\n\nbatch_size=4\n\n# train model with `GroupNorm` with `bs=4` on the `Pets` dataset\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_gn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:3.638690710067749 | Accuracy:0.022372881355932205\n\n\n\n\n\nEpoch:1 | Loss:3.5767452716827393 | Accuracy:0.03728813559322034\n\n\n\n\n\nEpoch:2 | Loss:3.532081365585327 | Accuracy:0.05152542372881356\n\n\n\n\n\nEpoch:3 | Loss:3.497438907623291 | Accuracy:0.06033898305084746\n\n\n\n\n\nEpoch:4 | Loss:3.437784194946289 | Accuracy:0.07457627118644068\n\n\n\n\n\nEpoch:5 | Loss:3.3992772102355957 | Accuracy:0.07322033898305084\n\n\n\n\n\nEpoch:6 | Loss:3.3322556018829346 | Accuracy:0.08203389830508474\n\n\n\n\n\nEpoch:7 | Loss:3.278475761413574 | Accuracy:0.09220338983050848\n\n\n\n\n\nEpoch:8 | Loss:3.2041637897491455 | Accuracy:0.12\n\n\n\n\n\nEpoch:9 | Loss:3.1338086128234863 | Accuracy:0.13288135593220338\n\n\n\n\n\nEpoch:10 | Loss:2.9662578105926514 | Accuracy:0.15593220338983052\n\n\n\n\n\nEpoch:11 | Loss:2.9380886554718018 | Accuracy:0.16203389830508474\n\n\n\n\n\nEpoch:12 | Loss:2.7531585693359375 | Accuracy:0.21627118644067797\n\n\n\n\n\nEpoch:13 | Loss:2.7896103858947754 | Accuracy:0.2223728813559322\n\n\n\n\n\nEpoch:14 | Loss:2.5649585723876953 | Accuracy:0.26372881355932204\n\n\n\n\n\nEpoch:15 | Loss:2.5243453979492188 | Accuracy:0.3071186440677966\n\n\n\n\n\nEpoch:16 | Loss:2.453778028488159 | Accuracy:0.3220338983050847\n\n\n\n\n\nEpoch:17 | Loss:2.575655460357666 | Accuracy:0.33016949152542374\n\n\n\n\n\nEpoch:18 | Loss:2.723491668701172 | Accuracy:0.3193220338983051\n\n\n\n\n\nEpoch:19 | Loss:3.0088090896606445 | Accuracy:0.3369491525423729\n\n\n\n\n\nEpoch:20 | Loss:3.221853494644165 | Accuracy:0.3213559322033898\n\n\n\n\n\nEpoch:21 | Loss:3.3212766647338867 | Accuracy:0.34576271186440677\n\n\n\n\n\nEpoch:22 | Loss:3.6144063472747803 | Accuracy:0.3247457627118644\n\n\n\n\n\nEpoch:23 | Loss:3.542142868041992 | Accuracy:0.34440677966101696\n\n\n\n\n\nEpoch:24 | Loss:3.8027701377868652 | Accuracy:0.32610169491525426\n\n\n\n1\n\n\n\n# train model with `BatchNorm` with `bs=4` on the `Pets` dataset\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_bn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:4.403476715087891 | Accuracy:0.01966101694915254\n\n\n\n\n\nEpoch:1 | Loss:3.615051746368408 | Accuracy:0.03932203389830508\n\n\n\n\n\nEpoch:2 | Loss:3.6922903060913086 | Accuracy:0.05084745762711865\n\n\n\n\n\nEpoch:3 | Loss:3.4302172660827637 | Accuracy:0.062372881355932205\n\n\n\n\n\nEpoch:4 | Loss:3.351684331893921 | Accuracy:0.08271186440677966\n\n\n\n\n\nEpoch:5 | Loss:3.2836146354675293 | Accuracy:0.0935593220338983\n\n\n\n\n\nEpoch:6 | Loss:3.2269628047943115 | Accuracy:0.10915254237288136\n\n\n\n\n\nEpoch:7 | Loss:3.2704873085021973 | Accuracy:0.1023728813559322\n\n\n\n\n\nEpoch:8 | Loss:3.071798801422119 | Accuracy:0.1423728813559322\n\n\n\n\n\nEpoch:9 | Loss:3.0656063556671143 | Accuracy:0.15457627118644068\n\n\n\n\n\nEpoch:10 | Loss:3.0375216007232666 | Accuracy:0.17288135593220338\n\n\n\n\n\nEpoch:11 | Loss:2.8739380836486816 | Accuracy:0.2094915254237288\n\n\n\n\n\nEpoch:12 | Loss:2.7329418659210205 | Accuracy:0.23186440677966103\n\n\n\n\n\nEpoch:13 | Loss:2.737560510635376 | Accuracy:0.24813559322033898\n\n\n\n\n\nEpoch:14 | Loss:2.541532516479492 | Accuracy:0.27728813559322035\n\n\n\n\n\nEpoch:15 | Loss:2.540792226791382 | Accuracy:0.3064406779661017\n\n\n\n\n\nEpoch:16 | Loss:2.485729217529297 | Accuracy:0.3328813559322034\n\n\n\n\n\nEpoch:17 | Loss:2.7257814407348633 | Accuracy:0.31389830508474575\n\n\n\n\n\nEpoch:18 | Loss:3.07981276512146 | Accuracy:0.3247457627118644\n\n\n\n\n\nEpoch:19 | Loss:3.1801645755767822 | Accuracy:0.31661016949152543\n\n\n\n\n\nEpoch:20 | Loss:3.270585298538208 | Accuracy:0.3328813559322034\n\n\n\n\n\nEpoch:21 | Loss:3.355048656463623 | Accuracy:0.3376271186440678\n\n\n\n\n\nEpoch:22 | Loss:3.362093687057495 | Accuracy:0.29898305084745763\n\n\n\n\n\nEpoch:23 | Loss:3.470551013946533 | Accuracy:0.3389830508474576\n\n\n\n\n\nEpoch:24 | Loss:3.5411648750305176 | Accuracy:0.31254237288135595\n\n\n\n1\n\n\n\n\nbatch_size=64\n\ntrn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=64, num_workers=4, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, num_workers=4, shuffle=False)\n\n\n# redefine PL versions to remove trained weights\nmodel_bn = Model(resnet34_bn)\nmodel_gn = Model(resnet34_gn)\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_bn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:4.571255683898926 | Accuracy:0.33152542372881355\n\n\n\n\n\nEpoch:1 | Loss:4.823599815368652 | Accuracy:0.33084745762711865\n\n\n\n\n\nEpoch:2 | Loss:4.738388538360596 | Accuracy:0.33152542372881355\n\n\n\n\n\nEpoch:3 | Loss:4.6921844482421875 | Accuracy:0.3383050847457627\n\n\n\n\n\nEpoch:4 | Loss:5.571420669555664 | Accuracy:0.3227118644067797\n\n\n\n\n\nEpoch:5 | Loss:4.973819255828857 | Accuracy:0.31864406779661014\n\n\n\n\n\nEpoch:6 | Loss:4.960039138793945 | Accuracy:0.31186440677966104\n\n\n\n\n\nEpoch:7 | Loss:4.72049617767334 | Accuracy:0.33152542372881355\n\n\n\n\n\nEpoch:8 | Loss:4.7438859939575195 | Accuracy:0.3410169491525424\n\n\n\n\n\nEpoch:9 | Loss:4.7650861740112305 | Accuracy:0.33220338983050846\n\n\n\n\n\nEpoch:10 | Loss:4.842560768127441 | Accuracy:0.33491525423728813\n\n\n\n\n\nEpoch:11 | Loss:5.002099514007568 | Accuracy:0.3410169491525424\n\n\n\n\n\nEpoch:12 | Loss:4.969579696655273 | Accuracy:0.3328813559322034\n\n\n\n\n\nEpoch:13 | Loss:4.797631740570068 | Accuracy:0.3328813559322034\n\n\n\n\n\nEpoch:14 | Loss:4.790388107299805 | Accuracy:0.33220338983050846\n\n\n\n\n\nEpoch:15 | Loss:4.84404993057251 | Accuracy:0.3464406779661017\n\n\n\n\n\nEpoch:16 | Loss:4.882577896118164 | Accuracy:0.3416949152542373\n\n\n\n\n\nEpoch:17 | Loss:4.831890106201172 | Accuracy:0.3403389830508475\n\n\n\n\n\nEpoch:18 | Loss:4.815413475036621 | Accuracy:0.34576271186440677\n\n\n\n\n\nEpoch:19 | Loss:4.880715370178223 | Accuracy:0.34779661016949154\n\n\n\n\n\nEpoch:20 | Loss:4.870474815368652 | Accuracy:0.34508474576271186\n\n\n\n\n\nEpoch:21 | Loss:4.8547258377075195 | Accuracy:0.3430508474576271\n\n\n\n\n\nEpoch:22 | Loss:4.814042568206787 | Accuracy:0.3505084745762712\n\n\n\n\n\nEpoch:23 | Loss:5.573678970336914 | Accuracy:0.29152542372881357\n\n\n\n\n\nEpoch:24 | Loss:4.861083030700684 | Accuracy:0.33220338983050846\n\n\n\n1\n\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_gn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:4.338170051574707 | Accuracy:0.36135593220338985\n\n\n\n\n\nEpoch:1 | Loss:4.264873027801514 | Accuracy:0.3593220338983051\n\n\n\n\n\nEpoch:2 | Loss:4.475521564483643 | Accuracy:0.368135593220339\n\n\n\n\n\nEpoch:3 | Loss:4.5568928718566895 | Accuracy:0.37559322033898307\n\n\n\n\n\nEpoch:4 | Loss:4.563418865203857 | Accuracy:0.36610169491525424\n\n\n\n\n\nEpoch:5 | Loss:4.532094955444336 | Accuracy:0.36677966101694914\n\n\n\n\n\nEpoch:6 | Loss:4.709390163421631 | Accuracy:0.36474576271186443\n\n\n\n\n\nEpoch:7 | Loss:4.703502178192139 | Accuracy:0.34983050847457625\n\n\n\n\n\nEpoch:8 | Loss:4.687512397766113 | Accuracy:0.36135593220338985\n\n\n\n\n\nEpoch:9 | Loss:4.453052997589111 | Accuracy:0.37559322033898307\n\n\n\n\n\nEpoch:10 | Loss:4.729727745056152 | Accuracy:0.3423728813559322\n\n\n\n\n\nEpoch:11 | Loss:4.887462139129639 | Accuracy:0.34847457627118644\n\n\n\n\n\nEpoch:12 | Loss:4.761058807373047 | Accuracy:0.36\n\n\n\n\n\nEpoch:13 | Loss:4.628625869750977 | Accuracy:0.36610169491525424\n\n\n\n\n\nEpoch:14 | Loss:4.939492225646973 | Accuracy:0.3735593220338983\n\n\n\n\n\nEpoch:15 | Loss:4.9373321533203125 | Accuracy:0.36\n\n\n\n\n\nEpoch:16 | Loss:4.884154796600342 | Accuracy:0.3701694915254237\n\n\n\n\n\nEpoch:17 | Loss:5.015425682067871 | Accuracy:0.34576271186440677\n\n\n\n\n\nEpoch:18 | Loss:5.0034356117248535 | Accuracy:0.34372881355932206\n\n\n\n\n\nEpoch:19 | Loss:5.081662178039551 | Accuracy:0.34372881355932206\n\n\n\n\n\nEpoch:20 | Loss:5.115207195281982 | Accuracy:0.3403389830508475\n\n\n\n\n\nEpoch:21 | Loss:4.923257827758789 | Accuracy:0.368135593220339\n\n\n\n\n\nEpoch:22 | Loss:5.064967632293701 | Accuracy:0.3701694915254237\n\n\n\n\n\nEpoch:23 | Loss:4.966062545776367 | Accuracy:0.368135593220339\n\n\n\n\n\nEpoch:24 | Loss:5.010922431945801 | Accuracy:0.376271186440678\n\n\n\n1\n\n\n\n\nbatch_size=1\n\ntrn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=1, num_workers=4, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, num_workers=4, shuffle=False)\n\n\nmodel_bn = Model(resnet34_bn)\nmodel_gn = Model(resnet34_gn)\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_bn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:3.6087236404418945 | Accuracy:0.04067796610169491\n\n\n\n\n\nEpoch:1 | Loss:3.8362090587615967 | Accuracy:0.025084745762711864\n\n\n\n\n\nEpoch:2 | Loss:3.6673178672790527 | Accuracy:0.03593220338983051\n\n\n\n\n\nEpoch:3 | Loss:3.7399044036865234 | Accuracy:0.03389830508474576\n\n\n\n\n\nEpoch:4 | Loss:4.054337501525879 | Accuracy:0.0488135593220339\n\n\n\n\n\nEpoch:5 | Loss:4.010653972625732 | Accuracy:0.04542372881355932\n\n\n\n\n\nEpoch:6 | Loss:4.764206886291504 | Accuracy:0.05288135593220339\n\n\n\n\n\nEpoch:7 | Loss:10.56059455871582 | Accuracy:0.04474576271186441\n\n\n\n\n\nEpoch:8 | Loss:5.048521041870117 | Accuracy:0.05830508474576271\n\n\n\n\n\nEpoch:9 | Loss:4.828557014465332 | Accuracy:0.06508474576271187\n\n\n\n\n\nEpoch:10 | Loss:7.225879192352295 | Accuracy:0.05694915254237288\n\n\n\n\n\nEpoch:11 | Loss:6.472527027130127 | Accuracy:0.06779661016949153\n\n\n\n\n\nEpoch:12 | Loss:9.755941390991211 | Accuracy:0.07050847457627119\n\n\n\n\n\nEpoch:13 | Loss:13.05939769744873 | Accuracy:0.059661016949152545\n\n\n\n\n\nEpoch:14 | Loss:18.591503143310547 | Accuracy:0.06508474576271187\n\n\n\n\n\nEpoch:15 | Loss:11.946345329284668 | Accuracy:0.06915254237288136\n\n\n\n\n\nEpoch:16 | Loss:16.744611740112305 | Accuracy:0.06983050847457627\n\n\n\n\n\nEpoch:17 | Loss:12.913531303405762 | Accuracy:0.07661016949152542\n\n\n\n\n\nEpoch:18 | Loss:23.76015281677246 | Accuracy:0.06508474576271187\n\n\n\n\n\nEpoch:19 | Loss:26.5297794342041 | Accuracy:0.06576271186440678\n\n\n\n\n\nEpoch:20 | Loss:35.212242126464844 | Accuracy:0.05898305084745763\n\n\n\n\n\nEpoch:21 | Loss:16.634546279907227 | Accuracy:0.06169491525423729\n\n\n\n\n\nEpoch:22 | Loss:21.815725326538086 | Accuracy:0.062372881355932205\n\n\n\n\n\nEpoch:23 | Loss:12.68907356262207 | Accuracy:0.0711864406779661\n\n\n\n\n\nEpoch:24 | Loss:19.639753341674805 | Accuracy:0.06779661016949153\n\n\n\n1\n\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=25, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model_gn, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:3.6178038120269775 | Accuracy:0.03593220338983051\n\n\n\n\n\nEpoch:1 | Loss:3.5887539386749268 | Accuracy:0.03864406779661017\n\n\n\n\n\nEpoch:2 | Loss:3.4937922954559326 | Accuracy:0.0576271186440678\n\n\n\n\n\nEpoch:3 | Loss:3.426539421081543 | Accuracy:0.06508474576271187\n\n\n\n\n\nEpoch:4 | Loss:3.4010708332061768 | Accuracy:0.06915254237288136\n\n\n\n\n\nEpoch:5 | Loss:3.352757453918457 | Accuracy:0.08949152542372882\n\n\n\n\n\nEpoch:6 | Loss:3.3006396293640137 | Accuracy:0.10033898305084746\n\n\n\n\n\nEpoch:7 | Loss:3.2513763904571533 | Accuracy:0.09966101694915254\n\n\n\n\n\nEpoch:8 | Loss:3.2186825275421143 | Accuracy:0.11254237288135593\n\n\n\n\n\nEpoch:9 | Loss:3.1824042797088623 | Accuracy:0.12067796610169491\n\n\n\n\n\nEpoch:10 | Loss:3.1842432022094727 | Accuracy:0.1152542372881356\n\n\n\n\n\nEpoch:11 | Loss:3.080850839614868 | Accuracy:0.1342372881355932\n\n\n\n\n\nEpoch:12 | Loss:3.1100575923919678 | Accuracy:0.1430508474576271\n\n\n\n\n\nEpoch:13 | Loss:3.085071563720703 | Accuracy:0.14508474576271185\n\n\n\n\n\nEpoch:14 | Loss:3.007901906967163 | Accuracy:0.17559322033898306\n\n\n\n\n\nEpoch:15 | Loss:3.1437573432922363 | Accuracy:0.1694915254237288\n\n\n\n\n\nEpoch:16 | Loss:3.110459089279175 | Accuracy:0.1864406779661017\n\n\n\n\n\nEpoch:17 | Loss:3.5012593269348145 | Accuracy:0.18847457627118644\n\n\n\n\n\nEpoch:18 | Loss:3.4454123973846436 | Accuracy:0.21084745762711865\n\n\n\n\n\nEpoch:19 | Loss:3.8177714347839355 | Accuracy:0.21152542372881356\n\n\n\n\n\nEpoch:20 | Loss:4.031371116638184 | Accuracy:0.1952542372881356\n\n\n\n\n\nEpoch:21 | Loss:4.404645919799805 | Accuracy:0.1769491525423729\n\n\n\n\n\nEpoch:22 | Loss:4.856805324554443 | Accuracy:0.1959322033898305\n\n\n\n\n\nEpoch:23 | Loss:4.558755874633789 | Accuracy:0.21152542372881356\n\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)"
  },
  {
    "objectID": "nbs/GeM Pooling.html",
    "href": "nbs/GeM Pooling.html",
    "title": "",
    "section": "",
    "text": "BLOG: https://amaarora.github.io/2020/08/30/gempool.html\nIn this notebook, we implement GeM Pooling and compare the results with Average Pooling on PETs dataset using ResNet-34 model.\n\nfrom fastai2.vision.all import *\nfrom nbdev.showdoc import *\nimport glob\nimport albumentations\nfrom torchvision import models\nfrom albumentations.pytorch.transforms import ToTensorV2\nset_seed(2)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.hub import load_state_dict_from_url\nsubscribe.html"
  },
  {
    "objectID": "nbs/GeM Pooling.html#dataset",
    "href": "nbs/GeM Pooling.html#dataset",
    "title": "",
    "section": "Dataset",
    "text": "Dataset\nThe implementation of the PetsDataset has been heavily inspired and partially copied (regex part) from fastai2 repo here.\n\nclass PetsDataset:\n    def __init__(self, paths, transforms=None):\n        self.image_paths = paths\n        self.transforms = transforms\n        \n    def __len__(self): \n        return len(self.image_paths)\n    \n    def setup(self, pat=r'(.+)_\\d+.jpg$', label2int=None):\n        \"adds a label dictionary to `self`\"\n        self.pat = re.compile(pat)\n        if label2int is not None:\n            self.label2int = label2int\n            self.int2label = {v:i for i,v in self.label2int.items()}\n        else:\n            labels = [os.path.basename(self.pat.search(str(p)).group(1))\n                  for p in self.image_paths]\n            self.labels = set(labels)\n            self.label2int = {label:i for i,label in enumerate(self.labels)}\n            self.int2label = {v:i for i,v in self.label2int.items()}\n\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path)\n        img = np.array(img)\n        \n        target = os.path.basename(self.pat.search(str(img_path)).group(1))\n        target = self.label2int[target]\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']      \n            \n        return img, torch.tensor(target, dtype=torch.long)\n\n\nimage_paths = get_image_files(path/'images')\nimage_paths\n\n(#7378) [Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg')...]\n\n\n\n# remove those images that are not 3 channel\nfrom tqdm.notebook import tqdm\nrun_remove = False\ndef remove(o):\n    img = Image.open(o)\n    img = np.array(img)\n    if img.shape[2] != 3:\n        os.remove(o)\nif run_remove:\n    for o in tqdm(image_paths): remove(o)\n\n\nimage_paths = get_image_files(path/'images')\nimage_paths\n\n(#7378) [Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg'),Path('/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg')...]\n\n\n\n# augmentations using `albumentations` library\nsz = 224\ntfms = albumentations.Compose([\n    albumentations.Resize(sz, sz) if sz else albumentations.NoOp(),\n    albumentations.OneOf(\n        [albumentations.Cutout(random.randint(1,8), 16, 16),\n         albumentations.CoarseDropout(random.randint(1,8), 16, 16)]\n    ),\n    albumentations.Normalize(always_apply=True),\n    ToTensorV2()\n])\n\n\ndataset = PetsDataset(image_paths, tfms)\n\n\n# to setup the `label2int` dictionary\ndataset.setup()\n\n\ndataset[0]\n\n(tensor([[[ 0.8618,  0.1597,  0.4166,  ..., -0.6452, -0.3198, -0.2171],\n          [ 1.1872,  0.3481,  0.4166,  ..., -0.3027,  0.0912,  0.3138],\n          [ 0.8104,  0.6049,  0.0227,  ..., -0.3712, -0.1657, -0.1828],\n          ...,\n          [ 1.2385,  0.4851,  0.0227,  ...,  0.8789,  1.2214,  0.8961],\n          [ 0.7077,  0.9474, -0.6965,  ...,  0.1254,  1.5297,  1.6667],\n          [ 0.1083, -0.0801,  0.3652,  ...,  0.2111,  0.5193,  0.6734]],\n \n         [[ 0.9230,  0.4328,  0.4503,  ..., -0.2850, -0.0224, -0.0399],\n          [ 1.3256,  0.7304,  0.4678,  ..., -0.0399,  0.1527,  0.3277],\n          [ 0.8354,  0.8354,  0.3102,  ..., -0.2500, -0.1975, -0.3200],\n          ...,\n          [ 1.3606,  1.3431,  0.6078,  ...,  0.9755,  1.3957,  1.1331],\n          [ 0.7654,  1.0455, -0.0574,  ...,  0.7654,  1.6232,  1.7458],\n          [ 0.4153,  0.5903,  0.9230,  ...,  0.7654,  0.8529,  1.0980]],\n \n         [[ 0.3393, -0.3578, -0.4275,  ..., -0.7936, -0.4624, -0.3578],\n          [ 0.6531, -0.2358, -0.4973,  ..., -0.3753, -0.0615,  0.1128],\n          [ 0.0431,  0.1128, -1.0201,  ..., -0.4101, -0.2707, -0.3578],\n          ...,\n          [ 0.7228,  0.3219, -0.5321,  ...,  0.4439,  1.0017,  0.7576],\n          [ 0.2173,  0.4265, -1.1247,  ..., -0.0790,  1.1411,  1.2457],\n          [-0.4450, -0.2881,  0.1302,  ...,  0.0082,  0.2696,  0.4439]]]),\n tensor(13))\n\n\n\ndataset[0][0].shape\n\ntorch.Size([3, 224, 224])"
  },
  {
    "objectID": "nbs/GeM Pooling.html#dataloaders",
    "href": "nbs/GeM Pooling.html#dataloaders",
    "title": "",
    "section": "DataLoaders",
    "text": "DataLoaders\nWe divide the image_paths into train and validation with 20% split.\n\nnval = int(len(image_paths)*0.2)\nnval\n\n1475\n\n\n\ntrn_img_paths = image_paths[:-nval]\nval_img_paths = image_paths[-nval:]\nassert len(trn_img_paths) + len(val_img_paths) == len(image_paths)\nlen(trn_img_paths), len(val_img_paths)\n\n(5903, 1475)\n\n\n\ntrn_dataset = PetsDataset(trn_img_paths, transforms=tfms)\nval_dataset = PetsDataset(val_img_paths, transforms=tfms)\n\n\n# use same `label2int` dictionary as in `dataset` for consistency across train and val\ntrn_dataset.setup(label2int=dataset.label2int)\nval_dataset.setup(label2int=dataset.label2int)\n\n\ntrn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=bs, num_workers=4, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs, num_workers=4, shuffle=False)\n\n\n# make sure eveyrthing works so far\nnext(iter(trn_loader))[0].shape, next(iter(val_loader))[0].shape\n\n(torch.Size([4, 3, 224, 224]), torch.Size([4, 3, 224, 224]))"
  },
  {
    "objectID": "nbs/GeM Pooling.html#model-with-gem-pooling",
    "href": "nbs/GeM Pooling.html#model-with-gem-pooling",
    "title": "",
    "section": "Model with GeM pooling",
    "text": "Model with GeM pooling\n*Implementation of GeM Pooling has been copied and replicated from the official code implementation of the research paper here.\nHere, we download the pretrained ResNet-34 model.\n\n# Vanilla resnet with `BatchNorm`\nmodel_resnet34 = models.resnet34(pretrained=True)\nin_features = model_resnet34.fc.in_features\n\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n\n\nfeatures = list(model_resnet34.children())[:-2]\npool = GeM()\n\n\nclass GeM_ResNet(nn.Module):    \n    def __init__(self, features, pool):\n        super(GeM_ResNet, self).__init__()\n        self.features = nn.Sequential(*features)\n        self.fc = nn.Linear(in_features, len(trn_dataset.label2int))\n        self.pool = pool\n    \n    def forward(self, x):\n        o = self.features(x)\n        o = self.pool(o).squeeze(-1).squeeze(-1)\n        o = self.fc(o)\n        return o\n\n\nmodel_pool = GeM_ResNet(features, pool)\nmodel_pool\n\nGeM_ResNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (fc): Linear(in_features=512, out_features=37, bias=True)\n  (pool): GeM(p=3.0000, eps=1e-06)\n)\n\n\nAs can be seen, the pool layer in this network is GeM instead of the common Average Pooling layer used by default."
  },
  {
    "objectID": "nbs/GeM Pooling.html#training-using-pytorchlightning",
    "href": "nbs/GeM Pooling.html#training-using-pytorchlightning",
    "title": "",
    "section": "Training using PytorchLightning",
    "text": "Training using PytorchLightning\nFinally, we use PytorchLightning for training the model.\n\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom pytorch_lightning import LightningModule, Trainer\n\n\nclass Model(LightningModule):\n    def __init__(self, base):\n        super().__init__()\n        self.base = base\n\n    def forward(self, x):\n        return self.base(x)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n        scheduler = LambdaLR(optimizer, lambda epoch: 0.95 ** epoch)\n        return [optimizer], [scheduler]\n        \n    def step(self, batch):\n        x, y  = batch\n        y_hat = self(x)\n        loss  = nn.CrossEntropyLoss()(y_hat, y)\n        return loss, y, y_hat\n\n    def training_step(self, batch, batch_nb):\n        loss, _, _ = self.step(batch)\n        return {'loss': loss}\n\n    def validation_step(self, batch, batch_nb):\n        loss, y, y_hat = self.step(batch)\n        return {'loss': loss, 'y': y.detach(), 'y_hat': y_hat.detach()}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        acc = self.get_accuracy(outputs)\n        print(f\"Epoch:{self.current_epoch} | Loss:{avg_loss} | Accuracy:{acc}\")\n        return {'loss': avg_loss}\n    \n    def get_accuracy(self, outputs):\n        from sklearn.metrics import accuracy_score\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        preds = y_hat.argmax(1)\n        return accuracy_score(y.cpu().numpy(), preds.cpu().numpy())\n\n\n# define PL versions \nmodel = Model(model_pool)\n\n\ndebug = False\ngpus = torch.cuda.device_count()\n\n\nbatch_size=64\n\ntrn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=64, num_workers=6, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, num_workers=6, shuffle=False)\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=5, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type       | Params\n------------------------------------\n0 | base | GeM_ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:0.38181644678115845 | Accuracy:0.88\n\n\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n\n\n\n\n\nEpoch:1 | Loss:0.30204614996910095 | Accuracy:0.9071186440677966\n\n\n\n\n\nEpoch:2 | Loss:0.290485143661499 | Accuracy:0.9098305084745762\n\n\n\n\n\nEpoch:3 | Loss:0.27039361000061035 | Accuracy:0.9166101694915254\n\n\n\n\n\nEpoch:4 | Loss:0.2537878453731537 | Accuracy:0.9233898305084746\n\n\n\n1"
  },
  {
    "objectID": "nbs/GeM Pooling.html#avgpool",
    "href": "nbs/GeM Pooling.html#avgpool",
    "title": "",
    "section": "AvgPool",
    "text": "AvgPool\n\n# define PL versions \nmodel_avg_pool = models.resnet34(pretrained=True)\nfeatures = list(model_avg_pool.children())[:-2]\nmodel_avg_pool.fc = nn.Linear(in_features, len(trn_dataset.label2int))\n\n\n# define PL versions \nmodel = Model(model_avg_pool)\n\n\ntrainer = Trainer(gpus=gpus, max_epochs=5, \n                  num_sanity_val_steps=1 if debug else 0)\ntrainer.fit(model, train_dataloader=trn_loader, val_dataloaders=val_loader)\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type   | Params\n--------------------------------\n0 | base | ResNet | 21 M  \n\n\n\n\n\n\n\n\nEpoch:0 | Loss:0.4223055839538574 | Accuracy:0.888135593220339\n\n\n\n\n\nEpoch:1 | Loss:0.3181498646736145 | Accuracy:0.9057627118644068\n\n\n\n\n\nEpoch:2 | Loss:0.26608720421791077 | Accuracy:0.92\n\n\n\n\n\nEpoch:3 | Loss:0.27744489908218384 | Accuracy:0.920677966101695\n\n\n\n\n\nEpoch:4 | Loss:0.27841755747795105 | Accuracy:0.9159322033898305\n\n\n\n1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "I work as Data Science Lead at REA Group, where we work with Property data, text and images. A part of my job is to experiment with the latest research and as part of this blog, I document my learnings as I go. I am also the authors of timm docs.\n\n\n\nYou will often see me writing about research papers (mostly in the field of Computer Vision) explaining them in a simple language in theory along with their PyTorch Implementation. Please feel free to subscribe to receive regular updates regarding new blog posts.\nDuring my time as Machine Learning Engineer at Weights and Biases, I also wrote the following blog posts:\n\nPrevious blog posts\n\n\n\n\n\n\nTitle\nLink\n\n\n\n\nExplained: Characterizing Signal Propagation to Close the Performance Gap in Unnormalized ResNets\n[link]\n\n\nRevisiting ResNets: Improved Training and Scaling Strategies\n[link]\n\n\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n[link]\n\n\nIs MLP-Mixer a CNN in Disguise?\n[link]\n\n\nAre fully connected and convolution layers equivalent? If so, how?\n[link]\n\n\nA faster way to get working and up-to-date conda environments using “fastchan”\n[link]\n\n\nInside Hugging Face’s Accelerate!\n[link]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAhead of Times - Issue 1 (Apr 24 - Apr 30)\n\n\nFirst issue of the weekly newsletter to help you stay ahead of the times with latest news & updates in the field of AI.\n\n\n\n\nNewsletter\n\n\n\n\nAs part of this newsletter, I share with you key updates, projects, GitHub repos, research trends, research papers in the field of Computer Vision, Large Language Models and Stable Diffusion.\n\n\n\n\n\n\nMay 2, 2023\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nPaper Review - ‘LaMini-LM’\n\n\nPaper review of “LaMini-LM - A Diverse Herd of Distilled Models from Large-Scale Instructions” and analysis on released 2.58M instruction dataset.\n\n\n\n\nLarge Language Models\n\n\nPaper Review\n\n\n\n\nAs part of this blog post, we regenerate a small sample of the 2.58M shared Instruction Dataset and also perform human evaluation on some of the generated models shared in the research paper.\n\n\n\n\n\n\nMay 1, 2023\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nThe Annotated CLIP (Part-2)\n\n\nLearning Transferable Visual Models From Natural Language Supervision\n\n\n\n\nMultimodal\n\n\nTransformers\n\n\nClip\n\n\n\n\nThis post is part-2 of the two series blog posts on CLIP (for part-1, please refer to my previous blog post). In this blog, we present the PyTorch code behind CLIP for model building and training. This blog post is in itself a working Jupyter Notebook.\n\n\n\n\n\n\nMar 11, 2023\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nThe Annotated CLIP (Part-1)\n\n\nLearning Transferable Visual Models From Natural Language Supervision\n\n\n\n\nMultimodal\n\n\nTransformers\n\n\n\n\nThis post is part-1 of the two series blog posts on CLIP. In this blog, we present an Introduction to CLIP in an easy to digest manner. We also compare CLIP to other research papers and look at the background and inspiration behind CLIP.\n\n\n\n\n\n\nMar 3, 2023\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nSwin Transformer\n\n\nHierarchical Vision Transformer using Shifted Windows\n\n\n\n\nComputer Vision\n\n\nModel Architecure\n\n\nTransformers\n\n\n\n\nSwin Transformer Model Architecture explained with PyTorch implementation line-by-line.\n\n\n\n\n\n\nJul 4, 2022\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nThe Annotated DETR\n\n\nEnd-to-End Object Detection with Transformers\n\n\n\n\nComputer Vision\n\n\nModel Architecure\n\n\nObject Detection\n\n\nTransformers\n\n\n\n\nDETR Model Architecture explained with PyTorch implementation line-by-line.\n\n\n\n\n\n\nJul 26, 2021\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nThe sad state of AI and tech startups in Australia today and what can we do about it\n\n\n\n\n\n\n\nAI\n\n\nJeremy Howard\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2021\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nAdam and friends\n\n\nAdam, SGD, RMSProp from scratch in PyTorch.\n\n\n\n\nComputer Vision\n\n\n\n\nBasic optimizers from scratch in PyTorch with working notebook.\n\n\n\n\n\n\nMar 13, 2021\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nVision Transformer\n\n\nAn Image is Worth 16x16 Words - Transformers for Image Recognition at Scale\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nTransformers\n\n\n\n\nIn this blog post, we will be looking at the Vision Transformer architectures in detail, and also re-implement in PyTorch from scratch.\n\n\n\n\n\n\nJan 18, 2021\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nThe EfficientDet Architecture in PyTorch\n\n\n\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nObject Detection\n\n\n\n\nIn this blog post, we will look at how to implement the EfficientDet architecture in PyTorch from scratch.\n\n\n\n\n\n\nJan 13, 2021\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nEfficientDet - Scalable and Efficient Object Detection\n\n\n\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nObject Detection\n\n\n\n\nAs part of this blog post I will explain how EfficientDets work step-by-step.\n\n\n\n\n\n\nJan 11, 2021\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nTop 100 solution - SIIM-ACR Pneumothorax Segmentation\n\n\n\n\n\n\n\nComputer Vision\n\n\nKaggle\n\n\nImage Segmentation\n\n\n\n\nIn this blog post, we will looking at Image Segmentation based problem in Pytorch with SIIM-ACR Pneumothorax Segmentation competition serving as a useful example and create a solution that will get us to the top-100 leaderboard position on Kaggle.\n\n\n\n\n\n\nSep 6, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nGeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval\n\n\n\n\n\n\n\nComputer Vision\n\n\n\n\nAs part of this blog post we will be looking at GeM pooling and also look at the research paper Fine-tuning CNN Image Retrieval with No Human Annotation. We also implement GeM Pooling from scratch in PyTorch.\n\n\n\n\n\n\nAug 30, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nU-Net A PyTorch Implementation in 60 lines of Code\n\n\nU-Net Convolutional Networks for Biomedical Image Segmentation\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nImage Segmentation\n\n\n\n\nAs part of this blog post we will implement the U-Net architecture in PyTorch in 60 lines of code.\n\n\n\n\n\n\nAug 30, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nSIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle\n\n\nWinning solution for SIIM-ISIC Melanoma Classification\n\n\n\n\nComputer Vision\n\n\nKaggle\n\n\n\n\nAs part of this blog post I share my winning solution for SIIM-ISIC Melanoma Classification Kaggle Competition.\n\n\n\n\n\n\nAug 23, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nEfficientNet\n\n\nRethinking Model Scaling for Convolutional Neural Networks\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\n\nLook at the current SOTA, with top-1 accuracy of 88.5% on ImageNet.\n\n\n\n\n\n\nAug 13, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nGroup Normalization\n\n\n\n\n\n\n\nComputer Vision\n\n\n\n\nIn this blog post, we will look at Group Normalization research paper and also implement Group Normalization in PyTorch from scratch.\n\n\n\n\n\n\nAug 9, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nDenseNet Architecture Explained with PyTorch Implementation from TorchVision\n\n\nDensely Connected Convolutional Networks\n\n\n\n\nProgramming\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\n\nIn this blog post, we introduce dense blocks, transition layers and look at the TorchVision implementation of DenseNet step-by-step.\n\n\n\n\n\n\nAug 2, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nSqueeze and Excitation Networks Explained with PyTorch Implementation\n\n\nSqueeze-and-Excitation Networks\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\n\nIn this blogpost, we re-implement the Squeeze-and-Excitation networks in PyTorch step-by-step with very minor updates to ResNet implementation in torchvision.\n\n\n\n\n\n\nJul 24, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nLabel Smoothing Explained using Microsoft Excel\n\n\nBetter language models and their implications\n\n\n\n\nComputer Vision\n\n\n\n\nIn this blogpost, we re-implement Label Smoothing in Microsoft Excel step by step.\n\n\n\n\n\n\nJul 18, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nAn introduction to PyTorch Lightning with comparisons to PyTorch\n\n\nBetter language models and their implications\n\n\n\n\nProgramming\n\n\nComputer Vision\n\n\n\n\nIn this blogpost, we will be going through an introduction to Pytorch Lightning and implement all the cool tricks like - Gradient Accumulation, 16-bit precision training, and also add TPU/multi-gpu support - all in a few lines of code. We will use Pytorch Lightning to work on SIIM-ISIC Melanoma Classification challenge on Kaggle.\n\n\n\n\n\n\nJul 12, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Focal Loss and when should you use it?\n\n\nBetter language models and their implications\n\n\n\n\nComputer Vision\n\n\nLoss Function\n\n\n\n\nIn this blogpost, we will understand what Focal Loss and when is it used. We will also take a dive into its math and implement step-by-step in PyTorch.\n\n\n\n\n\n\nJun 29, 2020\n\n\nAman Arora\n\n\n\n\n\n\n  \n\n\n\n\nThe Annotated GPT-2\n\n\nBetter language models and their implications\n\n\n\n\nNLP\n\n\nTransformers\n\n\n\n\nThis post presents an annotated version of the paper in the form of a line-by-line implementation in PyTorch. This document itself is a working notebook, and should be a completely usable implementation.\n\n\n\n\n\n\nFeb 18, 2020\n\n\nAman Arora\n\n\n\n\n\n\nNo matching items\n\n\n    \n        \n        Subscribe\n        * indicates required\n\n    Email Address  *"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work as Data Science Lead at REA Group, where we work with Property data, text and images. A part of my job is to experiment with the latest research and as part of this blog post, I document my learnings as I go.\nI am interested in making deep learning more accessible to everybody and as part of that, a lot of my time is also spent in contributing to open source projects such as TIMM and fastai.\nI am also self-taught - thanks to the wonderful fast.ai course.\n\n\n\n    \n        \n        Subscribe to Aman Arora's blog:\n        * indicates required\n\n    Email Address  *"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "",
    "section": "",
    "text": "<h2>Subscribe</h2>\n\n* indicates required\n\n\n<label for=\"mce-EMAIL\">Email Address  <span class=\"asterisk\">*</span>\n \n\n<div id=\"mce-responses\" class=\"clear\">\n    <div class=\"response\" id=\"mce-error-response\" style=\"display:none\"></div>\n    <div class=\"response\" id=\"mce-success-response\" style=\"display:none\"></div>\n</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\n<div style=\"position: absolute; left: -5000px;\" aria-hidden=\"true\"><input type=\"text\" name=\"b_e847230346a7c78d4745ae796_7a63b2b273\" tabindex=\"-1\" value=\"\"></div>\n<div class=\"clear\"><input type=\"submit\" value=\"Subscribe\" name=\"subscribe\" id=\"mc-embedded-subscribe\" class=\"button\"></div>\n</div>"
  },
  {
    "objectID": "posts/2020-02-18-annotatedGPT2.html#introduction",
    "href": "posts/2020-02-18-annotatedGPT2.html#introduction",
    "title": "The Annotated GPT-2",
    "section": "",
    "text": "Welcome to “The Annotated GPT-2”.\nOne of the most brilliant and well-explained articles I have ever read is The Annotated Transformer. It introduced Attention like no other post ever written. The simple idea was to present an “annotated” version of the paper Attention is all you need along with code.\nSomething I have come to realize with my little experience in Machine Learning, when you write things in code, the implementation and the secrets become clearer. It is not magic anymore.\n\nThere is nothing magic about magic. The magician merely understands something simple which doesn’t appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can “do magic.”\n– Jeffrey Friedl in the book Mastering Regular Expressions\n\nThe GPT-2 might seem like magic at first with all it’s glitter and beauty too, but hopefully I would have uncovered that magic for you and revealed all the tricks by the time you finish reading this post. That is my goal. To make it as simple as possible for the keen to understand how the GPT-2 model works underneath.\nNote: Pretty much the entirety of the code has been copied, inspired and referenced from Hugging Face’s implementation of the GPT-2, keeping merely the essentials for simplicity. If you want to train the GPT-2 model on parallel GPUs, save checkpoints while fine-tuning, run inference tasks on multiple CPUs and much more, I would recommend using the Hugging Face API. A simple tutorial on how to do so was recently released by Hugging Face and can be found here.\nIn this post, I am not trying to reinvent the wheel, but merely bringing together a list of prexisting excellent resources to make it easier for the reader to grasp GPT-2. I leave it up to the reader to further build upon these foundations in any area they choose.\n\nYou can’t build a great building on a weak foundation. You must have a solid foundation if you’re going to have a strong superstructure.\n– Gordon B. Hinckley"
  },
  {
    "objectID": "posts/2020-08-30-gempool.html#introduction",
    "href": "posts/2020-08-30-gempool.html#introduction",
    "title": "GeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval",
    "section": "",
    "text": "As mentioned in the title, today we will be looking at GeM pooling and also look at the research paper Fine-tuning CNN Image Retrieval with No Human Annotation. Recently, I started to participate in Google Landmark Recognition 2020 Kaggle competition and as a result I have been reading lots of research papers related to large scale image retrieval and recognition. GeM Pooling featured in almost all of them [1], [2], [3]. Therefore, today we will look at GeM Pooling and understand how it is different from the common Max and Average pooling techniques.\nWe will also look at the Fine-tuning CNN Image Retrieval with No Human Annotation paper briefly and understand about some common ways of doing large scale image retrieval and recognition.\nFinally, we also run our own little experiment to compare the performance of GeM Pooling with Average Pooling on PETs Dataset. In the experiment, we find that GeM Pooling doesn’t necessarily lead to performance gains on a classification task and a working notebook can be found here.\nIn this blog post, we touch upon the basics of image retrieval, and also understand GeM Pooling.\nSo, let’s get started."
  },
  {
    "objectID": "posts/2020-09-06-siimacr.html#introduction",
    "href": "posts/2020-09-06-siimacr.html#introduction",
    "title": "Top 100 solution - SIIM-ACR Pneumothorax Segmentation",
    "section": "",
    "text": "This week I spent most of my time implementing a solution for SIIM-ACR Pneumothorax Segmentation Kaggle competition and in today’s blog post, we will looking at how to work on Image Segmentation based problems in Pytorch with this competition serving as a useful example.\nThis blog post assumes the reader to have some idea about Image Segmentation, U-Net and PyTorch.\nThis post is based on a “code-first” approach with an aim to provide a reproducible training script that could then be used for other projects or Kaggle competitions. We train the model using pure PyTorch.\nFor a complete working notebook to follow along step-by-step refer here.\nFirst, we download the dataset from Kaggle and convert the .dcm files to .png images for both masks and radiography images. Next, we create a PyTorch Dataset that returns the image and mask as a dictionary. Finally, we create the model using the wonderful Pytorch Image Segmentation library by Pavel Yakubovskiy(https://github.com/qubvel). Once we have the model, we create our training and validation scripts and train it using ComboLoss that was also used by the competition winner - Anuar Aimoldin. Winning solution can be referenced here.\nWhile I found lots of blog posts showing Image Segmentation using the wonderful fastai library, there were fewer blog posts that attempted to do this task using pure Pytorch. I try the latter, because I have done the former many times and want to understand the minor details of training an Image Segmentation model in PyTorch. While it was much harder than training a model in fastai, getting stuck in problems helped me further expand my understanding on Image Segmentation.\nUsing pure PyTorch and some help from existing scripts, I was able to create a working solution that then landed in top-100 position on the private leaderboard with a dice score of 0.8421.\nWith that being said, let’s get started and look at the code pieces required to create such a solution step-by-step."
  },
  {
    "objectID": "posts/2020-08-23-siimisic.html#introduction",
    "href": "posts/2020-08-23-siimisic.html#introduction",
    "title": "SIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle",
    "section": "",
    "text": "Recently, Kaggle launched an interesting competition to identify melanoma in images of skin lesions. Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It’s also expected that almost 7,000 people will die from the disease. This was an important competition as with other cancers, early and accurate detection—potentially aided by data science can make treatment more effective.\nIt was also a particularly challenging competition with highly imbalanced dataset with only 2% positive cases and the rest 98% negative cases.\nIn this blog post, I will be sharing my journey from the start and also the solution that led to 153 private leaderboard rank from a total of 3314 teams."
  },
  {
    "objectID": "posts/2020-09-13-unet.html#introduction",
    "href": "posts/2020-09-13-unet.html#introduction",
    "title": "U-Net A PyTorch Implementation in 60 lines of Code",
    "section": "",
    "text": "Today’s blog post is going to be short and sweet. Today, we will be looking at how to implement the U-Net architecture in PyTorch in 60 lines of code.\nThis blog is not an introduction to Image Segmentation or theoretical explanation of the U-Net architecture, for that, I would like to refer the reader to this wonderful article by Harshall Lamba. Rather, this blog post is a step-by-step explaination of how to implement U-Net from scratch in PyTorch.\nIn this blogpost - first, we will understand the U-Net architecture - specifically, the input and output shapes of each block. We look at the U-Net Architecture with a factory production line analogy to keep things simple and easy to digest. Next, we will translate our understanding of U-Net architecture to concise PyTorch code.\nI also share a working notebook to train this implementation of U-Net of SIIM ACR Pneumothorax Kaggle competition here. Should be as simple as to switch and model in to the implemention mentioned in this blog post.\nSo, let’s get started."
  },
  {
    "objectID": "posts/2020-07-24-SeNet.html#introduction",
    "href": "posts/2020-07-24-SeNet.html#introduction",
    "title": "Squeeze and Excitation Networks Explained with PyTorch Implementation",
    "section": "",
    "text": "In this blog post, we will be looking at the Squeeze-and-Excitation networks. We will refer to the research paper by Hu et al and first understand what Squeeze-and-Excitation networks are before implementing the novel architecture in PyTorch with very few modifications to the popular ResNet architecture.\nFirst, we develop an intuition for what SE-Nets are and the novel idea behind their success. Next, we will look at the Squeeze and Excitation operations in a little more detail. Finally, we implement the Squeeze-and-Excitation networks in PyTorch with very minor updates to ResNet implementation in torchvision."
  },
  {
    "objectID": "posts/2022-07-04-swintransformerv1.html#personal-update",
    "href": "posts/2022-07-04-swintransformerv1.html#personal-update",
    "title": "Swin Transformer",
    "section": "",
    "text": "For someone who was actively releasing blogs almost all throughout 2020 & 2021, I am kinda sad to admit that this is my first blog for the year 2022. But, at the same time, I am super excited to be back. My personal responsibilities took priority for the last 1 year and I had to give up on releasing blog posts. Now that the storm has settled, I am happy to be back.\nI also resigned from my position as Machine Learning Engineer from Weights and Biases (W&B) earlier this year and have joined REA Group as Data Science Lead. It’s quite a big change in my day to day work life, but I am up for the challenge and enjoying every second of my new job so far. :)\nI wrote many blogs on various different research papers during my time at W&B that can be found here.\nA lot has changed in the past 1 year or so since I have been away. As I catch-up with the latest research, I hope to continue releasing more blog posts fortnightly and take you on this journey with me as well. Let’s learn together!"
  },
  {
    "objectID": "posts/2020-08-09-groupnorm.html#introduction",
    "href": "posts/2020-08-09-groupnorm.html#introduction",
    "title": "Group Normalization",
    "section": "",
    "text": "In this blog post today, we will look at Group Normalization research paper and also look at: - The drawback of Batch Normalization for smaller batch sizes\n- Introduction to Group Normalization as an alternative to BN - Other normalization techniques available and how does Group Normalization compare to those - Benefits of Group Normalization over other normalization techniques - Discuss the optimal number of groups as a hyperparameter in GN - Discuss effect of Group Normalization on deeper models (eg. Resnet-101) - Implement Group Normalization in PyTorch and Tensorflow - Implement ResNet-50 with [GroupNorm + Weight Standardization] on Pets dataset and compare performance to vanilla ResNet-50 with BatchNorm layer\nBatch Normalization is used in most state-of-the art computer vision to stabilise training. BN normalizes the features based on the mean and variance in a mini-batch. This has helped improve model performance, reduce training time and also helped very deep models converge.\nBut this technique also suffers from drawbacks - if batch size is too small, training becomes unstable with BN.\nThe aim of this blog post is not to study BN, many other wonderful posts have been written on that, but to look at other alternatives such as GN.\nThrough this blog post, I hope to introduce Group Normalization as an alternative to Batch Normalization and help the reader develop an intuition for cases where GN could perform better than BN.\n\n\nKnowingly or unknowingly, we have all used BN in our experiments when training a deep learning network. If you have trained a ResNet model or pretty much any other CV model using PyTorch or Tensorflow, you have made use of BN to normalize the deep learning network.\nFrom the Group Normalization research paper, &gt; We all know that BN has been established as a very effective component in deep learning. BN normalizes the features by the mean and variance computed within a batch. But despite its great success, BN exhibits drawbacks that are also caused by its distinct behavior of normalizing along the batch dimension. In particular, it is required for BN to work with sufficiently large batch size. A small batch size leads to innacurate estimation of the batch statistics and reducing BN’s batch size increases the model error dramatically.\nEssentially, what that means is that BN is not very effective if the batch sizes are too small. Especially for CV applications other than Image classification such as object detection, segmentation, video classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes.\nEspecially in such cases, GN can be used a strong alternative to BN.\nOr, there could be cases where you might want to try a bigger capacity model leaving less space in the GPU to fit a bigger batch size. In such cases as well, you might want to try GN as an alternative.\n\n\n\nIn the paper, the authors introduce GN as a simple alternative to BN. From the paper:\n\nGN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes.\n\nEssentially, GN takes away the dependance on batch size for normalization and in doing so mitigates the problem suffered by BN. There are also other techniques that have been proposed to avoid batch dimension - but we will discuss them later. For now, it is essential for the reader to realize that instead of normalizing accross the batch dimension, GN normalizes accross the groups (channel dimension). This has been further explained in depth later in this post here.\nFirst, let’s look at how GN compares to BN for training accross various batch sizes keeping all else same.\n\nAs can be seen in the image above, because GN does not depend on the batch size, the validation classification error (when the deep learning model is normalized using GN) is stable accross various batch sizes compared to BN.\n\nThe same trend as in fig-1 can also be observed in fig-2 where the validation error is consistent accross various batch sizes for GN as opposed to BN. Another key thing to note, the validation error for GN as reported in the research paper is very similar to that for BN - therefore, GN can be considered to be a strong alternative to BN.\nThe validation errors (from the research paper) for various batch sizes are presented in table-1 below:\n\nWhile BN performs slightly better than GN for batch size 32, GN performs better for all lower batch sizes."
  },
  {
    "objectID": "posts/2020-07-18-label-smoothing.html#introduction",
    "href": "posts/2020-07-18-label-smoothing.html#introduction",
    "title": "Label Smoothing Explained using Microsoft Excel",
    "section": "",
    "text": "In this blogpost, together, we: - Read and understand about Label Smoothing from Rethinking the Inception Architecture for Computer Vision research paper - Look at why we need Label Smoothing? - Re-implement Label Smoothing in Microsoft Excel step by step - Compare the results from our MS Excel implementation with Fastai/PyTorch versions of Label Smoothing\nWhy are we using Microsoft Excel?\nIt’s a valid question you might ask and I wasn’t a big fan of MS Excel either until I saw this video by Jeremy Howard about Cross Entropy Loss. In the video Jeremy explains Cross Entropy Loss using Microsoft Excel. It clicked and I understood it very well even with the fancy math in the cross entropy loss formula.\n\nAnd that is my hope here too! In this blogpost I hope that together we can see past the math and get the intuition for Label Smoothing and then later be able to implement it in a language/framework of our choice.\nSo, let’s get started!"
  },
  {
    "objectID": "posts/2021-03-13-optimizers.html#whos-adam-why-should-we-care-about-his-friends",
    "href": "posts/2021-03-13-optimizers.html#whos-adam-why-should-we-care-about-his-friends",
    "title": "Adam and friends",
    "section": "",
    "text": "Adam is an Optimizer. He has many friends but his dearest are SGD, Momentum & RMSprop.\nEach of Adam’s friends has contributed to Adam’s personality. So to get to know Adam very well, we should first meet the friends. We start out with SGD first, then meet Momentum, RMSprop and finally Adam.\n\nIn this blog post we are going to re-implement SGD, SGD with Momentum, RMSprop & Adam. The major contribution of this blog post is to help the reader re-implement these algorithms keeping the implementations simple & by using minimal lines of code. We try to understand these algoirthms from a code perspective rather than from a mathematical perspective. I would also like to refer the reader to Sebastian Ruder’s blog on Optimizers here for a more theoretical introduction. We also compare the implementations with PyTorch’s implementations to check accuracy.\n\nThis blog post has been structured the following way:"
  },
  {
    "objectID": "posts/2020-08-13-efficientnet.html#introduction",
    "href": "posts/2020-08-13-efficientnet.html#introduction",
    "title": "EfficientNet",
    "section": "",
    "text": "It brings me great pleasure as I begin writing about EfficientNets for two reasons: 1. At the time of writing, Fixing the train-test resolution discrepancy: FixEfficientNet (family of EfficientNet) is the current State of Art on ImageNet with 88.5% top-1 accuracy and 98.7% top-5 accuracy. 2. This blog post also sets up the base for future blog posts on Self-training with Noisy Student improves ImageNet classification, Fixing the train-test resolution discrepancy and Fixing the train-test resolution discrepancy: FixEfficientNet.\nIn this blog post, in The “Why” section, we take a look at the superior performance of EfficientNets compared to their counterparts and understand why we are looking into EfficientNets and “why” they are totally worth your time.\nNext, in “The How” section, we start to unravel the magic inside EfficientNets. Particularly, we look at two main contributions from the research paper: 1. Compound Scaling 2. The EfficientNet Architecture (developed using Neural Architecture Search)\nHaving introduced the two contributions in The “How”, we the compare the conventional methods of scaling with Compound Scaling approach in Comparing Conventional Methods with Compound Scaling.\nFinally we look at the details of the EfficientNet Architecture in The EfficientNet Architecture using NAS and learn how the authors used Nerual Architecture Search (NAS) to get EfficientNet-B0 architecture and scaled it using Compound Scaling technique to get EfficientNet B1-B7.\nSo, let’s get started!"
  },
  {
    "objectID": "posts/2020-08-02-densenets.html#introduction",
    "href": "posts/2020-08-02-densenets.html#introduction",
    "title": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision",
    "section": "",
    "text": "In this post today, we will be looking at DenseNet architecture from the research paper Densely Connected Convolutional Networks.\nThe overall agenda is to: - Understand what DenseNet architecture is - Introduce dense blocks, transition layers and look at a single dense block in more detail - Understand step-by-step the TorchVision implementation of DenseNet"
  },
  {
    "objectID": "posts/2021-07-26-annotateddetr.html#foreword",
    "href": "posts/2021-07-26-annotateddetr.html#foreword",
    "title": "The Annotated DETR",
    "section": "",
    "text": "Welcome to “The Annotated DETR”.\nOne of the most brilliant and well-explained articles I have read is The Annotated Transformer. It introduced Attention like no other post. The simple idea was to present an “annotated” version of the paper Attention is all you need along with code.\nSomething I have always believed in is that when you write things in code, the implementation and secrets become clearer. Nothing is hidden anymore. Reading the research paper with it’s code in front of me, is the best way for me to understand the paper.\n\nThere is nothing magic about magic. The magician merely understands something simple which doesn’t appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can “do magic.”\n– Jeffrey Friedl in the book Mastering Regular Expressions\n\nThe DETR Architecture might seem like magic at first with all it’s glitter and beauty too, but hopefully I would have uncovered that magic for you and revealed all the tricks by the time you finish reading this post. That is my goal -\n\nTo make it as simple as possible for the readers to understand how the DETR model works underneath.\n\nIn this post, I am not trying to reinvent the wheel, but merely bringing together a list of prexisting excellent resources to make it easier for the reader to grasp DETR. I leave it up to the reader to further build upon these foundations in any area they choose.\n\nYou can’t build a great building on a weak foundation. You must have a solid foundation if you’re going to have a strong superstructure.\n– Gordon B. Hinckley\n\nNOTE: All code referenced below has been copied from the official DETR implementation. Also, text directly quoted from the research paper is in Italics."
  },
  {
    "objectID": "nbs/Understanding Metric Learning.html#extract-labels-and-create-train-val-splits",
    "href": "nbs/Understanding Metric Learning.html#extract-labels-and-create-train-val-splits",
    "title": "Metric Learning",
    "section": "",
    "text": "img_paths = list(DATA_PATH.rglob('*.png'))\nlen(img_paths), img_paths[:2]\n\n(70000,\n [PosixPath('/home/ubuntu/.fastai/data/mnist_png/training/9/48159.png'),\n  PosixPath('/home/ubuntu/.fastai/data/mnist_png/training/9/1695.png')])\n\n\n\ndef extract_label(o):\n     return o.parent.stem\n\n\ndf = pd.DataFrame({'img_paths': img_paths})\ndf['labels'] = df.img_paths.map(extract_label)\ndf.head()\n\n\n\n\n\n\n\n\nimg_paths\nlabels\n\n\n\n\n0\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\n\n\n1\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\n\n\n2\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\n\n\n3\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\n\n\n4\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\n\n\n\n\n\n\n\n\ndef is_val(o):\n    return 'training' not in str(o)\n\n\ndf['is_val'] = -1\ndf['is_val'] = df.img_paths.map(is_val)\ndf.head()\n\n\n\n\n\n\n\n\nimg_paths\nlabels\nis_val\n\n\n\n\n0\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\nFalse\n\n\n1\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\nFalse\n\n\n2\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\nFalse\n\n\n3\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\nFalse\n\n\n4\n/home/ubuntu/.fastai/data/mnist_png/training/9...\n9\nFalse\n\n\n\n\n\n\n\n\ntrn = df.query(\"is_val==False\")\nval = df.query(\"is_val==True\")\n\nlen(trn), len(val)\n\n(60000, 10000)"
  },
  {
    "objectID": "nbs/Understanding Metric Learning.html#dataset",
    "href": "nbs/Understanding Metric Learning.html#dataset",
    "title": "Metric Learning",
    "section": "",
    "text": "class MNIST_Dataset():\n    def __init__(self, df, augmentations=None):\n        self.df            = df\n        self.img_paths     = df.img_paths.values\n        self.labels        = df.labels.values\n        self.augmentations = augmentations\n        \n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        label    = self.labels[idx]        \n        \n        img = Image.open(img_path)\n        img = img.convert('RGB')\n        img = np.array(img)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=img)\n            img       = augmented['image']\n        \n        img = np.transpose(img, (2, 0, 1)).astype(np.float32)    \n        \n        return {\n            'image': torch.tensor(img, dtype=torch.float), \n            'label': torch.tensor(int(label), dtype=torch.long)\n        }\n    \n    def __len__(self): \n        return len(self.df)\n\n\ntrain_aug = albumentations.Compose([\n    albumentations.Normalize()\n])\n\nval_aug = albumentations.Compose([\n    albumentations.Normalize()\n\n])\n\n\ntrain_dataset = MNIST_Dataset(trn, augmentations=train_aug)\nval_dataset   = MNIST_Dataset(val, augmentations=val_aug)\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_dataloader   = torch.utils.data.DataLoader(val_dataset, batch_size=64, num_workers=4, shuffle=False)\n\n\nnext(iter(train_dataloader))['image'].shape\n\ntorch.Size([64, 3, 28, 28])"
  },
  {
    "objectID": "nbs/Understanding Metric Learning.html#model",
    "href": "nbs/Understanding Metric Learning.html#model",
    "title": "Metric Learning",
    "section": "",
    "text": "class Resnet(nn.Module):\n    def __init__(self, pretrained=True, arch_name='resnet34', n_out=10):\n        super(Resnet, self).__init__()\n        self.pretrained = pretrained\n        self.base_model = getattr(models, arch_name)()\n        nftrs           = self.base_model.fc.in_features\n        self.base_model.fc = nn.Linear(nftrs, 2)\n        self.fc         = nn.Linear(2, n_out)\n\n\n    def forward(self, image, **kwargs):\n        o1 = self.base_model(image)  \n        o2 = self.fc(o1)\n        return o1, o2\n\n\n# project logits to 2 dim\nmodel = Resnet()\nmodel = model.to('cuda')"
  },
  {
    "objectID": "nbs/Understanding Metric Learning.html#training",
    "href": "nbs/Understanding Metric Learning.html#training",
    "title": "Metric Learning",
    "section": "",
    "text": "class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef train_one_epoch(train_loader, model, optimizer, loss_fn=nn.CrossEntropyLoss(), device='cuda'):\n    losses = AverageMeter()\n    model.train()\n    tk0 = tqdm(train_loader, total=len(train_loader))\n    for b_idx, data in enumerate(tk0):\n        data['image']  = data['image'].to(device)\n        data['label']  = data['label'].to(device)\n        \n        optimizer.zero_grad()\n        o1, o2 = model(**data)\n        \n        loss = loss_fn(o2, data['label'])\n\n        with torch.set_grad_enabled(True):\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        losses.update(loss.item(), train_loader.batch_size)\n        tk0.set_postfix(loss=losses.avg)\n    return losses.avg\n\n\nparams = list(model.parameters()) #+list(model.base_model.parameters()) + \noptimizer = torch.optim.Adam(params, lr=1e-4)\n\n\ndef evaluate(valid_loader, model, device='cuda'):\n    losses = AverageMeter()\n    final_preds = []\n    model.eval()\n    preds_  = torch.zeros(1,2)\n    labels_ = np.array([])\n    with torch.no_grad():\n        tk0 = tqdm(valid_loader, total=len(valid_loader))\n        for data in tk0:\n            for key, value in data.items():\n                data[key] = value.to(device)\n            o1, o2 = model(**data)\n            preds_ = torch.cat([preds_, o1.cpu()])\n            labels_ = np.append(labels_, data['label'].cpu().numpy())\n            loss   = nn.CrossEntropyLoss()(o2, data['label'])\n            preds  = o2.argmax(1)\n            acc    = (preds == data['label']).float().mean()\n            losses.update(acc.item(), valid_loader.batch_size) \n            preds = preds.cpu().numpy()\n            final_preds.extend(preds)\n            tk0.set_postfix(loss=losses.avg)\n    return losses.avg, preds_[1:], labels_\n\n\n\n\n# TRAINING using Tesla T4\nfor i in range(5):\n    train_one_epoch(train_dataloader, model, optimizer)\n\n100%|██████████| 938/938 [01:02&lt;00:00, 14.99it/s, loss=0.862]\n100%|██████████| 938/938 [01:02&lt;00:00, 14.94it/s, loss=0.368]\n100%|██████████| 938/938 [01:03&lt;00:00, 14.83it/s, loss=0.179]\n100%|██████████| 938/938 [01:03&lt;00:00, 14.79it/s, loss=0.134]\n100%|██████████| 938/938 [01:03&lt;00:00, 14.75it/s, loss=0.104] \n\n\n\nloss, preds, labels = evaluate(val_dataloader, model)\n\n100%|██████████| 157/157 [00:02&lt;00:00, 62.75it/s, loss=0.983]\n\n\n\npreds = pd.DataFrame(preds.numpy())\npreds['label'] = labels\npreds.head(2)\n\n\n\n\n\n\n\n\n0\n1\nlabel\n\n\n\n\n0\n38.511963\n16.713697\n9.0\n\n\n1\n64.323685\n28.391975\n9.0\n\n\n\n\n\n\n\n\nsns.scatterplot(data=preds, x=0, y=1, hue='label', palette='Set2', s=10);\nplt.title(\"MNIST: Test set distribution\");\n\n\n\n\n\n\n\n\n# credits: https://github.com/KaiyangZhou/pytorch-center-loss/blob/master/center_loss.py\n\nclass CenterLoss(nn.Module):\n    \"\"\"Center loss.\n    \n    Reference:\n    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n    \n    Args:\n        num_classes (int): number of classes.\n        feat_dim (int): feature dimension.\n    \"\"\"\n    def __init__(self, num_classes=10, feat_dim=2, use_gpu=True):\n        super(CenterLoss, self).__init__()\n        self.num_classes = num_classes\n        self.feat_dim = feat_dim\n        self.use_gpu = use_gpu\n\n        if self.use_gpu:\n            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n        else:\n            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n\n    def forward(self, x, labels):\n        \"\"\"\n        Args:\n            x: feature matrix with shape (batch_size, feat_dim).\n            labels: ground truth labels with shape (batch_size).\n        \"\"\"\n        batch_size = x.size(0)\n        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n        distmat.addmm_(1, -2, x, self.centers.t())\n\n        classes = torch.arange(self.num_classes).long()\n        if self.use_gpu: classes = classes.cuda()\n        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n\n        dist = distmat * mask.float()\n        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n\n        return loss\n\n\n# TRAINING using Tesla T4\nfor i in range(3):\n    train_one_epoch(train_dataloader, model, optimizer, loss_fn=CenterLoss(10,10,True))\n\n100%|██████████| 938/938 [01:02&lt;00:00, 14.89it/s, loss=9.15]\n100%|██████████| 938/938 [01:03&lt;00:00, 14.82it/s, loss=8.34]\n100%|██████████| 938/938 [01:03&lt;00:00, 14.67it/s, loss=8.35]\n\n\n\nloss, preds, labels = evaluate(val_dataloader, model)\n\n100%|██████████| 157/157 [00:02&lt;00:00, 63.02it/s, loss=0.103]  \n\n\n\npreds = pd.DataFrame(preds.numpy())\npreds['label'] = labels\npreds.head(2)\n\n\n\n\n\n\n\n\n0\n1\nlabel\n\n\n\n\n0\n0.203384\n-0.598501\n9.0\n\n\n1\n0.238211\n-0.528210\n9.0\n\n\n\n\n\n\n\n\nsns.scatterplot(data=preds, x=0, y=1, hue='label', palette='Set2', s=10);\nplt.title(\"MNIST: Test set distribution\");"
  },
  {
    "objectID": "nbs/Training.html#config",
    "href": "nbs/Training.html#config",
    "title": "SIIM-ACR PNEUMOTHORAX SEGMENTATION",
    "section": "",
    "text": "The ComboLoss function used in CRITERION below also comes from the winning solution by Anuar.\n\nIMG_SIZE         = 512\nDATA_DIR         = Path(f'../data/dataset{IMG_SIZE}/')\nTRAIN_IMG_DIR    = DATA_DIR/'train'\nTRAIN_LBL_DIR    = DATA_DIR/'mask'\nRLE_DF           = pd.read_csv('../data/train-rle.csv', names=['ImageId', 'EncodedPixels'], skiprows=1)\nKFOLD_PATH       = '../data/RLE_kfold.csv'\nTRAIN_BATCH_SIZE = 14\nVALID_BATCH_SIZE = 14\nUSE_SAMPLER      = False\nPOSTIVE_PERC     = 0.8\nENCODER          = 'se_resnext50_32x4d'\nENCODER_WEIGHTS  = 'imagenet'\nCLASSES          = ['mask']\nACTIVATION       = None \nDEVICE           = 'cuda'\nPRETRAINED_PATH  = '../data/bst_model512_fold2_0.9565.bin'\nPRETRAINED       = True\nLEARNING_RATE    = 2e-5\nEPOCHS           = 5\nLOSS_FN          = 'mixed'\nCRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\nUSE_CRIT         = True\nTRAIN_MODEL      = False\nEVALUATE         = False\nFOLD_ID          = 4\nSUBMIT_TO_KAGGLE = False\n\nAll pretrained models have been provided in a Kaggle dataset here."
  },
  {
    "objectID": "nbs/Training.html#utils",
    "href": "nbs/Training.html#utils",
    "title": "SIIM-ACR PNEUMOTHORAX SEGMENTATION",
    "section": "",
    "text": "General utility functions provided by Kaggle to perform perform run length encoding and decoding.\n\ndef run_length_decode(rle, height=1024, width=1024, fill_value=1):\n    component = np.zeros((height, width), np.float32)\n    component = component.reshape(-1)\n    rle = np.array([int(s) for s in rle.strip().split(' ')])\n    rle = rle.reshape(-1, 2)\n    start = 0\n    for index, length in rle:\n        start = start+index\n        end = start+length\n        component[start: end] = fill_value\n        start = end\n    component = component.reshape(width, height).T\n    return component\n\n\ndef run_length_encode(component):\n    component = component.T.flatten()\n    start = np.where(component[1:] &gt; component[:-1])[0]+1\n    end = np.where(component[:-1] &gt; component[1:])[0]+1\n    length = end-start\n    rle = []\n    for i in range(len(length)):\n        if i == 0:\n            rle.extend([start[0], length[0]])\n        else:\n            rle.extend([start[i]-end[i-1], length[i]])\n    rle = ' '.join([str(r) for r in rle])\n    return rle\n\n\ndef matplotlib_imshow(img, one_channel=False):\n    fig,ax = plt.subplots(figsize=(10,6))\n    ax.imshow(img.permute(1,2,0).numpy())\n\n\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy \n    n = len(images)\n    plt.figure(figsize=(16, 8))\n    image, mask = images['image'], images['mask']\n    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n    if mask.max()&gt;0:\n        plt.imshow(mask.squeeze(0), alpha=0.25)\n    plt.show()"
  },
  {
    "objectID": "nbs/Training.html#dataset",
    "href": "nbs/Training.html#dataset",
    "title": "SIIM-ACR PNEUMOTHORAX SEGMENTATION",
    "section": "",
    "text": "# create 5 folds train file if it doesn't exist already\nif not os.path.exists(KFOLD_PATH):\n    RLE_DF['has_mask'] = 0\n    RLE_DF.loc[RLE_DF.EncodedPixels!='-1', 'has_mask'] = 1\n    kf = StratifiedKFold()\n    RLE_DF['kfold']=-1\n    for fold, (train_index, test_index) in enumerate(kf.split(X=RLE_DF.ImageId, y=RLE_DF.has_mask)):\n            RLE_DF.loc[test_index, 'kfold'] = fold\n    RLE_DF.to_csv('../data/RLE_kfold.csv', index=False)\n\n\n# single fold training for now, rerun notebook to train for multi-fold\nDF       = pd.read_csv(KFOLD_PATH)\nTRAIN_DF = DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\nVAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\nlen(TRAIN_DF), len(VAL_DF)\n\n(10364, 2590)\n\n\n\n\n\n\nclass Dataset():\n    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None):\n        self.df             = rle_df\n        self.image_base_dir = image_base_dir\n        self.masks_base_dir = masks_base_dir\n        self.image_ids      = rle_df.ImageId.values\n        self.augmentation   = augmentation\n    \n    def __getitem__(self, i):\n        image_id  = self.image_ids[i]\n        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n        mask_path = os.path.join(self.masks_base_dir, image_id+'.png')\n        image     = cv2.imread(img_path, 1)\n        mask      = cv2.imread(mask_path, 0)     \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = {\"image\": image, \"mask\": mask}\n            sample = self.augmentation(**sample)\n            image, mask = sample['image'], sample['mask']\n\n        return {\n            'image': image, \n            'mask' : mask\n        }\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n\n# Train transforms\nTFMS = albu.Compose([\n    albu.HorizontalFlip(),\n    albu.Rotate(10),\n    albu.Normalize(),\n    ToTensor(),\n])\n\n# Test transforms\nTEST_TFMS = albu.Compose([\n    albu.Normalize(),\n    ToTensor(),\n])\n\n\n# train dataset\ntrain_dataset = Dataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS) \nval_dataset   = Dataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS) \n\n\n# sanity check\nimage, mask = train_dataset[1237]['image'], train_dataset[1237]['mask']\nimage.shape, mask.shape\n\n(torch.Size([3, 512, 512]), torch.Size([1, 512, 512]))\n\n\n\n# plot one with mask \nvisualize(**train_dataset[1])\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\nA sampler similar to the winning solution used by Anuar Aimoldin to help with easy convergence. (not used as part of this solution, but provided for the reader to experiment)\n\nclass PneumoSampler(Sampler):\n    def __init__(self, train_df, positive_perc=0.8):\n        assert positive_perc &gt; 0, 'percentage of positive pneumothorax images must be greater then zero'\n        self.train_df = train_df\n        self.positive_perc = positive_perc\n        self.positive_idxs = self.train_df.query('has_mask==1').index.values\n        self.negative_idxs = self.train_df.query('has_mask!=1').index.values\n        self.n_positive = len(self.positive_idxs)\n        self.n_negative = int(self.n_positive * (1 - self.positive_perc) / self.positive_perc)\n        \n    def __iter__(self):\n        negative_sample = np.random.choice(self.negative_idxs, size=self.n_negative)\n        shuffled = np.random.permutation(np.hstack((negative_sample, self.positive_idxs)))\n        return iter(shuffled.tolist())\n\n    def __len__(self):\n        return self.n_positive + self.n_negative\n\n\nSAMPLER = PneumoSampler(TRAIN_DF, positive_perc=POSTIVE_PERC)\n\n\n\n\n\n# dataloaders\ntrain_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \n                              shuffle=True if not USE_SAMPLER else False, \n                              num_workers=4, \n                              sampler=SAMPLER if USE_SAMPLER else None)\nval_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=4)\n\n\n# sanity check\nimages, masks = next(iter(train_dataloader))['image'], next(iter(train_dataloader))['mask']\nimages.shape, masks.shape\n\n(torch.Size([14, 3, 512, 512]), torch.Size([14, 1, 512, 512]))\n\n\n\n# plot train images\nimg_grid = torchvision.utils.make_grid(images[:9], nrow=3, normalize=True)\nmatplotlib_imshow(img_grid)"
  },
  {
    "objectID": "nbs/Training.html#model-and-training",
    "href": "nbs/Training.html#model-and-training",
    "title": "SIIM-ACR PNEUMOTHORAX SEGMENTATION",
    "section": "",
    "text": "In this script we use ComboLoss from winning solution but it is also possible to use MixedLoss shared below if USE_CRIT is set to False in CONFIG.\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\ndef dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\nclass MixedLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n\n    def forward(self, input, target):\n        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n        return loss.mean()\n\n\n\n\nFrom PyTorch Segmentation Models:\n\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\n\n\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score &lt; self.best_score + self.delta:\n            self.counter += 1\n            print(\n                \"EarlyStopping counter: {} out of {}\".format(\n                    self.counter, self.patience\n                )\n            )\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        model_path = Path(model_path)\n        parent = model_path.parent\n        os.makedirs(parent, exist_ok=True)\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print(\n                \"Validation score improved ({} --&gt; {}). Model saved at at {}!\".format(\n                    self.val_score, epoch_score, model_path\n                )\n            )\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\n\n\n\nclass AverageMeter:\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=1, device='cuda'):\n    losses = AverageMeter()\n    model = model.to(device)\n    model.train()\n    if accumulation_steps &gt; 1: \n        optimizer.zero_grad()\n    tk0 = tqdm(train_loader, total=len(train_loader))\n    for b_idx, data in enumerate(tk0):\n        for key, value in data.items():\n            data[key] = value.to(device)\n        if accumulation_steps == 1 and b_idx == 0:\n            optimizer.zero_grad()\n        out  = model(data['image'])\n        loss = loss_fn(out, data['mask'])\n        with torch.set_grad_enabled(True):\n            loss.backward()\n            if (b_idx + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n        losses.update(loss.item(), train_loader.batch_size)\n        tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n    return losses.avg\n\n\n\n\n\n# pixel-wise accuracy\ndef acc_metric(input, target):\n    inp = torch.where(input&gt;0.5, torch.tensor(1, device='cuda'), torch.tensor(0, device='cuda'))\n    acc = (inp.squeeze(1) == target).float().mean()\n    return acc\n\n\n# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n\ndef metric(probability, truth, threshold=0.5, reduction='none'):\n    batch_size = len(truth)\n    with torch.no_grad():\n        probability = probability.view(batch_size, -1)\n        truth = truth.view(batch_size, -1)\n        assert(probability.shape == truth.shape)\n\n        p = (probability &gt; threshold).float()\n        t = (truth &gt; 0.5).float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n        neg_index = torch.nonzero(t_sum == 0)\n        pos_index = torch.nonzero(t_sum &gt;= 1)\n\n        dice_neg = (p_sum == 0).float()\n        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n\n        dice_neg = dice_neg[neg_index]\n        dice_pos = dice_pos[pos_index]\n        dice = torch.cat([dice_pos, dice_neg])\n\n        num_neg = len(neg_index)\n        num_pos = len(pos_index)\n\n    return dice\n\n\ndef evaluate(valid_loader, model, device='cuda', metric=dice_metric):\n    losses = AverageMeter()\n    model = model.to(device)\n    model.eval()\n    tk0 = tqdm(valid_loader, total=len(valid_loader))\n    with torch.no_grad():\n        for b_idx, data in enumerate(tk0):\n            for key, value in data.items():\n                data[key] = value.to(device)\n            out   = model(data['image'])\n            out   = torch.sigmoid(out)\n            dice  = metric(out, data['mask']).cpu()\n            losses.update(dice.mean().item(), valid_loader.batch_size)\n            tk0.set_postfix(dice_score=losses.avg)\n    return losses.avg\n\n\nif PRETRAINED: \n    model.load_state_dict(torch.load(PRETRAINED_PATH))\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)\n\n\n\n\n\ncriterion = MixedLoss(10.0, 2.0) if not USE_CRIT else CRITERION \nes = EarlyStopping(patience=10, mode='max')\n\n\ncriterion\n\nComboLoss(\n  (bce): StableBCELoss()\n  (dice): DiceLoss()\n  (jaccard): JaccardLoss()\n  (lovasz): LovaszLoss()\n  (lovasz_sigmoid): LovaszLossSigmoid()\n  (focal): FocalLoss2d()\n)\n\n\n\nif TRAIN_MODEL:\n    for epoch in range(EPOCHS):\n        loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n        dice = evaluate(val_dataloader, model, metric=metric)\n        scheduler.step()\n        print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL DICE: {dice}\")\n        es(dice, model, model_path=f\"../data/bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(dice,4)}.bin\")\n        best_model = f\"../data/bst_model{IMG_SIZE}__fold{FOLD_ID}_{np.round(es.best_score,4)}.bin\"\n        if es.early_stop:\n            print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n            break\nif EVALUATE:\n    valid_score = evaluate(val_dataloader, model, metric=metric)\n    print(f\"Valid dice score: {valid_score}\")"
  },
  {
    "objectID": "nbs/Training.html#submission-on-test-images-single-model",
    "href": "nbs/Training.html#submission-on-test-images-single-model",
    "title": "SIIM-ACR PNEUMOTHORAX SEGMENTATION",
    "section": "",
    "text": "class TestDataset():\n    def __init__(self, sample_sub, image_base_dir, augmentation=None):\n        self.image_base_dir = image_base_dir\n        self.image_ids      = sample_sub.ImageId.values\n        self.augmentation   = augmentation\n    \n    def __getitem__(self, i):\n        image_id  = self.image_ids[i]\n        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n        image     = cv2.imread(img_path)\n        image     = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image)\n            image  = sample['image']\n\n        return {\n            'image': image, \n        }\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n\nSAMPLE_SUBMISSION = pd.read_csv('/home/ubuntu/repos/kaggle/acr_Pneumothorax/data/stage-2/stage_2_sample_submission.csv')\nTEST_IMG_DIR = DATA_DIR/'test'\n\nlen(SAMPLE_SUBMISSION)\n\n3205\n\n\n\ntest_dataset    = TestDataset(SAMPLE_SUBMISSION, TEST_IMG_DIR, TEST_TFMS)\ntest_dataloader = DataLoader(test_dataset, 16, shuffle=False, num_workers=6)\n\n\nmodel.load_state_dict(torch.load('&lt;path_to_bst_model&gt;'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nmodel = model.to('cuda')\n\n\ndef post_process(probability, threshold, min_size):\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((1024, 1024), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() &gt; min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num\n\n\ndef predict(test_dataloader, model, device='cuda'):\n    encoded_pixels = []\n    losses = AverageMeter()\n    model = model.to(device)\n    model.eval()\n    tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n    for b_idx, data in enumerate(tk0):\n        data['image'] = data['image'].to(device)\n        out   = model(data['image'])\n        out   = out.detach().cpu().numpy()[:, 0, :, :]\n        for out_ in out:\n            import pdb;pdb.set_trace()\n            if out_.shape != (1024, 1024):\n                out_ = cv2.resize(out_, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n            predict, num_predict = post_process(out_, 0.5, 3500)\n            if num_predict == 0:\n                encoded_pixels.append('-1')\n            else:\n                r = run_length_encode(predict)\n                encoded_pixels.append(r)\n    return encoded_pixels\n\n\nif SUBMIT_TO_KAGGLE:    \n    enc_pixels = predict(test_dataloader, model)\n\n    SAMPLE_SUBMISSION['EncodedPixels'] = enc_pixels\n    SAMPLE_SUBMISSION.to_csv('./submission.csv', index=False)\n\n    !kaggle competitions submit -c siim-acr-pneumothorax-segmentation -f submission.csv -m \"512x512 fold-2\""
  },
  {
    "objectID": "nbs/Training.html#submit-to-kaggle---five-fold",
    "href": "nbs/Training.html#submit-to-kaggle---five-fold",
    "title": "SIIM-ACR PNEUMOTHORAX SEGMENTATION",
    "section": "",
    "text": "class TestDataset():\n    def __init__(self, sample_sub, image_base_dir, augmentation=None):\n        self.image_base_dir = image_base_dir\n        self.image_ids      = sample_sub.ImageId.values\n        self.augmentation   = augmentation\n    \n    def __getitem__(self, i):\n        image_id  = self.image_ids[i]\n        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n        image     = cv2.imread(img_path)\n        image     = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image)\n            image  = sample['image']\n\n        return {\n            'image_id': image_id,\n            'image': image\n        }\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n\ntest_dataset    = TestDataset(SAMPLE_SUBMISSION, TEST_IMG_DIR, TEST_TFMS)\ntest_dataloader = DataLoader(test_dataset, 16, shuffle=False, num_workers=6)\n\n\ndef inference_image(model, images, device='cuda'):\n    images = images.to(device)\n    predicted = model(images)\n    masks = torch.sigmoid(predicted)\n    masks = masks.squeeze(1).cpu().detach().numpy()\n    return masks\n\n\ndef inference_model(model, loader, device, use_flip=False):\n    mask_dict = {}\n    for data in tqdm(loader):\n        image_ids = data['image_id']\n        images    = data['image']\n        masks = inference_image(model, images, device)\n        for name, mask in zip(image_ids, masks):\n            mask_dict[name] = mask.astype(np.float32)\n    return mask_dict\n\n\n# Path to pretrained models from https://www.kaggle.com/aroraaman/siimacr-pretrained/\ncheckpoints_list=['../data/bst_model512_fold1_0.9492.bin', \n                  '../data/bst_model512_fold2_0.9565.bin', \n                  '../data/bst_model512_fold3_0.9608.bin', \n                  '../data/bst_model512_fold4_0.9604.bin']\n\n\nmask_dict = defaultdict(int)\nfor pred_idx, checkpoint_path in enumerate(checkpoints_list):\n    model.load_state_dict(torch.load(checkpoint_path))\n    model.eval()\n    current_mask_dict = inference_model(model, test_dataloader, device='cuda')\n    for name, mask in current_mask_dict.items():\n        mask_dict[name] = (mask_dict[name] * pred_idx + mask) / (pred_idx + 1)\n\n100%|██████████| 201/201 [00:37&lt;00:00,  5.35it/s]\n100%|██████████| 201/201 [00:37&lt;00:00,  5.33it/s]\n100%|██████████| 201/201 [00:38&lt;00:00,  5.20it/s]\n100%|██████████| 201/201 [00:38&lt;00:00,  5.18it/s]\n\n\n\nlen(mask_dict)\n\n3205\n\n\n\nencoded_pixels = []\nfor image_name, out_ in tqdm(mask_dict.items()):\n    if out_.shape != (1024, 1024):\n        out_ = cv2.resize(out_, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n    predict, num_predict = post_process(out_, 0.275, 3500)\n    if num_predict == 0:\n        encoded_pixels.append('-1')\n    else:\n        r = run_length_encode(predict)\n        encoded_pixels.append(r)\n\n100%|██████████| 3205/3205 [00:09&lt;00:00, 344.91it/s]\n\n\n\nSAMPLE_SUBMISSION['EncodedPixels'] = encoded_pixels\nSAMPLE_SUBMISSION.to_csv('./submission.csv', index=False)\n\n!kaggle competitions submit -c siim-acr-pneumothorax-segmentation -f submission.csv -m \"512x512 0-4 fold\"\n\n100%|█████████████████████████████████████████| 908k/908k [00:07&lt;00:00, 119kB/s]\nSuccessfully submitted to SIIM-ACR Pneumothorax Segmentation"
  },
  {
    "objectID": "nbs/Training.html#model-paths-for-ensemble",
    "href": "nbs/Training.html#model-paths-for-ensemble",
    "title": "SIIM-ACR PNEUMOTHORAX SEGMENTATION",
    "section": "",
    "text": "../data/bst_model512_0.8193.bin (fold-0)\n../data/bst_model512_fold1_0.9492.bin (fold-1)\n../data/bst_model512_fold2_0.9565.bin(fold-2)\n../data/bst_model512_fold3_0.9608.bin (fold-3)"
  },
  {
    "objectID": "thankyou.html",
    "href": "thankyou.html",
    "title": "Thank You!",
    "section": "",
    "text": "Thank you for subscribing to my blog. I send normally monthly updates to your inbox sharing new blog posts!\n\n\n\n    \n        \n        Subscribe to Aman Arora's blog:\n        * indicates required\n\n    Email Address  *"
  },
  {
    "objectID": "posts/Understanding_CLIP.html",
    "href": "posts/Understanding_CLIP.html",
    "title": "The Annotated CLIP (Part-1)",
    "section": "",
    "text": "Hello, and welcome back everybody to the blog! This is my first blog of the year 2023 and as publicly announced on Twitter, I am returning to blogging with a commitment of 1 blog a week, planned to be released every Monday at 9am AEST.\n\n\nStarting 01 Mar, 2023 I’ll be going back to blogging 1 post a week every Monday at 9am AEST. These blogs will be about AI research, new technologies, updates, frameworks, Kaggle competitions and more. If you have a topic that you’d like me to cover, please let me know. :)\n\n— Aman Arora ((amaarora?)) February 7, 2023\n\n\nAlso, in case you missed it, I was also recently interviewed by Radek Osmulski - “How to Blog to Advance Your Career and Learn Faster” (in AI). In the video, we discuss and talk about my motivation for writing blogs, blogging to advance your career and learn, how to get started with blogging & more!\n\n\n\n\nI have also updated my personal blog to use Quarto. The idea is to release all future blog posts which are working Jupyter Notebooks themeselves.\nNow, with personal updates out of the way, let’s get started with CLIP.\nsubscribe.html"
  },
  {
    "objectID": "posts/Understanding_CLIP.html#introduction",
    "href": "posts/Understanding_CLIP.html#introduction",
    "title": "The Annotated CLIP (Part-1)",
    "section": "2 Introduction",
    "text": "2 Introduction\nAs part of this blog post we will be uncovering the inner workings of CLIP - Learning Transferable Visual Models From Natural Language Supervision (Radford et al. (2021)), and we will be looking at it’s PyTorch implementation in part-2 of the blog that will be released next week.\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” CoRR abs/2103.00020. https://arxiv.org/abs/2103.00020.\nCLIP in itself does not present a new idea, but implements an older idea of learning Image representations from text. CLIP applies this idea to a large scale dataset (400M images), and achieves zero shot transfer on ImageNet that matches ResNet-50. At the time of writing CLIP was the first model architecture to achieve such great zero shot results on ImageNet.\n\n2.1 Key Contributions\nIf I am to summarise key contributions from the paper:\n\nNew (image, text) pair dataset: We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet.\nZero-shot performance that is competitive with supervised models: We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training.\nHigh zero shot performance on ImageNet: We match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\nOpen source model and weights: We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\n\nBefore we start looking at the inner workings of CLIP, let’s look at some background that led to the development of CLIP."
  },
  {
    "objectID": "posts/Understanding_CLIP.html#motivation-for-clip",
    "href": "posts/Understanding_CLIP.html#motivation-for-clip",
    "title": "The Annotated CLIP (Part-1)",
    "section": "3 Motivation for CLIP",
    "text": "3 Motivation for CLIP\nCLIP was written in 2021, at a time where text transformer based models like GPT-3 (and others) were competitive across many tasks on various benchmark datasets, swhile requiring little to no dataset specific training data. This was made possible by pretraining on huge amounts of data found directly from the web. Pre-training on Wikipidea articles (WebText which contained the text subset of these 45 million links), became standard practice.\nIt was clear that models pre-trained on high web-scale collections of text surpassed that of high-quality crowd-labeled NLP datasets.\nBut, for vision based models, it was still standard practice to pre-train models on crowd-labeled datasets such as ImageNet. The question then is Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision?\nBefore CLIP - VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and ConVIRT (Zhang et al., 2020) that demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text. While exciting as proofs of concept, using natural language supervision for image representation learning was still rare.\nFor example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This was well below the 88.4% accuracy of state of art at the time.\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset. However, both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively.\nNatural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their “zero-shot” capabilities.\nIn my head, if I am to summarise “What is CLIP?”, the answer would be:\nCLIP is a way to effective pretrain vision models (& more), which is architecture agnostic, by training on huge amounts of data much beyond ImageNet. In CLIP, we are not learning from expensive labels, but rather, model learns visual representations from free text.\nLearning good visual and vision-language representations is critical to solving computer vision problems — image retrieval, image classification, video understanding.\nAs part of this paper the authors introduce the CLIP “model”, which is trained on 400M (image, text) pairs. It is a neural network that learns visual concepts from natural language supervision. But, in no way is CLIP limited to the specific model architectures introduced in the paper.\n\n\n\n\n\n\nNote\n\n\n\nCLIP Is architecture agnostic. You can use any models as visual and text encoders.\nA team led by Ross Wightman, Cade Gordon, and Vaishaal Shankar have an open source repository OpenCLIP that is an open source implementation of CLIP and enables training any vision models with contrastive image-text supervision. Recently, Ross Wightman announced an 847M param ConvNext model trained via CLIP training that achieves 79.43% ImageNet zero-shot eval.\n\n\n\n\n\n\n\n\nNote\n\n\n\nCLIP was the first paper to demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet (more on the dataset later). CLIP researchers were able to match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\n\n\nAs I am catching up with deep learning research, one latest update that I’ve seen happen in the last year or so is the shear scale of models. Pretraining on huge amounts of data at scale has led to further advancements in the field of deep learning.\nAlso, CLIP image and text encoders are a critical part of Stable Diffusion, released last year.\nCLIP isn’t the only model that learns visual features from natural language supervision. Google also released a model architecture called Align (Jia et al. (2021)) that pretrains text and image encoders using data at scale.\n\nJia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. “Scaling up Visual and Vision-Language Representation Learning with Noisy Text Supervision.” CoRR abs/2102.05918. https://arxiv.org/abs/2102.05918.\nWe find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find it can be competitive with prior task-specific supervised models. We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model’s capability."
  },
  {
    "objectID": "posts/Understanding_CLIP.html#approach",
    "href": "posts/Understanding_CLIP.html#approach",
    "title": "The Annotated CLIP (Part-1)",
    "section": "4 Approach",
    "text": "4 Approach\nAt the core of our approach is the idea of learning perception from supervision contained in natural language. As discussed before, this is not at all a new idea.\nLearning from natural language has several potential strengths over other training methods. Can you think of some?\n\nIt’s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification.\nLearning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn’t “just” learn a representation but also connects that representation to language which enables flexible zero-shot transfer.\n\n\n4.1 Summary with Pseudo-Code\nIn this section I will present the summary of CLIP architecture from the paper.\nThe overall approach has been presented in Figure 3 below. Can you notice the similarities to ConVirt in Figure 2?\n\n\n\nFigure 3: Summary of CLIP approach\n\n\n\n\n\n\n\n\nNote\n\n\n\nCLIP Is architecture agnostic. You can use any models as visual and text encoders in Figure 3.\nA team led by Ross Wightman, Cade Gordon, and Vaishaal Shankar have a repository OpenCLIP that is an open source implementation of CLIP and enables training for any vision models with contrastive image-text supervision.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlso, recently, Ross Wightman also announced a 847M param ConvNext model trained via CLIP training that achieves 79.43% ImageNet zero-shot eval. Astonishing, right?\n\n\nThe approach presented in Figure 3 has been summarised in pseudo code from paper:\n# image_encoder - ResNet or Vision Transformer \n# text_encoder - CBOW or Text Transformer \n# I[n, h, w, c] - minibatch of aligned images \n# T[n, l] - minibatch of aligned texts \n# W_i[d_i, d_e] - learned proj of image to embed \n# W_t[d_t, d_e] - learned proj of text to embed \n# t - learned temperature parameter \n\n# extract feature representations of each modality \nI_f = image_encoder(I) #[n, d_i] \nT_f = text_encoder(T) #[n, d_t] \n\n# joint multimodal embedding [n, d_e] \nI_e = l2_normalize(np.dot(I_f, W_i), axis=1) \nT_e = l2_normalize(np.dot(T_f, W_t), axis=1) \n\n# scaled pairwise cosine similarities [n, n] \nlogits = np.dot(I_e, T_e.T) * np.exp(t) \n\n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss = (loss_i + loss_t)/2\nLet’s look at what it all means with the help of Microsoft Excel.\n\n\n\nFigure 4: Illustration of contrastive loss\n\n\nLet’s say we have 8 images with corresponding text descriptions as in Figure 4. CLIP presents a training framework to maximise the cosine similarity of text and image embeddings.\nAs from the pseudo-code, first we pass the images through image encoder and texts through text encoder respective to get image and text features.\n\n# ! pip install git+https://github.com/openai/CLIP.git\nimport torch\nimport torch.nn as nn\nimport clip\nimport timm\n\n\nx = torch.randn(8, 3, 224, 224)\nvisual_enc = timm.create_model('resnet50', num_classes=0)\nI_f = visual_enc(x)\nn, d_i = I_f.shape\nn, d_i\n\n(8, 2048)\n\n\nTherefore in the above case, referencing to pseuo-code, \\(n=8\\) and \\(d_i = 2048\\).\n\ntext_enc = nn.Embedding(100, 768)\nT_f = text_enc(torch.arange(8))\nn, d_t = T_f.shape\nn, d_t\n\n(8, 768)\n\n\nSimilarly, referencing to pseudo-code, \\(n=8\\) and \\(d_t=768\\).\nLet’s just assume the embedding dimension \\(d_e = 1024\\), now, as per psuedo-code, we can have two projection layers- \\(W_i[d_i, d_e]\\) and \\(W_t[d_t, d_e]\\) both for text and image features respectively.\n\nd_e = 1024\nW_i = nn.Linear(d_i, d_e)\nW_t = nn.Linear(d_t, d_e)\nW_i, W_t\n\n(Linear(in_features=2048, out_features=1024, bias=True),\n Linear(in_features=768, out_features=1024, bias=True))\n\n\nNow that we have our projection layers, as per pseudo-code, we get our joint multimodal embeddings through dot product.\n\nI_e = W_i(I_f)\nT_e = W_t(T_f)\nI_e.shape, T_e.shape\n\n(torch.Size([8, 1024]), torch.Size([8, 1024]))\n\n\nNow that we both our Image and Text embeddings, we can find cosine-similarity.\n\n# scaled pairwise cosine similarities [n, n] \nlogits = I_e @ T_e.T\nlogits.shape\n\ntorch.Size([8, 8])\n\n\n\n\n\nFigure 5: Cosine similarity between text and image features\n\n\nWe want the cosine similarity of real (text, image) pairs to be high (right diagonal), and everywhere else to be low.\nSo what loss function could we use in this case? The answer is in the pseudo-code.\n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss = (loss_i + loss_t)/2\nWhen we do cross entropy loss with both axis=1 and axis=0, we are pushing logits to be high for the diagonal and low everywhere else. This is also referred to as Contrastive Loss. Thus, by doing this the CLIP model is able to learn visual features directly from text.\nBy training on a dataset of 400 million (image, text) pairs, the CLIP model is able to attain zero shot performance on ImageNet that is comparable to ResNet-50!\nAnd that’s really all the magic behind CLIP."
  },
  {
    "objectID": "posts/Understanding_CLIP.html#model-architecture",
    "href": "posts/Understanding_CLIP.html#model-architecture",
    "title": "The Annotated CLIP (Part-1)",
    "section": "5 Model Architecture",
    "text": "5 Model Architecture\nHaving looked at the CLIP in theory, it is now time to look at the CLIP model architectures in code.\nWe will first look at the Image Encoder.\n\n5.1 Image Encoder\nFrom section 2.4 of the CLIP paper:\nWe consider two different architectures for the image encoder. For the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al., 2020). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” CoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\nHe, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. “Bag of Tricks for Image Classification with Convolutional Neural Networks.” CoRR abs/1812.01187. http://arxiv.org/abs/1812.01187.\n\nSo the first change from original ResNet to the ResNet architecture used in CLIP Image Encoder, is the stem. In the original ResNet architecture:\nThe input stem has a 7 × 7 convolution with an output channel of 64 and a stride of 2, followed by a 3 × 3 max pooling layer also with a stride of 2. The input stem reduces the input width and height by 4 times and increases its channel size to 64.\nself.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\nself.bn1 = norm_layer(self.inplanes)\nself.relu = nn.ReLU(inplace=True)\nself.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\nA 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions, with the first and second convolutions have their output channel of 32 and a stride of 2, while the last convolution uses a 64 output channel.\n# the 3-layer stem\nself.conv1 = nn.Conv2d(\n    3, width // 2, kernel_size=3, stride=2, padding=1, bias=False\n)\nself.bn1 = nn.BatchNorm2d(width // 2)\nself.relu1 = nn.ReLU(inplace=True)\nself.conv2 = nn.Conv2d(\n    width // 2, width // 2, kernel_size=3, padding=1, bias=False\n)\nself.bn2 = nn.BatchNorm2d(width // 2)\nself.relu2 = nn.ReLU(inplace=True)\nself.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\nself.bn3 = nn.BatchNorm2d(width)\nself.relu3 = nn.ReLU(inplace=True)\nself.avgpool = nn.AvgPool2d(2)"
  },
  {
    "objectID": "posts/Understanding_CLIP.html#motivation-for-clip-prior-work",
    "href": "posts/Understanding_CLIP.html#motivation-for-clip-prior-work",
    "title": "The Annotated CLIP (Part-1)",
    "section": "3 Motivation for CLIP & Prior Work",
    "text": "3 Motivation for CLIP & Prior Work\nCLIP was written in 2021, at a time where text transformer based models like GPT-3 (and others) were competitive across many tasks on various benchmark datasets, swhile requiring little to no dataset specific training data. This was made possible by pretraining on huge amounts of data found directly from the web. Pre-training on Wikipidea articles (WebText which contained the text subset of these 45 million links), became standard practice.\nIt was clear that models pre-trained on high web-scale collections of text surpassed that of high-quality crowd-labeled NLP datasets.\nBut, for vision based models, it was still standard practice to pre-train models on crowd-labeled datasets such as ImageNet. The question then is Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision?\nAt the time it was still common practice to train on crowd labeled datasets for computer vision.\nFor example, Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) had demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset. But both approaches used static softmax classifiers to perform prediction, which severely curtails their flexibility and limits their “zero-shot” capabilities.\nBut also, before CLIP some attempts had been made to learn image representations directly from text - VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and ConVIRT (Zhang et al., 2020).\n\n\n\n\n\n\nImportant\n\n\n\nIn my humble opinion, ConVirt - “Contrastive Learning of Medical Visual Representations from Paired Images and Text” (Zhang et al., 2020) is of most interest of all prior work.\n\n\n\nFigure 1: Two example chest X-ray images with different abnormality categories\n\n\nConVirt introduced a new method of pretraining medical image encoders with the paired text data (as in Figure 1) via a bidirectional contrastive objective between the two modalities. This method was domain-agnostic, and required no additional expert input.\nHowever, ConVirt had been applied in the medical context. CLIP expanded this idea to general visual recognition. Thus, ConVirt directly inspired CLIP.\n\n\n\nFigure 2: Overview of ConVirt\n\n\nThe figure above shows the overview of ConVirt, and as you’ll notice, CLIP is quite similar.\nFrom the ConVirt paper:\nConVIRT has directly inspired subsequent studies such as the CLIP framework (Radford et al., 2021) and the ALIGN model (Jia et al., 2021), which showed that direct adaptations of ConVIRT-style pretraining at much larger scales lead to state-of-the-art general visual recognition capabilities."
  },
  {
    "objectID": "posts/Understanding_CLIP.html#conclusion",
    "href": "posts/Understanding_CLIP.html#conclusion",
    "title": "The Annotated CLIP (Part-1)",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nAs part of this blog post, we introduced the CLIP architecture to the reader. We saw how the CLIP approach is very similar to ConVirt with some very minor differences which we will discuss further in part-2 of the blog to be released next week.\nIn part-2, we will also be looking at the PyTorch code of CLIP, and the training code from OpenClip.\nIf you enjoyed reading, please feel free to subscribe to receive regular updates regarding new blog posts."
  },
  {
    "objectID": "posts/2023-03-06_Understanding_CLIP.html",
    "href": "posts/2023-03-06_Understanding_CLIP.html",
    "title": "The Annotated CLIP (Part-1)",
    "section": "",
    "text": "Hello, and welcome back everybody to the blog! This is my first blog of the year 2023 and as publicly announced on Twitter, I am returning to blogging with a commitment of 1 blog a week, planned to be released every Monday at 9am AEST.\n\n\nStarting 01 Mar, 2023 I’ll be going back to blogging 1 post a week every Monday at 9am AEST. These blogs will be about AI research, new technologies, updates, frameworks, Kaggle competitions and more. If you have a topic that you’d like me to cover, please let me know. :)\n\n— Aman Arora ((amaarora?)) February 7, 2023\n\n\nAlso, in case you missed it, I was also recently interviewed by Radek Osmulski - “How to Blog to Advance Your Career and Learn Faster” (in AI). In the video, we discuss and talk about my motivation for writing blogs, blogging to advance your career and learn, how to get started with blogging & more!\n\n\n\n\nI have also updated my personal blog to use Quarto. The idea is to release all future blog posts which are working Jupyter Notebooks themeselves.\nNow, with personal updates out of the way, let’s get started with CLIP."
  },
  {
    "objectID": "posts/2023-03-06_Understanding_CLIP.html#introduction",
    "href": "posts/2023-03-06_Understanding_CLIP.html#introduction",
    "title": "The Annotated CLIP (Part-1)",
    "section": "2 Introduction",
    "text": "2 Introduction\nAs part of this blog post we will be uncovering the inner workings of CLIP - Learning Transferable Visual Models From Natural Language Supervision (Radford et al. (2021)), and we will be looking at it’s PyTorch implementation in part-2 of the blog that will be released next week.\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” CoRR abs/2103.00020. https://arxiv.org/abs/2103.00020.\nCLIP in itself does not present a new idea, but implements an older idea of learning Image representations from text. CLIP applies this idea to a large scale dataset (400M images), and achieves zero shot transfer on ImageNet that matches ResNet-50. At the time of writing CLIP was the first model architecture to achieve such great zero shot results on ImageNet.\n\n2.1 Key Contributions\nIf I am to summarise key contributions from the paper:\n\nNew (image, text) pair dataset: We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet.\nZero-shot performance that is competitive with supervised models: We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training.\nHigh zero shot performance on ImageNet: We match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\nOpen source model and weights: We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\n\nBefore we start looking at the inner workings of CLIP, let’s look at some background that led to the development of CLIP."
  },
  {
    "objectID": "posts/2023-03-06_Understanding_CLIP.html#motivation-for-clip-prior-work",
    "href": "posts/2023-03-06_Understanding_CLIP.html#motivation-for-clip-prior-work",
    "title": "The Annotated CLIP (Part-1)",
    "section": "3 Motivation for CLIP & Prior Work",
    "text": "3 Motivation for CLIP & Prior Work\nCLIP was written in 2021, at a time where text transformer based models like GPT-3 (and others) were competitive across many tasks on various benchmark datasets, swhile requiring little to no dataset specific training data. This was made possible by pretraining on huge amounts of data found directly from the web. Pre-training on Wikipidea articles (WebText which contained the text subset of these 45 million links), became standard practice.\nIt was clear that models pre-trained on high web-scale collections of text surpassed that of high-quality crowd-labeled NLP datasets.\nBut, for vision based models, it was still standard practice to pre-train models on crowd-labeled datasets such as ImageNet. The question then is Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision?\nAt the time it was still common practice to train on crowd labeled datasets for computer vision.\nFor example, Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) had demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset. But both approaches used static softmax classifiers to perform prediction, which severely curtails their flexibility and limits their “zero-shot” capabilities.\nBut also, before CLIP some attempts had been made to learn image representations directly from text - VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and ConVIRT (Zhang et al., 2020).\n\n\n\n\n\n\nImportant\n\n\n\nIn my humble opinion, ConVirt - “Contrastive Learning of Medical Visual Representations from Paired Images and Text” (Zhang et al., 2020) is of most interest of all prior work.\n\n\n\nFigure 1: Two example chest X-ray images with different abnormality categories\n\n\nConVirt introduced a new method of pretraining medical image encoders with the paired text data (as in Figure 1) via a bidirectional contrastive objective between the two modalities. This method was domain-agnostic, and required no additional expert input.\nHowever, ConVirt had been applied in the medical context. CLIP expanded this idea to general visual recognition. Thus, ConVirt directly inspired CLIP.\n\n\n\nFigure 2: Overview of ConVirt\n\n\nThe figure above shows the overview of ConVirt, and as you’ll notice, CLIP is quite similar.\nFrom the ConVirt paper:\nConVIRT has directly inspired subsequent studies such as the CLIP framework (Radford et al., 2021) and the ALIGN model (Jia et al., 2021), which showed that direct adaptations of ConVIRT-style pretraining at much larger scales lead to state-of-the-art general visual recognition capabilities."
  },
  {
    "objectID": "posts/2023-03-06_Understanding_CLIP.html#approach",
    "href": "posts/2023-03-06_Understanding_CLIP.html#approach",
    "title": "The Annotated CLIP (Part-1)",
    "section": "4 Approach",
    "text": "4 Approach\nAt the core of our approach is the idea of learning perception from supervision contained in natural language. As discussed before, this is not at all a new idea.\nLearning from natural language has several potential strengths over other training methods. Can you think of some?\n\nIt’s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification.\nLearning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn’t “just” learn a representation but also connects that representation to language which enables flexible zero-shot transfer.\n\n\n4.1 Summary with Pseudo-Code\nIn this section I will present the summary of CLIP architecture from the paper.\nThe overall approach has been presented in Figure 3 below. Can you notice the similarities to ConVirt in Figure 2?\n\n\n\nFigure 3: Summary of CLIP approach\n\n\n\n\n\n\n\n\nNote\n\n\n\nCLIP Is architecture agnostic. You can use any models as visual and text encoders in Figure 3.\nA team led by Ross Wightman, Cade Gordon, and Vaishaal Shankar have a repository OpenCLIP that is an open source implementation of CLIP and enables training for any vision models with contrastive image-text supervision.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlso, recently, Ross Wightman also announced a 847M param ConvNext model trained via CLIP training that achieves 79.43% ImageNet zero-shot eval. Astonishing, right?\n\n\nThe approach presented in Figure 3 has been summarised in pseudo code from paper:\n# image_encoder - ResNet or Vision Transformer \n# text_encoder - CBOW or Text Transformer \n# I[n, h, w, c] - minibatch of aligned images \n# T[n, l] - minibatch of aligned texts \n# W_i[d_i, d_e] - learned proj of image to embed \n# W_t[d_t, d_e] - learned proj of text to embed \n# t - learned temperature parameter \n\n# extract feature representations of each modality \nI_f = image_encoder(I) #[n, d_i] \nT_f = text_encoder(T) #[n, d_t] \n\n# joint multimodal embedding [n, d_e] \nI_e = l2_normalize(np.dot(I_f, W_i), axis=1) \nT_e = l2_normalize(np.dot(T_f, W_t), axis=1) \n\n# scaled pairwise cosine similarities [n, n] \nlogits = np.dot(I_e, T_e.T) * np.exp(t) \n\n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss = (loss_i + loss_t)/2\nLet’s look at what it all means with the help of Microsoft Excel.\n\n\n\nFigure 4: Illustration of contrastive loss\n\n\nLet’s say we have 8 images with corresponding text descriptions as in Figure 4. CLIP presents a training framework to maximise the cosine similarity of text and image embeddings.\nAs from the pseudo-code, first we pass the images through image encoder and texts through text encoder respective to get image and text features.\n\n# ! pip install git+https://github.com/openai/CLIP.git\nimport torch\nimport torch.nn as nn\nimport clip\nimport timm\n\n\nx = torch.randn(8, 3, 224, 224)\nvisual_enc = timm.create_model('resnet50', num_classes=0)\nI_f = visual_enc(x)\nn, d_i = I_f.shape\nn, d_i\n\n(8, 2048)\n\n\nTherefore in the above case, referencing to pseuo-code, \\(n=8\\) and \\(d_i = 2048\\).\n\ntext_enc = nn.Embedding(100, 768)\nT_f = text_enc(torch.arange(8))\nn, d_t = T_f.shape\nn, d_t\n\n(8, 768)\n\n\nSimilarly, referencing to pseudo-code, \\(n=8\\) and \\(d_t=768\\).\nLet’s just assume the embedding dimension \\(d_e = 1024\\), now, as per psuedo-code, we can have two projection layers- \\(W_i[d_i, d_e]\\) and \\(W_t[d_t, d_e]\\) both for text and image features respectively.\n\nd_e = 1024\nW_i = nn.Linear(d_i, d_e)\nW_t = nn.Linear(d_t, d_e)\nW_i, W_t\n\n(Linear(in_features=2048, out_features=1024, bias=True),\n Linear(in_features=768, out_features=1024, bias=True))\n\n\nNow that we have our projection layers, as per pseudo-code, we get our joint multimodal embeddings through dot product.\n\nI_e = W_i(I_f)\nT_e = W_t(T_f)\nI_e.shape, T_e.shape\n\n(torch.Size([8, 1024]), torch.Size([8, 1024]))\n\n\nNow that we both our Image and Text embeddings, we can find cosine-similarity.\n\n# scaled pairwise cosine similarities [n, n] \nlogits = I_e @ T_e.T\nlogits.shape\n\ntorch.Size([8, 8])\n\n\n\n\n\nFigure 5: Cosine similarity between text and image features\n\n\nWe want the cosine similarity of real (text, image) pairs to be high (right diagonal), and everywhere else to be low.\nSo what loss function could we use in this case? The answer is in the pseudo-code.\n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss = (loss_i + loss_t)/2\nWhen we do cross entropy loss with both axis=1 and axis=0, we are pushing logits to be high for the diagonal and low everywhere else. This is also referred to as Contrastive Loss. Thus, by doing this the CLIP model is able to learn visual features directly from text.\nBy training on a dataset of 400 million (image, text) pairs, the CLIP model is able to attain zero shot performance on ImageNet that is comparable to ResNet-50!\nAnd that’s really all the magic behind CLIP."
  },
  {
    "objectID": "posts/2023-03-06_Understanding_CLIP.html#conclusion",
    "href": "posts/2023-03-06_Understanding_CLIP.html#conclusion",
    "title": "The Annotated CLIP (Part-1)",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nAs part of this blog post, we introduced the CLIP architecture to the reader. We saw how the CLIP approach is very similar to ConVirt with some very minor differences which we will discuss further in part-2 of the blog to be released next week.\nIn part-2, we will also be looking at the PyTorch code of CLIP, and the training code from OpenClip.\nIf you enjoyed reading, please feel free to subscribe to receive regular updates regarding new blog posts."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html",
    "title": "The Annotated CLIP (Part-2)",
    "section": "",
    "text": "As part of this blog post we will be uncovering the inner workings of CLIP - Learning Transferable Visual Models From Natural Language Supervision by looking at it’s PyTorch implementation. For a gentle introduction to CLIP, please refer to part-1 of the blog.\n\n\n\nFigure 1: Summary of CLIP approach\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll code referenced in this blog post has been copied (and sometimes modified) from the wonderful Open CLIP repository.\nAlso note that code-folding has been set in this blog post, so you will have to unfold code to have a look at it. :)\n\n\nIn this blog post, we will be covering the following with references to further resources where necessary:\n\nData download and preparation\nCLIP architecture in code\nCLIP image encoder\nCLIP text encoder\nCLIP loss function\n\n\nFrom the open clip repository, I found the most complex part to be data preparation. That in itself could be a separate blog post, and therefore, I have only covered it partly here as the main focus is to look at the CLIP architecture. As part of this blog post we are going to assume that there is some magic function that can read the input images and texts and return tensors of shape [N, 3, 224, 224] & [N, 77] respectively, where \\(N\\) is the batch size."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#introduction",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#introduction",
    "title": "The Annotated CLIP (Part-2)",
    "section": "2 Introduction",
    "text": "2 Introduction\nAs part of this blog post we will be uncovering the inner workings of CLIP - Learning Transferable Visual Models From Natural Language Supervision (Radford et al. (2021)) by looking at it’s PyTorch implementation. For a gentle introduction to CLIP, please refer to part-1 of the blog.\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” CoRR abs/2103.00020. https://arxiv.org/abs/2103.00020."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#clip-architecture",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#clip-architecture",
    "title": "The Annotated CLIP (Part-2)",
    "section": "5 CLIP Architecture",
    "text": "5 CLIP Architecture\n\n\n\nFigure 2: Summary of CLIP approach\n\n\nFrom Figure 2, we can see that we have a text encoder and image encoder. These encoders are responsible for taking in the image and the text and and converting them to an embedding space.\nAs mentioned in Section 1, we will assume that there is some magic function that can read the .tar files and return tensors of shape [1, 3, 224, 224] for each image, and [1, 77], for each text.\nThese inputs then get encoded to embedding space using image and text encoder respectively.\nThe image encoder encodes images to embeddings \\(I_1, I_2, I_2 ... I_N\\), and the text encoder encodes respective image captions to \\(T_1, T_2, T_3 ... T_N\\).\n\n\n\n\n\n\nNote\n\n\n\nIn practice, the embedding size in CLIP is 1024. Therefore is our batch size \\(N = 8\\), the 8 input images will get encoded to a tensor of shape \\((8, 1024)\\), and also the 8 input texts will get encoded to a tensor of shape \\((8, 1024)\\).\n\n\nLet’s start by looking at the inputs and outputs of the overall CLIP model.\nFirst, we load the config, as part of this blog post we will work with ResNet-50 architecture as an example. So, let’s start by loading the corresponding config.\n\n\nCode\nimport json, torch\nfrom open_clip.model import CLIP\n\n\n\n\nCode\nwith open('../../open_clip/src/open_clip/model_configs/RN50.json') as o:\n    cfg = json.load(o)\ncfg\n\n\n{'embed_dim': 1024,\n 'vision_cfg': {'image_size': 224,\n  'layers': [3, 4, 6, 3],\n  'width': 64,\n  'patch_size': None},\n 'text_cfg': {'context_length': 77,\n  'vocab_size': 49408,\n  'width': 512,\n  'heads': 8,\n  'layers': 12}}\n\n\n\n\nCode\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = CLIP(**cfg).to(device)\nimage = torch.randn(1, 3, 224, 224).to(device)\ntext  = torch.randint(low=0, high=cfg['text_cfg']['vocab_size'], size=(1, 77)).to(device)\nimage_features, text_features, logit_scale   = model(image, text)\nimage_features.shape, text_features.shape, logit_scale\n\n\n(torch.Size([1, 1024]),\n torch.Size([1, 1024]),\n tensor(14.2857, device='cuda:0', grad_fn=<ExpBackward0>))\n\n\nAs can be seen above, the model expects image and text as inputs where in this case:\nimage = torch.randn(1, 3, 224, 224).to(device)\ntext  = torch.randint(low=0, high=cfg['text_cfg']['vocab_size'], size=(1, 77)).to(device)\nYou might recognize image shape easily - it represents 1 image with 3 channels (RGB) and (H,W) = 224. For text, each text is tokenized to max length 77. The integer numbers represent token_id with max value of cfg['text_cfg']['vocab_size'].\nMakes sense so far?\nAs for the outputs, the model returns three outputs - image_features, text_features and logit_scale.\nlogit_scale has been covered in more detail in Section 8 of this blog post. For now, think of it as a learnable parameter during model training.\nAs for image_features & text_features, these are the respective embeddings \\(I_1, I_2, I_2 ... I_N\\), and the text encoder encodes respective image captions to \\(T_1, T_2, T_3 ... T_N\\) with reference to Figure 2.\n\n\n\n\n\n\nTip\n\n\n\nBefore you proceed further, remember, the encoders are responsible for encoding the input image and text to embeddings of dimension - \\(1024\\).\nWe could have also done something like:\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = CLIP(**cfg).to(device)\nimage = torch.randn(1, 3, 224, 224).to(device)\ntext  = torch.randint(low=0, high=cfg['text_cfg']['vocab_size'], size=(1, 77)).to(device)\nimage_features = model.encode_image(image)\ntext_features = model.encode_text(text)\n\n\nNext, let’s look at the respective architectures of Image and Text encoders in more detail."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#image-encoder",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#image-encoder",
    "title": "The Annotated CLIP (Part-2)",
    "section": "4 Image Encoder",
    "text": "4 Image Encoder\nWe consider two different architectures for the image encoder. For the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al. (2020)). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\n\n4.1 Modified ResNet\nLet’s start with the first architecture.\nFor the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” CoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\nZhang, Richard. 2019. “Making Convolutional Networks Shift-Invariant Again.” CoRR abs/1904.11486. http://arxiv.org/abs/1904.11486.\n\nThere are 3 major changes as mentioned to the ResNet architecture in CLIP:\n\nThere are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\nPerforms anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\nThe final pooling layer is a QKV attention instead of an average pool\n\n\n4.1.1 ResNet stem\nLet’s look at all of them one by one in code. First, we start with There are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\n– Add image here\nIn the vanilla ResNet architecture, the stem consists of a 7x7 stride-2 convolution. This is what the stem looks like in the vanilla ResNet architecture.\nclass VanillaResNet:\n    def __init__(...):\n        self.stem = nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)\nHowever, in the paper T. He et al. (2018), where at the time, the authors raised ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet. From the paper, one of the tweaks used in the architecture:\n\nHe, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. “Bag of Tricks for Image Classification with Convolutional Neural Networks.” CoRR abs/1812.01187. http://arxiv.org/abs/1812.01187.\nA 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions.\n– Add image here\nIn code this looks like:\nclass ModifiedResNet:\n    def __init__(...):\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n    \n    def forward(self, x):\n        x = self.stem(x)\n\n\n4.1.2 Blur Pool\nThe next change is to use BlurPooling - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1.\nIn this section I will introduce BlurPooling and share how it is implemented in the ModifiedResNet architecture.\n\n\nFrom the research paper,\nModern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling.\n\n\n4.1.3 Final pooling layer\nThe last change in the network architecture is to use QKV attention instead of an average pool. We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\n\nimport torch\nimport torch.nn as nn\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x, key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n\n        return x[0]\n\n\n\n\n4.2 Modified ViT\nFrom the paper:\nFor the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al. (2020)). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” CoRR abs/2010.11929. https://arxiv.org/abs/2010.11929.\n\nSince the architecture is very similar to vanilla Vision Transformer, with a very minor change of adding LayerNorm after combining Patch embeddings and positional embeddings, I will not be covering the architecture in detail in this blog post.\nFor reference to ViT, please refer to my previous blog post that covers the architecture in detail with PyTorch code implementation - Vision Transformer"
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#text-encoder",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#text-encoder",
    "title": "The Annotated CLIP (Part-2)",
    "section": "7 Text Encoder",
    "text": "7 Text Encoder\nIn this section, let’s look at the text encoder of CLIP architecture. From the paper:\nThe text encoder is a Transformer (Vaswani et al. (2017)) with the architecture modifications described in Radford et al. (2019). As a base size we use a 63M-parameter 12layer 512-wide model with 8 attention heads. The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015). For computational efficiency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary objective, though exploration of this is left as future work.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” CoRR abs/1706.03762. http://arxiv.org/abs/1706.03762.\n\nI have previously covered the complete GPT-2 architecture used as text encoder in my previous blog post at The annotated GPT-2 and therefore, won’t be covering it here in this blog post."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#training-clip",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#training-clip",
    "title": "The Annotated CLIP (Part-2)",
    "section": "6 Training CLIP",
    "text": "6 Training CLIP\nIn this section, we will look at the important parts of the training code in CLIP.\n\n6.1 Preparing dataset\nThe first step is to prepare the dataset. One of the easiest ways to train CLIP using Open CLIP is to generate the dataset in the form of webdataset. First, let’s download the file from HF.\n\n!wget https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT/resolve/main/mscoco.parquet -q\n\n\nfrom img2dataset import download\nimport shutil\nimport os\n\noutput_dir = os.path.abspath(\"sample\")\n\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\n\ndownload(\n    processes_count=os.cpu_count(),\n    thread_count=os.cpu_count()*2,\n    url_list=\"/home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_10000.parquet\",\n    image_size=256,\n    output_folder=output_dir,\n    output_format=\"webdataset\",\n    input_format=\"parquet\",\n    url_col=\"URL\",\n    caption_col=\"TEXT\",\n    enable_wandb=False,\n    number_sample_per_shard=1000, \n    distributor=\"multiprocessing\",\n)\n\nStarting the downloading of this file\n\n\n0it [00:00, ?it/s]\n\n\nSharding file number 1 of 1 called /home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_10000.parquet\nFile sharded in 10 shards\nDownloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n\n\n10it [02:02, 12.27s/it]\n\n\nworker  - success: 0.839 - failed to download: 0.151 - failed to resize: 0.010 - images per sec: 12 - count: 1000\ntotal   - success: 0.839 - failed to download: 0.151 - failed to resize: 0.010 - images per sec: 12 - count: 1000\nworker  - success: 0.853 - failed to download: 0.138 - failed to resize: 0.009 - images per sec: 11 - count: 1000\ntotal   - success: 0.846 - failed to download: 0.144 - failed to resize: 0.009 - images per sec: 21 - count: 2000\nworker  - success: 0.844 - failed to download: 0.144 - failed to resize: 0.012 - images per sec: 11 - count: 1000\ntotal   - success: 0.845 - failed to download: 0.144 - failed to resize: 0.010 - images per sec: 31 - count: 3000\nworker  - success: 0.865 - failed to download: 0.128 - failed to resize: 0.007 - images per sec: 10 - count: 1000\ntotal   - success: 0.850 - failed to download: 0.140 - failed to resize: 0.009 - images per sec: 41 - count: 4000\nworker  - success: 0.849 - failed to download: 0.141 - failed to resize: 0.010 - images per sec: 10 - count: 1000\ntotal   - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 51 - count: 5000\nworker  - success: 0.847 - failed to download: 0.142 - failed to resize: 0.011 - images per sec: 10 - count: 1000\ntotal   - success: 0.850 - failed to download: 0.141 - failed to resize: 0.010 - images per sec: 60 - count: 6000\nworker  - success: 0.856 - failed to download: 0.134 - failed to resize: 0.010 - images per sec: 10 - count: 1000\ntotal   - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 70 - count: 7000\nworker  - success: 0.861 - failed to download: 0.130 - failed to resize: 0.009 - images per sec: 10 - count: 1000\ntotal   - success: 0.852 - failed to download: 0.139 - failed to resize: 0.010 - images per sec: 80 - count: 8000\nworker  - success: 0.876 - failed to download: 0.119 - failed to resize: 0.005 - images per sec: 10 - count: 1000\ntotal   - success: 0.854 - failed to download: 0.136 - failed to resize: 0.009 - images per sec: 89 - count: 9000\nworker  - success: 0.861 - failed to download: 0.129 - failed to resize: 0.010 - images per sec: 9 - count: 1000\ntotal   - success: 0.855 - failed to download: 0.136 - failed to resize: 0.009 - images per sec: 86 - count: 10000\n\n\n\nfrom pathlib import Path \nlist(Path('./sample/').glob(\"*.tar\"))\n\n[PosixPath('sample/00004.tar'),\n PosixPath('sample/00002.tar'),\n PosixPath('sample/00008.tar'),\n PosixPath('sample/00009.tar'),\n PosixPath('sample/00006.tar'),\n PosixPath('sample/00000.tar'),\n PosixPath('sample/00007.tar'),\n PosixPath('sample/00001.tar'),\n PosixPath('sample/00003.tar'),\n PosixPath('sample/00005.tar')]\n\n\npython -m training.main \\\n    --save-frequency 1 \\\n    --zeroshot-frequency 1 \\\n    --train-data=\"/home/ubuntu/GIT_REPOS/amaarora.github.io/posts/sample/{00000..00009}.tar\" \\\n    --warmup 10 \\\n    --batch-size=16 \\\n    --lr=1e-3 \\\n    --wd=0.1 \\\n    --epochs=30 \\\n    --workers=8 \\\n    --model RN50 \\\n    --train-num-samples 8550\nargs = Namespace(train_data='/home/ubuntu/GIT_REPOS/amaarora.github.io/posts/sample/{00000..00009}.tar', train_data_upsampling_factors=None, val_data=None, train_num_samples=10000, val_num_samples=None, dataset_type='auto', dataset_resampled=False, csv_separator='\\t', csv_img_key='filepath', csv_caption_key='title', imagenet_val=None, imagenet_v2=None, logs='./logs/', log_local=False, name=None, workers=0, batch_size=16, epochs=30, epochs_cooldown=None, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-08, wd=0.1, warmup=10, use_bn_sync=False, skip_scheduler=False, lr_scheduler='cosine', lr_cooldown_end=0.0, lr_cooldown_power=1.0, save_frequency=1, save_most_recent=False, zeroshot_frequency=1, val_frequency=1, resume=None, precision='amp', model='RN50', pretrained='', pretrained_image=False, lock_image=False, lock_image_unlocked_groups=0, lock_image_freeze_bn_stats=False, image_mean=None, image_std=None, aug_cfg={}, grad_checkpointing=False, local_loss=False, gather_with_grad=False, force_image_size=None, force_quick_gelu=False, force_patch_dropout=None, force_custom_text=False, torchscript=False, trace=False, accum_freq=1, dist_url='env://', dist_backend='nccl', report_to='', wandb_notes='', wandb_project_name='open-clip', debug=False, copy_codebase=False, horovod=False, ddp_static_graph=False, no_set_device_rank=False, seed=0, grad_clip_norm=None, lock_text=False, lock_text_unlocked_layers=0, lock_text_freeze_layer_norm=False, log_every_n_steps=100, coca_caption_loss_weight=2.0, coca_contrastive_loss_weight=1.0, remote_sync=None, remote_sync_frequency=300, remote_sync_protocol='s3', delete_previous_checkpoint=False, distill_model=None, distill_pretrained=None)"
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#prerequisites",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#prerequisites",
    "title": "The Annotated CLIP (Part-2)",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nAs part of this blog post, I am going to assume that the reader has a good understanding of the ResNet architecture (K. He et al. (2015)) and Vision Transformer (Dosovitskiy et al. (2020)).\nI am also going to assume that the reader also has a good basic understanding of CLIP after having read part-1 of this blog series.\nIf the reader would like a refresher on the ResNet architecture, please refer to the following video from paper reading group, that I hosted at Weights and Biases.\n\n\n\n\nAlso, the authors use Vision Transformer as one of the backbones in Image Encoder. For complete understanding of ViT with PyTorch code implementation, refer to my previous blog post (in collaboration with Dr Habib Bukhari) - Vision Transformer. We won’t be covering ViT architecture as part of this blog post.\nFor the text encoder, the authors used the GPT-2 architecture. I have previously covered the entirety of the model with PyTorch code implementation at The annotated GPT-2.\nNow, with prerequisites and introductions out of the way, let’s get started with the first item which is “Data download and preparation”."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#data-download-using-img2dataset-and-preparation-using-webdataset",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#data-download-using-img2dataset-and-preparation-using-webdataset",
    "title": "The Annotated CLIP (Part-2)",
    "section": "3 Data download using img2dataset and preparation using webdataset",
    "text": "3 Data download using img2dataset and preparation using webdataset\nBefore we can start training any models, we need data. In this part of the blog post we are looking at data preparation part of CLIP. Remember, that CLIP was trained on 400M (image, text) pairs.\nFrom the paper:\nWe create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision.\nSo, how does one create these image text pairs in practice? One of the easiest ways to train CLIP using Open CLIP is to generate the dataset in the form of webdataset using img2dataset.\nWe will only be creating a tiny version consisting of only 1,000 (image, text) and not the complete 400M dataset used in CLIP.\n\n\nCode\n%%time\n# !pip install img2dataset \nfrom img2dataset import download\nimport shutil\nimport os\n\noutput_dir = os.path.abspath(\"sample\")\n\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\n\ndownload(\n    processes_count=os.cpu_count(),\n    thread_count=os.cpu_count()*2,\n    url_list=\"/home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_1000.parquet\",\n    image_size=256,\n    output_folder=output_dir,\n    output_format=\"webdataset\",\n    input_format=\"parquet\",\n    url_col=\"URL\",\n    caption_col=\"TEXT\",\n    enable_wandb=False,\n    number_sample_per_shard=100, \n    distributor=\"multiprocessing\",\n)\n\n\nStarting the downloading of this file\nSharding file number 1 of 1 called /home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_1000.parquet\n\n\n0it [00:00, ?it/s]\n\n\n\nFile sharded in 10 shards\nDownloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n\n\n10it [00:31,  3.19s/it]\n\n\nworker  - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 12 - count: 100\ntotal   - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 12 - count: 100\nworker  - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 12 - count: 100\ntotal   - success: 0.845 - failed to download: 0.145 - failed to resize: 0.010 - images per sec: 23 - count: 200\nworker  - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 8 - count: 100\ntotal   - success: 0.847 - failed to download: 0.143 - failed to resize: 0.010 - images per sec: 23 - count: 300\nworker  - success: 0.850 - failed to download: 0.150 - failed to resize: 0.000 - images per sec: 9 - count: 100\ntotal   - success: 0.848 - failed to download: 0.145 - failed to resize: 0.007 - images per sec: 30 - count: 400\nworker  - success: 0.840 - failed to download: 0.160 - failed to resize: 0.000 - images per sec: 10 - count: 100\ntotal   - success: 0.846 - failed to download: 0.148 - failed to resize: 0.006 - images per sec: 38 - count: 500\nworker  - success: 0.830 - failed to download: 0.160 - failed to resize: 0.010 - images per sec: 10 - count: 100\ntotal   - success: 0.843 - failed to download: 0.150 - failed to resize: 0.007 - images per sec: 31 - count: 600\nworker  - success: 0.830 - failed to download: 0.150 - failed to resize: 0.020 - images per sec: 9 - count: 100\ntotal   - success: 0.841 - failed to download: 0.150 - failed to resize: 0.009 - images per sec: 35 - count: 700\nworker  - success: 0.880 - failed to download: 0.100 - failed to resize: 0.020 - images per sec: 6 - count: 100\ntotal   - success: 0.846 - failed to download: 0.144 - failed to resize: 0.010 - images per sec: 40 - count: 800\nworker  - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 4 - count: 100\ntotal   - success: 0.846 - failed to download: 0.144 - failed to resize: 0.010 - images per sec: 34 - count: 900\nworker  - success: 0.900 - failed to download: 0.100 - failed to resize: 0.000 - images per sec: 4 - count: 100\ntotal   - success: 0.851 - failed to download: 0.140 - failed to resize: 0.009 - images per sec: 38 - count: 1000\nCPU times: user 71.6 ms, sys: 51 ms, total: 123 ms\nWall time: 32.6 s\n\n\nSo it takes ~35 seconds to create the tiny dataset on my 8 core machine. Please refer to img2dataset for information on other available (image, text) pair datasets.\nBut, what do the downloads look like? Let’s find out.\n\n\nCode\nimport numpy as np\nfrom pathlib import Path\n\nnp.array(sorted(list(Path('./sample/').glob('*tar'))))\n\n\narray([PosixPath('sample/00000.tar'), PosixPath('sample/00001.tar'),\n       PosixPath('sample/00002.tar'), PosixPath('sample/00003.tar'),\n       PosixPath('sample/00004.tar'), PosixPath('sample/00005.tar'),\n       PosixPath('sample/00006.tar'), PosixPath('sample/00007.tar'),\n       PosixPath('sample/00008.tar'), PosixPath('sample/00009.tar')],\n      dtype=object)\n\n\nAs we can see above, the script from img2dataset downloads {00000...00009).tar files. What’s in these .tar files? Answer lies in the documentation of webdataset. I won’t be covering more details as part of this blog post as we have a lot to cover stil!\n\n\n\n\n\n\nFurther reading\n\n\n\nOne key thing that I haven’t covered as part of this blog post, is that how do these .tar files get converted to inputs to the CLIP model? Let me share briefly here and point to the right directions.\nFirst, a data pipeline is created using wds (webdataset) package. You can find the pipeline being created here.\nThis pipeline takes in a tokenizer that’s HFTokenizer, see here. This HFTokenizer tokenizes the input and returns input_ids of context_length = 77."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#training",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#training",
    "title": "The Annotated CLIP (Part-2)",
    "section": "4 Training",
    "text": "4 Training\nNow, to train a CLIP model of your choice on a single GPU, simply clone the Open Clip repository and run the following command in your terminal in the src/ directory:\npython -m training.main \\\n    --save-frequency 1 \\\n    --zeroshot-frequency 1 \\\n    --train-data=\"/home/ubuntu/GIT_REPOS/amaarora.github.io/posts/sample/{00000..00009}.tar\" \\\n    --warmup 10 \\\n    --batch-size=16 \\\n    --lr=1e-3 \\\n    --wd=0.1 \\\n    --epochs=30 \\\n    --workers=8 \\\n    --model RN50 \\\n    --train-num-samples 852\nThis should kick off training on your machine. Now, that we can train CLIP models on our own machines, let’s look at some of the details of training scrip and the CLIP architecture."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-img-encoder",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-img-encoder",
    "title": "The Annotated CLIP (Part-2)",
    "section": "6 Image Encoder",
    "text": "6 Image Encoder\nFrom the paper:\nWe consider two different architectures for the image encoder. For the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al. (2020)). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\n\n6.1 Modified ResNet\nLet’s start with the first architecture.\nFor the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” CoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\nThere are 3 major changes as mentioned to the ResNet architecture in CLIP:\n\nThere are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\nPerforms anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\nThe final pooling layer is a QKV attention instead of an average pool\n\n\n6.1.1 ResNet stem\nLet’s look at all of them one by one in code. First, we start with There are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\n\n\n\nFigure 3: Overview of ResNet architecture\n\n\nIn the vanilla ResNet architecture, the stem consists of a 7x7 stride-2 convolution. This is what the stem looks like in the vanilla ResNet architecture.\nclass VanillaResNet:\n    def __init__(...):\n        self.stem = nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)\nHowever, in the paper T. He et al. (2018), where at the time, the authors raised ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet. From the paper, one of the tweaks used in the architecture:\nA 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions.\n\n\n\nFigure 4: Overview of ResNet tweak\n\n\n\n\n\n\n\n\nWarning\n\n\n\nOn top of replacing \\(7x7\\) stride-2 convolution with 3 consecutive \\(3x3\\) convolutions, the authors also replaced max pooling with average pooling, but image above shows max pooling as it has been directly copied from T. He et al. (2018).\n\nHe, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. “Bag of Tricks for Image Classification with Convolutional Neural Networks.” CoRR abs/1812.01187. http://arxiv.org/abs/1812.01187.\n\n\nIn code this looks like:\nclass ModifiedResNet:\n    def __init__(...):\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n    \n    def forward(self, x):\n        x = self.stem(x)\n\n\n6.1.2 Blur Pool\nThe next change is to use BlurPooling - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1. This change has been adopted from Zhang (2019).\nIn this section I will introduce BlurPooling and share how it is implemented in the ModifiedResNet architecture.\n\n\nFrom the research paper,\nModern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling.\nBlur Pooling in CLIP has been implemented inside the Bottleneck block as below:\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n        self.stem = create_stem() # stem consists of 3 3x3 convs instead of 1 7x7 stride-2 conv\n        self.downsample = None\n        self.stride = stride\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n        out = self.stem()\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\nNow the blurring occurs in downsample. Previously, as can be seen in Figure 3, this downsample layer would be a stride-2 \\(1x1\\) convolution.\nIn ModifiedResnet, this downsample consists of:\nself.downsample = nn.Sequential(OrderedDict([\n        (\"-1\", nn.AvgPool2d(stride)),\n        (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n        (\"1\", nn.BatchNorm2d(planes * self.expansion))\n    ]))\nWhere, the convolution is stride-1.\nThe blurring occurs in nn.AvgPool2d(stride). How? See example below:\n\n\nCode\nimport torch.nn as nn \nfrom PIL import Image\nimport numpy as np \nimport torch\nimport matplotlib.pyplot as plt\n\npool = nn.AvgPool2d(kernel_size=2)\nimg  = np.array(Image.open('../images/clip.png'))\nx    = torch.tensor(img, dtype=torch.float64).permute(2, 0, 1)\nout  = pool(pool(x))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8))\nax[0].imshow(x.int().permute(1,2,0))\nax[0].set_title(\"Input image before average pooling\");\nax[1].imshow(out.int().permute(1,2,0))\nax[1].set_title(\"Input image after average pooling\");\n\n\n\n\n\nAs can be seen above, passing the input image through a nn.AvgPool2d blurs the image, and therefore, anti-aliases the input as per Zhang (2019). As a result, the resulting model is translation invariant.\n\nZhang, Richard. 2019. “Making Convolutional Networks Shift-Invariant Again.” CoRR abs/1904.11486. http://arxiv.org/abs/1904.11486.\n\n\n6.1.3 Final pooling layer\nThis brings us to the final change in ModifiedResnet.\nThe last change in the network architecture is to use QKV attention instead of an average pool. From the paper:\nWe also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\n\n\nCode\nimport torch\nimport torch.nn as nn\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x, key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n\n        return x[0]\n\n\nAs can be seen from the code above, we perform multi head self-attention from Vaswani et al. (2017), on x. One key thing to note above in the forward method is :\nx = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\nThis corresponds to “query is conditioned on the global average-pooled representation of the image” from the paper because the final output that is returned is x[0] and x[0] is global average pooled representation of the image.\n\n\n\n6.2 Modified ViT\nFrom the paper:\nFor the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al. (2020)). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” CoRR abs/2010.11929. https://arxiv.org/abs/2010.11929.\n\nSince the architecture is very similar to vanilla Vision Transformer, with a very minor change of adding LayerNorm after combining Patch embeddings and positional embeddings, I will not be covering the architecture in detail in this blog post.\nFor reference to ViT, please refer to my previous blog post that covers the architecture in detail with PyTorch code implementation - Vision Transformer\nHaving covered both Image encoders used in CLIP architecture, it is now time to move on to the text encoder."
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-contrastive-loss",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#sec-contrastive-loss",
    "title": "The Annotated CLIP (Part-2)",
    "section": "8 Contrastive Loss",
    "text": "8 Contrastive Loss\nOne thing that I was most interested in when I started to write the CLIP blog post was to look at Contrastive Loss and understand it in PyTorch code.\nIn this section, that is exactly what we will be doing.\nIf you remember from Section 6, the images get encoded as image features to shape torch.Size([16, 1024]) and texts get encoded to text features of shape torch.Size([16, 1024]).\nLet’s look at the inputs and outputs of ClipLoss before implementing ourselves.\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport numpy as np \nimport torch.nn as nn\nfrom open_clip.loss import ClipLoss\n\n\n\n\nCode\nimage_features = torch.randn(16, 1024)\ntext_features  = torch.randn(16, 1024)\nloss_fn        = ClipLoss()\nlogit_scale    = nn.Parameter(torch.tensor(np.log(1/0.07)))\nloss = loss_fn(image_features, text_features, logit_scale)\nloss\n\n\ntensor(134.1310, grad_fn=<DivBackward0>)\n\n\nOne thing you’ll notice is that the ClipLoss expects a parameter called logit_scale. Now, what is this logit_scale parameter?\nFrom the paper:\nThe learnable temperature parameter \\(τ\\) was initialized to the equivalent of 0.07 from (Veeling et al. (2018)) and clipped to prevent scaling the logits by more than 100 which we found necessary to prevent training instability.\nVeeling, Bastiaan S., Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. 2018. “Rotation Equivariant CNNs for Digital Pathology.” https://doi.org/10.48550/ARXIV.1806.03962.\n\nBut, rather than being initialised to \\(0.07\\), this temperature parameter \\(τ\\) get’s initialized as nn.Parameter(torch.tensor(np.log(1/0.07))). For further explanation, see this issue here.\nNow, having looked at the inputs and outputs and also logit_scale, it is time to look at the source code. Remember contrastive loss and what it does from part-1 of the blog post? As a quick revision, let me re-post the image here too.\n\n\n\nFigure 5: Cosine similarity between text and image features\n\n\nContrastive loss is trying to maximise the cosine similarity on the diagonal and minimise it elsewhere. But, how? In pseudo-code this looked something like:\n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss = (loss_i + loss_t)/2\nOnce, we have logits, which is the \\(8 x 8\\) matrix as in Figure 5 above, we calculate Cross Entropy Loss once for axis=0 and once for axis=1, this way, we are trying to match the diagonal to corresponding image and text because the labels are aligned on both the axis.\nBut, how does this look like in code? Let’s see.\n\n\n\n\n\n\nNote\n\n\n\nThe below implementation of ClipLoss is a minimalistic version of the complete implementation from open clip.\n\n\n\n\nCode\nclass ClipLoss(nn.Module):\n    def __init__(\n            self,\n    ):\n        super().__init__()\n\n    def get_ground_truth(self, device, num_logits) -> torch.Tensor:\n        labels = torch.arange(num_logits, device=device, dtype=torch.long)\n        return labels\n\n    def get_logits(self, image_features, text_features, logit_scale):\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logit_scale * text_features @ image_features.T        \n        return logits_per_image, logits_per_text\n\n    def forward(self, image_features, text_features, logit_scale, output_dict=False):\n        device = image_features.device\n        logits_per_image, logits_per_text = self.get_logits(image_features, text_features, logit_scale)\n        labels = self.get_ground_truth(device, logits_per_image.shape[0])\n        total_loss = (\n            F.cross_entropy(logits_per_image, labels) +\n            F.cross_entropy(logits_per_text, labels)\n        ) / 2\n        return {\"contrastive_loss\": total_loss}\n\n\n\n\nCode\nimage_features = torch.randn(16, 1024)\ntext_features  = torch.randn(16, 1024)\nloss_fn        = ClipLoss()\nlogit_scale    = nn.Parameter(torch.tensor(np.log(1/0.07)))\nloss = loss_fn(image_features, text_features, logit_scale)\nloss\n\n\n{'contrastive_loss': tensor(140.2225, grad_fn=<DivBackward0>)}\n\n\nSo, how does the above implementation match pseudo-code?\nLet’s start with labels. Since the labels are aligned, that is the \\(0th\\) image on axis=0 corresponds to \\(0th\\) text on axis=1, therefore, we can just say that labels = torch.arange(num_logits, device=device, dtype=torch.long). In this case the value of labels comes out to be tensor([ 0,  1,  2,  3,  4,  5,  6,  7], device='cuda:0') based on Figure 5. By minimising Cross Entropy loss for these labels once for axis=0 and once for axis=1, we are making sure that cosine-similarity on the diagonal is maximum and lower otherwise.\nIn code (as opposed to pseudo-code), rather than minimising cross entropy for axis=0, and axis=1, we calculate:\n    logits_per_image = logit_scale * image_features @ text_features.T\n    logits_per_text = logit_scale * text_features @ image_features.T    \nThis is same as calculating logits once for axis=1, and once for axis=0, therefore, our total loss is:\n    total_loss = (\n        F.cross_entropy(logits_per_image, labels) +\n        F.cross_entropy(logits_per_text, labels)\n    ) / 2\nThis is equivalent to pseudo code from paper:\n    labels = np.arange(n) \n    loss_i = cross_entropy_loss(logits, labels, axis=0) \n    loss_t = cross_entropy_loss(logits, labels, axis=1) \n    loss = (loss_i + loss_t)/2"
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#modified-resnet",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#modified-resnet",
    "title": "The Annotated CLIP (Part-2)",
    "section": "7 Modified ResNet",
    "text": "7 Modified ResNet\nLet’s start with the first architecture.\nFor the first, we use ResNet-50 (K. He et al. (2015)) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from T. He et al. (2018) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” CoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\nThere are 3 major changes as mentioned to the ResNet architecture in CLIP:\n\nThere are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\nPerforms anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\nThe final pooling layer is a QKV attention instead of an average pool\n\n\n7.0.1 ResNet stem\nLet’s look at all of them one by one in code. First, we start with There are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.\n\n\n\nFigure 2: Overview of ResNet architecture\n\n\nIn the vanilla ResNet architecture, the stem consists of a 7x7 stride-2 convolution. This is what the stem looks like in the vanilla ResNet architecture.\nclass VanillaResNet:\n    def __init__(...):\n        self.stem = nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)\nHowever, in the paper T. He et al. (2018), where at the time, the authors raised ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet. From the paper, one of the tweaks used in the architecture:\nA 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions.\n\n\n\nFigure 3: Overview of ResNet tweak\n\n\n\n\n\n\n\n\nWarning\n\n\n\nOn top of replacing \\(7x7\\) stride-2 convolution with 3 consecutive \\(3x3\\) convolutions, the authors also replaced max pooling with average pooling, but image above shows max pooling as it has been directly copied from T. He et al. (2018).\n\nHe, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. “Bag of Tricks for Image Classification with Convolutional Neural Networks.” CoRR abs/1812.01187. http://arxiv.org/abs/1812.01187.\n\n\nIn code this looks like:\nclass ModifiedResNet:\n    def __init__(...):\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n    \n    def forward(self, x):\n        x = self.stem(x)\n\n\n7.0.2 Blur Pool\nThe next change is to use BlurPooling - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1. This change has been adopted from Zhang (2019).\nIn this section I will introduce BlurPooling and share how it is implemented in the ModifiedResNet architecture.\n\n\nFrom the research paper,\nModern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling.\nBlur Pooling in CLIP has been implemented inside the Bottleneck block as below:\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n        self.stem = create_stem() # stem consists of 3 3x3 convs instead of 1 7x7 stride-2 conv\n        self.downsample = None\n        self.stride = stride\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n        out = self.stem()\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\nNow the blurring occurs in downsample. Previously, as can be seen in Figure 2, this downsample layer would be a stride-2 \\(1x1\\) convolution.\nIn ModifiedResnet, this downsample consists of:\nself.downsample = nn.Sequential(OrderedDict([\n        (\"-1\", nn.AvgPool2d(stride)),\n        (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n        (\"1\", nn.BatchNorm2d(planes * self.expansion))\n    ]))\nWhere, the convolution is stride-1.\nThe blurring occurs in nn.AvgPool2d(stride). How? See example below:\n\n\nCode\nimport torch.nn as nn \nfrom PIL import Image\nimport numpy as np \nimport torch\nimport matplotlib.pyplot as plt\n\npool = nn.AvgPool2d(kernel_size=2)\nimg  = np.array(Image.open('../images/clip.png'))\nx    = torch.tensor(img, dtype=torch.float64).permute(2, 0, 1)\nout  = pool(pool(x))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8))\nax[0].imshow(x.int().permute(1,2,0))\nax[0].set_title(\"Input image before average pooling\");\nax[1].imshow(out.int().permute(1,2,0))\nax[1].set_title(\"Input image after average pooling\");\n\n\n\n\n\nAs can be seen above, passing the input image through a nn.AvgPool2d blurs the image, and therefore, anti-aliases the input as per Zhang (2019). As a result, the resulting model is translation invariant.\n\nZhang, Richard. 2019. “Making Convolutional Networks Shift-Invariant Again.” CoRR abs/1904.11486. http://arxiv.org/abs/1904.11486.\n\n\n7.0.3 Final pooling layer\nThis brings us to the final change in ModifiedResnet.\nThe last change in the network architecture is to use QKV attention instead of an average pool. From the paper:\nWe also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.\n\n\nCode\nimport torch\nimport torch.nn as nn\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x, key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n\n        return x[0]\n\n\nAs can be seen from the code above, we perform multi head self-attention from Vaswani et al. (2017), on x. One key thing to note above in the forward method is :\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” CoRR abs/1706.03762. http://arxiv.org/abs/1706.03762.\nx = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\nThis corresponds to “query is conditioned on the global average-pooled representation of the image” from the paper because the final output that is returned is x[0] and x[0] is global average pooled representation of the image.\n\n\n7.1 Modified ViT\nFrom the paper:\nFor the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al. (2020)). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” CoRR abs/2010.11929. https://arxiv.org/abs/2010.11929.\n\nSince the architecture is very similar to vanilla Vision Transformer, with a very minor change of adding LayerNorm after combining Patch embeddings and positional embeddings, I will not be covering the architecture in detail in this blog post.\nFor reference to ViT, please refer to my previous blog post that covers the architecture in detail with PyTorch code implementation - Vision Transformer"
  },
  {
    "objectID": "posts/2023-03-11_Understanding_CLIP_part_2.html#conclusion",
    "href": "posts/2023-03-11_Understanding_CLIP_part_2.html#conclusion",
    "title": "The Annotated CLIP (Part-2)",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nAs part of this blog post I built upon part-1 of this blog series on CLIP.\nWe also briefly looked at data preparation as (image, text) pairs for CLIP training using img2dataset and data loading using webdataset packages.\nWe took a deep dive into the Image Encoder section and looked at all the three tweaks in ModifiedResnet compared to vanilla ResNet architecture.\nFinally, we also took a deep dive in contrastive loss and compared the actual PyTorch implementation with pseudo-code from the CLIP research paper.\nIf you enjoyed reading this blog post, please consider subscribing to my blog for more!"
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html",
    "href": "posts/2023-04-30_LaMini-LM.html",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "",
    "text": "Overview of LaMini-LM\n\n\nAs part of this blog post, we will be reviewing the LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions (Wu et al. (2023)) paper released on Apr 27, 2023.\nThe main objectives of this blog post are:\n\nTo provide a thorough review of LaMini-LM (Wu et al. (2023)) reseasrch paper.\nWe will try to replicate most figures and share commentary on why some of the results might me misleading. Specifically we replicate Figure 5 in Section 3.2.\nRecreate a small sample of the 2.58M shared Instruction Dataset to showcase the complete process - see Section 2.\nCompare similarity on generated instructions using sentence embeddings generated by using Sentence Transformer from HuggingFace.\nShare all code in PyTorch that is used as part of this blog post.\nCompare evaluation results from the paper with our own evaluation on multiple NLP datasets - see Section 6.\nWe do our own human evaluation in Section 6.1. Honestly speaking, for the three results, the results for the 1.5B GPT-2 LaMini model, the results were unsatisfactory. 😔"
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#dataset-generation",
    "href": "posts/2023-04-30_LaMini-LM.html#dataset-generation",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "2 Dataset Generation",
    "text": "2 Dataset Generation\n\n2.1 Introduction\nThe authors provide a large dataset of 2.58M instructions and responses.\nTwo examples in the LaMini-instruction data:\n{\n    \"instruction\": \"What is the capital of Australia?\\n\\nIs Canberra the only city chosen to serve as a capital due to a compromise between Sydney and Melbourne?\", \n    \"response\": \"The capital of Australia is Canberra. Yes, Canberra was chosen as the capital of Australia in 1908 due to a compromise between Sydney and Melbourne, as neither city wanted the other to become the capital.\",\n    \"instruction_source\": \"generated_flan\"\n}\n\n\n{\n    \"instruction\": \"Create a poem that has the word \\\"ocean\\\" in every other line.\", \n    \"response\": \"The ocean's depth is a mystery,\\nIts waves crashing with energy.\\nSalt air fills the coastal scene,\\nThe ocean's blue hues pure and clean.\\n\\nSeagulls cry out a melancholy song,\\nThe ocean's rhythm steady and strong.\\nSunsets paint the water gold,\\nThe ocean's secrets forever untold.\",\n    \"instruction_source\": \"self_instruct_without_topic\"\n}\nThe dataset is built on top of existing resources and also newer “instructions”+“responses”.\nFrom the paper:\n\nWe collate instructions from various prior datasets such as self-instruct (Wang et al. (2022)), P3 (Sanh et al. (2022)), FLAN (Longpre et al. (2023)) and Alpaca (Taori et al. (2023)).\n\nThe researchers have collated existing resources and also generated a new set “instructions+responses” using gpt-3.5-turbo (ChatGPT) using Self-Instruct approach (Wang et al. (2022)). At the time of writing I believe this is the biggest Instruction dataset available.\nBelow I provide an overview of the existing datasets that are part of the 2.58M LaMini-LM dataset:\n\nSelf-Instruct: Instruction, input, and output samples from a language model. (Wang et al. (2022))\nP3: P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks. Hosted at HuggingFace here.\nFLAN: Instruction dataset on a wide variety of datasets (473 datasets, 146 task categories, and 1,836 total tasks) using various instruction templates. Refer to the GitHub repo for more details.\nAlpaca: 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003. (Taori et al. (2023))\n\nThe authors use two strategies to generate instructions on top of existing ones which they called:\n\nExample-guided\nTopic-guided\n\nLet’s look at them in detail in the following sections.\n\n\n2.2 Example Guided\nExample guided generation follows Wang et al. (2022) & Taori et al. (2023).\nSpecifically, the authors include only few random examples, and some limited constraints as shown in the example prompt in Figure 1.\nNewer instructions are generated by providing these examples from existing datasets - Self-Instruct (\\(X_{SI}\\)), P3 (\\(X_{P3}\\)) & FLAN (\\(X_{FLAN}\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe number of in-context examples used for generation of \\(X_{SI}\\) is 3 whereas for \\(X_{P3}\\) & \\(X_{FLAN}\\) is 2. This is because the instructions in \\(X_{P3}\\) & \\(X_{FLAN}\\) are longer in length compared to \\(X_{SI}\\). This is due to token limits of ChatGPT.\n\n\n\n\n\nFigure 1: An example of instruction generation prompt based on three random examples from self-instruct\n\n\nTo generate your own instructions using ChatGPT, either paste the above prompt in ChatGPT, or we can use the openai API like so:\n\n\nCode\nimport openai\nopenai.api_key = \"sk_\" #Your API key goes here \n\nN = 20\nexamples = [\n    'What are some things you can do to de-stress?', \n    'How can individuals and organizations reduce unconscious bias?',\n    'Write a program to compute the sum of integers from k to n.'\n]\n\nprompt=f\"\"\"\n<example>{examples[0]}</example>\n<example>{examples[1]}</example>\n<example>{examples[2]}</example>\n\nGenerate {N} diverse examples that are similar to the provided examples.\nYou do not need to provide a response to the generated examples.\nEach example must include an instruction.\nEach generated instruction can be either an imperative sentence or a question.\nEach example must start with the label \"<example>\" and end with the label \"</example>\".\n\"\"\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=messages,\n    temperature=0, # not specified in the paper \n)\nresponse.choices[0].message[\"content\"]\n\n\nIn the above code, you can see how we can easily replace the examples list with a function that looks like - get_random_examples(n=3, subset='self-instruct') and based on that get example instructions from the existing datasets. By using different subsets, we can generate different examples.\nThe instructions that are generated by using examples from \\(X_{SI}\\), \\(X_{P3}\\) & \\(X_{FLAN}\\) are referred to as \\(\\hat{X}_{SI}\\), \\(\\hat{X}_{P3}\\) & \\(\\hat{X}_{FLAN}\\). So, the below 20 generated instructions would be part of \\(\\hat{X}_{SI}\\) because the 3 examples are from \\(X_{SI}\\).\nRunning the above returns an output that looks like:\n<example>What are some healthy breakfast options?</example>\n<example>How can you improve your public speaking skills?</example>\n<example>Write a recipe for a vegan lasagna.</example>\n<example>What are some ways to save money on groceries?</example>\n<example>How can you improve your time management skills?</example>\n<example>Write a workout plan for beginners.</example>\n<example>What are some tips for studying effectively?</example>\n<example>How can you improve your writing skills?</example>\n<example>Write a program to find the largest number in an array.</example>\n<example>What are some ways to improve your memory?</example>\n<example>How can you improve your interpersonal communication skills?</example>\n<example>Write a step-by-step guide for making a paper airplane.</example>\n<example>What are some ways to reduce your carbon footprint?</example>\n<example>How can you improve your problem-solving skills?</example>\n<example>Write a program to check if a number is prime.</example>\n<example>What are some ways to improve your creativity?</example>\n<example>How can you improve your leadership skills?</example>\n<example>Write a guide for making homemade soap.</example>\n<example>What are some healthy breakfast options?</example>\n<example>What are some ways to improve your emotional intelligence?</example>\n\n\n2.3 Topic Guided\nThe process and prompt for topic guided instruction generation is slightly different from example-guided instruction generation.\nThe overall process for topic-guided generation looks like:\n\nFind a list of common categories from Wikipidea (Total 3.5M)\nFilter out topics based on two rules.\n\nThe category must be less than three words.\nThe category must comprise more than 10 sub-categories and 50 pages.\n\nUse the prompt in Figure 2 and provide random examples from the same dataset and 3 topics obtained after filtering.\n\n\n\n\n\n\n\nNote\n\n\n\nAfter filtering, the authors obtain a list of 3.5K categories that serve as common topics.\n\n\n\n\n\nFigure 2: An example of instruction generation prompt based on three random examples from self-instruct and three random topics.\n\n\n\n\nCode\nimport openai\nopenai.api_key = \"sk_\" #Your API key goes here \n\nN = 20\nexamples = [\n    'Try coming up with a creative way to stay motivated during a workout.', \n    'In your opinion, what are the qualities of an effective sports coach?',\n    'Return the SSN number for the person: \"Yann LeCun\"'\n]\ntopics = ['Machine Learning', 'Infantry', 'Design bureaus']\n\nprompt=f\"\"\"\n<example>{examples[0]}</example>\n<example>{examples[1]}</example>\n<example>{examples[2]}</example>\n\nGenerate {N} diverse examples that are similar to the provided examples with the topics {topics[0]}, {topics[1]}, {topics[2]}\".\nYou do not need to provide a response to the generated examples. \nEach example must include an instruction. Each generated instruction can be either an imperative sentence or a question. \nEach example must start with the label \"<example>\" and end with the label \"</example>\".\".\"\"\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=messages,\n    temperature=0, # not specified in the paper \n)\nprint(response.choices[0].message[\"content\"])\n\n\nAs before, we can easily replace the examples list with a function that looks like - get_random_examples(n=3, subset='self-instruct') & also replace topics with a function that looks like - get_random_topics(n=3, subset='wiki-categories').\nRunning the above code returns an output that looks like:\n<example>What are some common machine learning algorithms and their applications?</example>\n<example>Design a new weapon for the infantry that is both effective and lightweight.</example>\n<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>\n<example>How can machine learning be used to improve healthcare outcomes?</example>\n<example>Create a workout plan for an infantry soldier to improve their endurance and strength.</example>\n<example>What are some key considerations when designing a user interface for a mobile app?</example>\n<example>Find a machine learning library that is compatible with Python.</example>\n<example>Develop a training program for infantry soldiers to improve their marksmanship skills.</example>\n<example>What are some ethical concerns surrounding the use of machine learning in decision-making?</example>\n<example>Design a new vehicle for the infantry that can navigate difficult terrain.</example>\n<example>Research and compare different design bureaus to find one that aligns with your project goals.</example>\n<example>How can machine learning be used to improve customer service in the retail industry?</example>\n<example>Create a nutrition plan for an infantry soldier to optimize their performance in the field.</example>\n<example>What are some best practices for designing a logo for a new brand?</example>\n<example>Implement a machine learning algorithm to predict customer churn for a telecommunications company.</example>\n<example>Develop a training program for infantry soldiers to improve their communication and teamwork skills.</example>\n<example>What are some challenges that arise when designing for virtual reality?</example>\n<example>Find a design bureau that specializes in creating interactive exhibits for museums.</example>\n<example>How can machine learning be used to improve fraud detection in the financial industry?</example>\n<example>Design a new piece of equipment for the infantry that can be used in urban environments.</example>\nSome key things to note just from the small sample above, instructions like\n\n“Design a new piece of equipment for the infantry that can be used in urban environments”\n“Research and compare different design bureaus to find one that aligns with your project goals”\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nare noisy. As also mentioned in the paper, ChatGPT has failed to provide enough context for the instructions.\n\n“Design a new piece of equipment for the infantry that can be used in urban environments”\n\nThe above instruction IMHO is very generic.\n\n“Research and compare different design bureaus to find one that aligns with your project goals”\n\nThe model has failed to define project goals or say anything about the “project”\n\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nThe model is asking to generate contact information, and it’s the response as we will see in the next section that’s more vague, not just the instruction.\n\n\n2.4 Response Generation\nLet’s collate the above instructions and generate responses for each one to create the resulting pairs. One could simply copy paste the instructions in ChatGPT or use the openAI API as before.\nLet’s take five instructions as examples to generate a .jsonl type dataset as below which can then be used to finetune models using the openAI API.\n\n\nCode\nimport openai\nfrom collections import defaultdict\n\nopenai.api_key = \"sk_\" #Your API key goes here \n\ndataset = defaultdict(dict)\n\ninstructions = [\n    \"<example>What are some common machine learning algorithms and their applications?</example>\",\n    \"<example>Design a new weapon for the infantry that is both effective and lightweight.</example>\",\n    \"<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>\",\n    \"<example>How can machine learning be used to improve healthcare outcomes?</example>\",\n    \"<example>Create a workout plan for an infantry soldier to improve their endurance and strength.</example>\",\n]\n\nfor idx, inst in enumerate(instructions):    \n    prompt = f\"\"\"Given the following instruction separated by `<example>`, generate a response.\n    Response must start with the label \"<response>\" and end with the label \"</response>\".\n    \n    Instruction: {inst}    \n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model='gpt-3.5-turbo',\n        messages=messages,\n        temperature=0, # not specified in the paper \n    )\n    dataset[idx] = {'prompt': inst, 'completion':response.choices[0].message[\"content\"]}\n\n\nRunning above code will give us a dataset that can be used to finetune the base models using OpenAI. This dataset looks something like:\ndefaultdict(dict,\n            {0: {'prompt': '<example>What are some common machine learning algorithms and their applications?</example>',\n              'completion': '<response>Some common machine learning algorithms and their applications include: \\n- Linear Regression: used for predicting numerical values\\n- Logistic Regression: used for classification problems\\n- Decision Trees: used for both classification and regression problems\\n- Random Forest: used for classification, regression, and feature selection\\n- Support Vector Machines: used for classification and regression problems\\n- K-Nearest Neighbors: used for classification and regression problems\\n- Neural Networks: used for complex problems such as image recognition and natural language processing</response>'},\n             1: {'prompt': '<example>Design a new weapon for the infantry that is both effective and lightweight.</example>',\n              'completion': '<response>A possible solution for a new weapon for the infantry that is both effective and lightweight could be a compact assault rifle that uses advanced materials and technology to reduce weight without sacrificing firepower. The rifle could have a modular design that allows for easy customization and upgrades, and could also incorporate features such as a suppressor and a holographic sight for improved accuracy. Additionally, the rifle could be designed to be easily disassembled and reassembled for maintenance and cleaning in the field.</response>'},\n             2: {'prompt': '<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>',\n              'completion': '<response>Here is the contact information for a design bureau specializing in sustainable architecture:</response>'},\n             3: {'prompt': '<example>How can machine learning be used to improve healthcare outcomes?</example>',\n              'completion': '<response>Machine learning can be used to improve healthcare outcomes by analyzing large amounts of patient data to identify patterns and predict potential health issues. This can lead to earlier diagnosis and treatment, as well as personalized treatment plans based on individual patient characteristics. Additionally, machine learning can help healthcare providers identify patients who are at risk for readmission or complications, allowing for targeted interventions to improve outcomes and reduce costs.</response>'},\n             4: {'prompt': '<example>Create a workout plan for an infantry soldier to improve their endurance and strength.</example>',\n              'completion': '<response>Here is a workout plan for an infantry soldier to improve their endurance and strength:</response>\\n- Warm up: 5-10 minutes of light cardio (jogging, jumping jacks, etc.)\\n- Circuit 1: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 push-ups\\n  - 20 squats\\n  - 20 lunges (10 per leg)\\n  - 1-minute plank\\n- Circuit 2: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 burpees\\n  - 20 jumping jacks\\n  - 20 mountain climbers (10 per leg)\\n  - 1-minute wall sit\\n- Circuit 3: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 dumbbell rows (10 per arm)\\n  - 20 dumbbell curls (10 per arm)\\n  - 20 dumbbell overhead presses (10 per arm)\\n  - 1-minute rest\\n- Cool down: 5-10 minutes of stretching and foam rolling.'}})\nFrom the smallest of examples, it appears as though:\n{\n    'prompt': '<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>',\n    'completion': '<response>Here is the contact information for a design bureau specializing in sustainable architecture:</response>'\n}\nis not of high quality. This small exercise indicates that there might be more noise in the 2.58M “instruction+response” dataset shared by the authors of LaMini-LM."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#dataset-exploration",
    "href": "posts/2023-04-30_LaMini-LM.html#dataset-exploration",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "3 Dataset Exploration",
    "text": "3 Dataset Exploration\nIn the last section I shared how the dataset generation looks like for LaMini-LM. In this section we will explore the 2.58M instruction dataset. The dataset has been shared publicly and is available on HuggingFace here.\n\n\n\nFigure 3: Dataset Preview on Huggingface.\n\n\n\n3.1 Statistics\nSome statistics about the dataset from the research paper have been shared in Figure 4 below.\n\n\n\nFigure 4: Data statistics of the generated dataset.\n\n\nAs can be seen aboce, in total there are 2.58M samples in LaMini-LM. It can be observed that the instructions for \\(D_{P3}\\) & \\(D_{FLAN}\\) are in general longer compared to the rest.\n\n\n\n\n\n\nNote\n\n\n\nThis was also mentioned in Section 2.2, and this is why authors used 2 in-context examples for \\({X_{P3}}\\) and \\(X_{FLAN}\\) compared to 3 in \\(X_{SI}\\).\n\n\n\n\n3.2 Diversity\nAs part of this section we will be looking at the diversity in the generated instructions. We will also try to recreate Figure 5 ourselves using sentence-transformers.\nThe authors took a sample of 50K instructions from \\({\\hat{X}_{SI}}\\), \\({\\hat{X}_{A}}\\), \\({\\hat{X}_{P3}}\\) & \\(X_{P3}\\) and visualised t-SNE of instruction sentence embeddings that were computed using Sentence Transformer.\nThe t-SNE figure has been shared below.\n\n\n\nFigure 5: The t-SNE visualizations of 50k sample of instruction sentence embeddings.\n\n\nSome comments about the the t-SNE visualisation directly from the paper:\n\nWe observe that \\(\\hat{X}_{SI}\\) exhibits greater diversity than \\(\\hat{X}_A\\) and \\(\\hat{X}_{P3}\\) is slightly more diverse than \\(X_{P3}\\).\n\nBut in no way does having a wider spread in \\(\\hat{X}_{SI}\\) and \\(\\hat{X}_{P3}\\) signify that the instructions are of higher quality. What if the instructions are meaningless?\nFor example one of the instructions from the small 20 instructions that were generated in Section 2.3 is:\n\nRetrieve the contact information for a design bureau specializing in sustainable architecture.\n\nAnd it’s not just the instruction, but rather the response too:\n{\n    'prompt': '<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>',\n    'completion': '<response>Here is the contact information for a design bureau specializing in sustainable architecture:</response>'\n}\nI think by training on such examples that might not be of high quality, we are allowing the model to hallucinate.\n\n\n\n\n\n\nHallucination\n\n\n\nWhen the model tries to answer questions it has no information about, the model is referred to be “hallucinating”.\n\n\n\n\nCode\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom umap import UMAP\nfrom tqdm.notebook import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import load_dataset, load_dataset_builder\n\n\n\n\nCode\nds_builder = load_dataset_builder(\"MBZUAI/LaMini-instruction\")\nds_builder.info.features\n\n\n{'instruction': Value(dtype='string', id=None),\n 'response': Value(dtype='string', id=None),\n 'instruction_source': Value(dtype='string', id=None)}\n\n\n\n\nCode\ndataset = load_dataset(\"MBZUAI/LaMini-instruction\")\ndataset\n\n\nFound cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['instruction', 'response', 'instruction_source'],\n        num_rows: 2585615\n    })\n})\n\n\nTotal of 2582019 samples in the dataset ➡️ 2.58M. Also, we have a column instruction_source that matches Dataset in Figure 4. First, we filter out the datasets based on source. We are trying to replicate Figure 5.\n\n\nCode\nX_alpaca = dataset.filter(lambda example: example[\"instruction_source\"]=='alpaca')\nX_p3     = dataset.filter(lambda example: example[\"instruction_source\"]=='original_p3')\nX_hat_si = dataset.filter(lambda example: example[\"instruction_source\"] in ['self_instruct_with_topic', 'self_instruct_without_topic'])\nX_hat_p3 = dataset.filter(lambda example: example[\"instruction_source\"]=='generated_p3')\nX_alpaca['train'].num_rows, X_p3['train'].num_rows, X_hat_si['train'].num_rows, X_hat_p3['train'].num_rows\n\n\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9195cf0efbc66452.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4d4436fd5c79b44c.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-990830c59dd517ae.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b3523be466a9a289.arrow\n\n\n(51985, 464396, 550137, 297312)\n\n\nNext, let’s keep the 50K sample from each source as per the research paper.\n\n\nCode\nsample_dict = {}\nfor X in tqdm([X_hat_si, X_hat_p3, X_alpaca, X_p3]):\n    np.random.seed(123)\n    idxs = np.random.choice(X['train'].num_rows, 50000)\n    sample_50k = X['train'][idxs]\n    src = np.unique(sample_50k['instruction_source'])[0]\n    if str(src).startswith('self_instruct'): src = 'generated_self_instruct'\n    sample_dict[src] = sample_50k['instruction']\ndf = pd.DataFrame(sample_dict)\ndf.head(2)\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      generated_self_instruct\n      generated_p3\n      alpaca\n      original_p3\n    \n  \n  \n    \n      0\n      How does tobacco use affect the cardiovascular...\n      What do you read in your free time?\\nRead a be...\n      Classify the types of data structures.\n      I know that the answer to the question \"What h...\n    \n    \n      1\n      Watch a sitcom and write down three humorous s...\n      Suppose a survey found that the majority of pa...\n      Determine how this example sentence illustrate...\n      The toddler became cranky. \\n\\nI am hesitating...\n    \n  \n\n\n\n\nNow that we have the 50K sample, we could just use sentence-transformer to create the embeddings. I have already done that using a GPU.\n\n\nCode\n# Convert to Sentence embeddings and then apply `UMAP` to get 2D projections\nif not os.path.exists('../assets/projections_alpaca.npy'):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    for col in tqdm(df.columns):\n        sentence_embeddings = model.encode(df[col], batch_size=256, show_progress_bar=True, device='cuda')\n        umap_2d = UMAP(random_state=0)\n        umap_2d.fit(sentence_embeddings)\n        projections = umap_2d.transform(sentence_embeddings)\n        np.save(f'../assets/projections_{col}.npy', projections)\n\n\nLet’s load the UMAP projections and store in a new DataFrame called df_proj.\n\n\nCode\ndf_proj = pd.DataFrame()\nfor col in df.columns:\n    projections = np.load(f'../assets/projections_{col}.npy')\n    _df = pd.DataFrame(projections, columns=[f'{col}_0', f'{col}_1'])\n    _df[col] = df[col]\n    df_proj = pd.concat([df_proj, _df], axis=1)\n\n\n\n\nCode\nax = sns.scatterplot(data=df_proj, x='generated_self_instruct_0', y='generated_self_instruct_1')\nsns.scatterplot(data=df_proj, x='alpaca_0', y='alpaca_1')\nax.set(xlabel='X', ylabel='Y');\nplt.legend(title='Dataset', loc='upper left', labels=['Self Instruct', 'Alpaca']);\n\n\n\n\n\n\n\n\n\n\n\nThank you authors!\n\n\n\nPreviously, the dataset shared on Huggingface did not contain instruction_source column, but the authors were really kind enough to add it.\n\n\nHi! Thanks for the feedback. You are correct, we'll update the HF repo data with a new column shortly.\n\n— Alham Fikri Aji ((AlhamFikri?)) April 30, 2023\n\n\n\n\n\n\n3.3 Human Evaluation\nFrom the paper:\nWe follow the human evaluation protocol given by Wang et al. (2022a), which categorizes the quality of the generated text into four levels:\n\nRate-A: The generated text is of high quality;\nRate-B: The generated text is acceptable but has minor errors;\nRate-C: The generated text has significant errors in content.\nRate-D: The generated text is completely unacceptable.\n\nWe randomly sample 20 examples from each subset of \\(D_{ALL}\\) and one of the co-authors scores the generated text.\nIn general, both the generated instructions and the generated responses are of high quality as shown in Figure 6.\n\n\n\nFigure 6: Human evaluation results for the generated instruction dataset.\n\n\nAs part of this blog post, let’s look at self_instruct_with_topic and perform human evaluation on 20 samples ourselves.\n\n\nCode\ndataset_si_with_topic = dataset.filter(lambda example: example[\"instruction_source\"] == 'self_instruct_with_topic')\ndataset_si_with_topic = dataset_si_with_topic.shuffle(seed=42)\nsample_20 = dataset_si_with_topic['train'][range(20)]\n\n\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d9007dd1dde13ff9.arrow\nLoading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d54e2b73deab01dc.arrow\n\n\nNow let’s score the 20 samples for self_instruct_with_topic, below, I used a simple IpyWidget that I created using ChatGPT. :)\n\n\nCode\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create widgets to display the current example\ninstruction_widget = widgets.HTML(layout=widgets.Layout(width='50%'))\nresponse_widget = widgets.HTML(layout=widgets.Layout(width='25%'))\nscore_widget = widgets.Dropdown(options=[('', 0), ('1', 1), ('2', 2), ('3', 3), ('4', 4)], layout=widgets.Layout(width='25%'))\n\n# Create a container for the example\nexample_container = widgets.HBox([instruction_widget, response_widget, score_widget])\n\n# Create buttons for navigation\nprevious_button = widgets.Button(description='Previous')\nnext_button = widgets.Button(description='Next')\nsubmit_button = widgets.Button(description='Submit')\nclear_button = widgets.Button(description='Clear')\n\n# Keep track of the current example index\ncurrent_index = 0\n\n# Initialize a list to store the scores\nscores = [0] * len(sample_20['instruction'])\n\ndef update_example(index):\n    instruction_widget.value = sample_20['instruction'][index]\n    response_widget.value = sample_20['response'][index]\n    score_widget.value = scores[index]\n\ndef on_previous(button):\n    global current_index\n    scores[current_index] = score_widget.value\n    current_index = max(0, current_index - 1)\n    update_example(current_index)\n\ndef on_next(button):\n    global current_index\n    scores[current_index] = score_widget.value\n    current_index = min(len(sample_20['instruction']) - 1, current_index + 1)\n    update_example(current_index)\n\ndef on_submit(button):\n    scores[current_index] = score_widget.value\n    print('Scores:', scores)\n\ndef on_clear(button):\n    scores[current_index] = 0\n    score_widget.value = 0\n\n# Set button callbacks\nprevious_button.on_click(on_previous)\nnext_button.on_click(on_next)\nsubmit_button.on_click(on_submit)\nclear_button.on_click(on_clear)\n\n# Display the example container and navigation buttons\ndisplay(example_container)\ndisplay(widgets.HBox([previous_button, next_button]))\ndisplay(widgets.HBox([submit_button, clear_button]))\n\n# Initialize the first example\nupdate_example(current_index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: IpyWidget for scoring instructions & responses\n\n\nYou can see how hard it is to score even 20 samples each. Scoring is an intensive task especially when it is about topics that the labeler has no idea about. Above, the instruction is “What are some behavioral patterns exhibited by Zygaenoidea moths?”.\nAs a labeler, I have no idea what “Zygaenoidea moths” are, let alone know their characterstics. I had to search for “Zygaenoidea moths” on google, and that linked me to scholarly articles.\nThrough this simple exercise, I hope I have showcased how difficult it can be to rate responses generated by the LLM."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-data-gen",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-data-gen",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "2 Dataset Generation",
    "text": "2 Dataset Generation\n\n2.1 Introduction\nThe authors provide a large dataset of 2.58M instructions and responses.\nTwo examples in the LaMini-instruction data:\n{\n    \"instruction\": \"What is the capital of Australia?\\n\\nIs Canberra the only city chosen to serve as a capital due to a compromise between Sydney and Melbourne?\", \n    \"response\": \"The capital of Australia is Canberra. Yes, Canberra was chosen as the capital of Australia in 1908 due to a compromise between Sydney and Melbourne, as neither city wanted the other to become the capital.\",\n    \"instruction_source\": \"generated_flan\"\n}\n\n\n{\n    \"instruction\": \"Create a poem that has the word \\\"ocean\\\" in every other line.\", \n    \"response\": \"The ocean's depth is a mystery,\\nIts waves crashing with energy.\\nSalt air fills the coastal scene,\\nThe ocean's blue hues pure and clean.\\n\\nSeagulls cry out a melancholy song,\\nThe ocean's rhythm steady and strong.\\nSunsets paint the water gold,\\nThe ocean's secrets forever untold.\",\n    \"instruction_source\": \"self_instruct_without_topic\"\n}\nThe dataset is built on top of existing resources and also newer “instructions”+“responses”.\nFrom the paper:\n\nWe collate instructions from various prior datasets such as self-instruct (Wang et al. (2022)), P3 (Sanh et al. (2022)), FLAN (Longpre et al. (2023)) and Alpaca (Taori et al. (2023)).\n\nThe researchers have collated existing resources and also generated a new set “instructions+responses” using gpt-3.5-turbo (ChatGPT) using Self-Instruct approach (Wang et al. (2022)). At the time of writing I believe this is the biggest Instruction dataset available.\nBelow I provide an overview of the existing datasets that are part of the 2.58M LaMini-LM dataset:\n\nSelf-Instruct: Instruction, input, and output samples from a language model. (Wang et al. (2022))\nP3: P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks. Hosted at HuggingFace here.\nFLAN: Instruction dataset on a wide variety of datasets (473 datasets, 146 task categories, and 1,836 total tasks) using various instruction templates. Refer to the GitHub repo for more details.\nAlpaca: 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003. (Taori et al. (2023))\n\nThe authors use two strategies to generate instructions on top of existing ones which they called:\n\nExample-guided\nTopic-guided\n\nLet’s look at them in detail in the following sections.\n\n\n2.2 Example Guided\nExample guided generation follows Wang et al. (2022) & Taori et al. (2023).\nSpecifically, the authors include only few random examples, and some limited constraints as shown in the example prompt in Figure 1.\nNewer instructions are generated by providing these examples from existing datasets - Self-Instruct (\\(X_{SI}\\)), P3 (\\(X_{P3}\\)) & FLAN (\\(X_{FLAN}\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe number of in-context examples used for generation of \\(X_{SI}\\) is 3 whereas for \\(X_{P3}\\) & \\(X_{FLAN}\\) is 2. This is because the instructions in \\(X_{P3}\\) & \\(X_{FLAN}\\) are longer in length compared to \\(X_{SI}\\). This is due to token limits of ChatGPT.\n\n\n\n\n\nFigure 1: An example of instruction generation prompt based on three random examples from self-instruct\n\n\nTo generate your own instructions using ChatGPT, either paste the above prompt in ChatGPT, or we can use the openai API like so:\n\n\nCode\nimport openai\nopenai.api_key = \"sk_\" #Your API key goes here \n\nN = 20\nexamples = [\n    'What are some things you can do to de-stress?', \n    'How can individuals and organizations reduce unconscious bias?',\n    'Write a program to compute the sum of integers from k to n.'\n]\n\nprompt=f\"\"\"\n<example>{examples[0]}</example>\n<example>{examples[1]}</example>\n<example>{examples[2]}</example>\n\nGenerate {N} diverse examples that are similar to the provided examples.\nYou do not need to provide a response to the generated examples.\nEach example must include an instruction.\nEach generated instruction can be either an imperative sentence or a question.\nEach example must start with the label \"<example>\" and end with the label \"</example>\".\n\"\"\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=messages,\n    temperature=0, # not specified in the paper \n)\nresponse.choices[0].message[\"content\"]\n\n\nIn the above code, you can see how we can easily replace the examples list with a function that looks like - get_random_examples(n=3, subset='self-instruct') and based on that get example instructions from the existing datasets. By using different subsets, we can generate different examples.\nThe instructions that are generated by using examples from \\(X_{SI}\\), \\(X_{P3}\\) & \\(X_{FLAN}\\) are referred to as \\(\\hat{X}_{SI}\\), \\(\\hat{X}_{P3}\\) & \\(\\hat{X}_{FLAN}\\). So, the below 20 generated instructions would be part of \\(\\hat{X}_{SI}\\) because the 3 examples are from \\(X_{SI}\\).\nRunning the above returns an output that looks like:\n<example>What are some healthy breakfast options?</example>\n<example>How can you improve your public speaking skills?</example>\n<example>Write a recipe for a vegan lasagna.</example>\n<example>What are some ways to save money on groceries?</example>\n<example>How can you improve your time management skills?</example>\n<example>Write a workout plan for beginners.</example>\n<example>What are some tips for studying effectively?</example>\n<example>How can you improve your writing skills?</example>\n<example>Write a program to find the largest number in an array.</example>\n<example>What are some ways to improve your memory?</example>\n<example>How can you improve your interpersonal communication skills?</example>\n<example>Write a step-by-step guide for making a paper airplane.</example>\n<example>What are some ways to reduce your carbon footprint?</example>\n<example>How can you improve your problem-solving skills?</example>\n<example>Write a program to check if a number is prime.</example>\n<example>What are some ways to improve your creativity?</example>\n<example>How can you improve your leadership skills?</example>\n<example>Write a guide for making homemade soap.</example>\n<example>What are some healthy breakfast options?</example>\n<example>What are some ways to improve your emotional intelligence?</example>\n\n\n2.3 Topic Guided\nThe process and prompt for topic guided instruction generation is slightly different from example-guided instruction generation.\nThe overall process for topic-guided generation looks like:\n\nFind a list of common categories from Wikipidea (Total 3.5M)\nFilter out topics based on two rules.\n\nThe category must be less than three words.\nThe category must comprise more than 10 sub-categories and 50 pages.\n\nUse the prompt in Figure 2 and provide random examples from the same dataset and 3 topics obtained after filtering.\n\n\n\n\n\n\n\nNote\n\n\n\nAfter filtering, the authors obtain a list of 3.5K categories that serve as common topics.\n\n\n\n\n\nFigure 2: An example of instruction generation prompt based on three random examples from self-instruct and three random topics.\n\n\n\n\nCode\nimport openai\nopenai.api_key = \"sk_\" #Your API key goes here \n\nN = 20\nexamples = [\n    'Try coming up with a creative way to stay motivated during a workout.', \n    'In your opinion, what are the qualities of an effective sports coach?',\n    'Return the SSN number for the person: \"Yann LeCun\"'\n]\ntopics = ['Machine Learning', 'Infantry', 'Design bureaus']\n\nprompt=f\"\"\"\n<example>{examples[0]}</example>\n<example>{examples[1]}</example>\n<example>{examples[2]}</example>\n\nGenerate {N} diverse examples that are similar to the provided examples with the topics {topics[0]}, {topics[1]}, {topics[2]}\".\nYou do not need to provide a response to the generated examples. \nEach example must include an instruction. Each generated instruction can be either an imperative sentence or a question. \nEach example must start with the label \"<example>\" and end with the label \"</example>\".\".\"\"\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo',\n    messages=messages,\n    temperature=0, # not specified in the paper \n)\nprint(response.choices[0].message[\"content\"])\n\n\nAs before, we can easily replace the examples list with a function that looks like - get_random_examples(n=3, subset='self-instruct') & also replace topics with a function that looks like - get_random_topics(n=3, subset='wiki-categories').\nRunning the above code returns an output that looks like:\n<example>What are some common machine learning algorithms and their applications?</example>\n<example>Design a new weapon for the infantry that is both effective and lightweight.</example>\n<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>\n<example>How can machine learning be used to improve healthcare outcomes?</example>\n<example>Create a workout plan for an infantry soldier to improve their endurance and strength.</example>\n<example>What are some key considerations when designing a user interface for a mobile app?</example>\n<example>Find a machine learning library that is compatible with Python.</example>\n<example>Develop a training program for infantry soldiers to improve their marksmanship skills.</example>\n<example>What are some ethical concerns surrounding the use of machine learning in decision-making?</example>\n<example>Design a new vehicle for the infantry that can navigate difficult terrain.</example>\n<example>Research and compare different design bureaus to find one that aligns with your project goals.</example>\n<example>How can machine learning be used to improve customer service in the retail industry?</example>\n<example>Create a nutrition plan for an infantry soldier to optimize their performance in the field.</example>\n<example>What are some best practices for designing a logo for a new brand?</example>\n<example>Implement a machine learning algorithm to predict customer churn for a telecommunications company.</example>\n<example>Develop a training program for infantry soldiers to improve their communication and teamwork skills.</example>\n<example>What are some challenges that arise when designing for virtual reality?</example>\n<example>Find a design bureau that specializes in creating interactive exhibits for museums.</example>\n<example>How can machine learning be used to improve fraud detection in the financial industry?</example>\n<example>Design a new piece of equipment for the infantry that can be used in urban environments.</example>\nSome key things to note just from the small sample above, instructions like\n\n“Design a new piece of equipment for the infantry that can be used in urban environments”\n“Research and compare different design bureaus to find one that aligns with your project goals”\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nare noisy. As also mentioned in the paper, ChatGPT has failed to provide enough context for the instructions.\n\n“Design a new piece of equipment for the infantry that can be used in urban environments”\n\nThe above instruction IMHO is very generic.\n\n“Research and compare different design bureaus to find one that aligns with your project goals”\n\nThe model has failed to define project goals or say anything about the “project”\n\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nThe model is asking to generate contact information, and it’s the response as we will see in the next section that’s more vague, not just the instruction.\n\n\n2.4 Response Generation\nLet’s collate the above instructions and generate responses for each one to create the resulting pairs. One could simply copy paste the instructions in ChatGPT or use the openAI API as before.\nLet’s take five instructions as examples to generate a .jsonl type dataset as below which can then be used to finetune models using the openAI API.\n\n\nCode\nimport openai\nfrom collections import defaultdict\n\nopenai.api_key = \"sk_\" #Your API key goes here \n\ndataset = defaultdict(dict)\n\ninstructions = [\n    \"<example>What are some common machine learning algorithms and their applications?</example>\",\n    \"<example>Design a new weapon for the infantry that is both effective and lightweight.</example>\",\n    \"<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>\",\n    \"<example>How can machine learning be used to improve healthcare outcomes?</example>\",\n    \"<example>Create a workout plan for an infantry soldier to improve their endurance and strength.</example>\",\n]\n\nfor idx, inst in enumerate(instructions):    \n    prompt = f\"\"\"Given the following instruction separated by `<example>`, generate a response.\n    Response must start with the label \"<response>\" and end with the label \"</response>\".\n    \n    Instruction: {inst}    \n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model='gpt-3.5-turbo',\n        messages=messages,\n        temperature=0, # not specified in the paper \n    )\n    dataset[idx] = {'prompt': inst, 'completion':response.choices[0].message[\"content\"]}\n\n\nRunning above code will give us a dataset that can be used to finetune the base models using OpenAI. This dataset looks something like:\ndefaultdict(dict,\n            {0: {'prompt': '<example>What are some common machine learning algorithms and their applications?</example>',\n              'completion': '<response>Some common machine learning algorithms and their applications include: \\n- Linear Regression: used for predicting numerical values\\n- Logistic Regression: used for classification problems\\n- Decision Trees: used for both classification and regression problems\\n- Random Forest: used for classification, regression, and feature selection\\n- Support Vector Machines: used for classification and regression problems\\n- K-Nearest Neighbors: used for classification and regression problems\\n- Neural Networks: used for complex problems such as image recognition and natural language processing</response>'},\n             1: {'prompt': '<example>Design a new weapon for the infantry that is both effective and lightweight.</example>',\n              'completion': '<response>A possible solution for a new weapon for the infantry that is both effective and lightweight could be a compact assault rifle that uses advanced materials and technology to reduce weight without sacrificing firepower. The rifle could have a modular design that allows for easy customization and upgrades, and could also incorporate features such as a suppressor and a holographic sight for improved accuracy. Additionally, the rifle could be designed to be easily disassembled and reassembled for maintenance and cleaning in the field.</response>'},\n             2: {'prompt': '<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>',\n              'completion': '<response>Here is the contact information for a design bureau specializing in sustainable architecture:</response>'},\n             3: {'prompt': '<example>How can machine learning be used to improve healthcare outcomes?</example>',\n              'completion': '<response>Machine learning can be used to improve healthcare outcomes by analyzing large amounts of patient data to identify patterns and predict potential health issues. This can lead to earlier diagnosis and treatment, as well as personalized treatment plans based on individual patient characteristics. Additionally, machine learning can help healthcare providers identify patients who are at risk for readmission or complications, allowing for targeted interventions to improve outcomes and reduce costs.</response>'},\n             4: {'prompt': '<example>Create a workout plan for an infantry soldier to improve their endurance and strength.</example>',\n              'completion': '<response>Here is a workout plan for an infantry soldier to improve their endurance and strength:</response>\\n- Warm up: 5-10 minutes of light cardio (jogging, jumping jacks, etc.)\\n- Circuit 1: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 push-ups\\n  - 20 squats\\n  - 20 lunges (10 per leg)\\n  - 1-minute plank\\n- Circuit 2: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 burpees\\n  - 20 jumping jacks\\n  - 20 mountain climbers (10 per leg)\\n  - 1-minute wall sit\\n- Circuit 3: 3 rounds of the following exercises with minimal rest in between:\\n  - 20 dumbbell rows (10 per arm)\\n  - 20 dumbbell curls (10 per arm)\\n  - 20 dumbbell overhead presses (10 per arm)\\n  - 1-minute rest\\n- Cool down: 5-10 minutes of stretching and foam rolling.'}})\nFrom the smallest of examples, it appears as though:\n{\n    'prompt': '<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>',\n    'completion': '<response>Here is the contact information for a design bureau specializing in sustainable architecture:</response>'\n}\nis not of high quality. This small exercise indicates that there might be more noise in the 2.58M “instruction+response” dataset shared by the authors of LaMini-LM."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-dataset-review",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-dataset-review",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "4 Dataset Review",
    "text": "4 Dataset Review\nAs part of Section 2 and Section 3, by calling the OpenaiAPI ourselves, we saw that there might be noise in the dataset.\ngpt-3.5-turbo fails to provide context in some of the instructions that we saw before like:\n\n“Research and compare different design bureaus to find one that aligns with your project goals”\n“Retrieve the contact information for a design bureau specializing in sustainable architecture.”\n\nThis means that there is possibility there is noise in the dataset. It is harder to look at text and figure out noise and clean datasets, IMHO, this is an open research question and I will try to work on this in my next blog post.\nAlso, from the simple exercise, we saw how hard it can be to label Instruction and Response. There is no direct way, if the labeler doesn’t have knowledge about the topic, then the task becomes even more intensive."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-data-exploration",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-data-exploration",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "3 Dataset Exploration",
    "text": "3 Dataset Exploration\nIn the last section I shared how the dataset generation looks like for LaMini-LM. In this section we will explore the 2.58M instruction dataset. The dataset has been shared publicly and is available on HuggingFace here.\n\n\n\nFigure 3: Dataset Preview on Huggingface.\n\n\n\n3.1 Statistics\nSome statistics about the dataset from the research paper have been shared in Figure 4 below.\n\n\n\nFigure 4: Data statistics of the generated dataset.\n\n\nAs can be seen aboce, in total there are 2.58M samples in LaMini-LM. It can be observed that the instructions for \\(D_{P3}\\) & \\(D_{FLAN}\\) are in general longer compared to the rest.\n\n\n\n\n\n\nNote\n\n\n\nThis was also mentioned in Section 2.2, and this is why authors used 2 in-context examples for \\({X_{P3}}\\) and \\(X_{FLAN}\\) compared to 3 in \\(X_{SI}\\).\n\n\n\n\n3.2 Diversity\nAs part of this section we will be looking at the diversity in the generated instructions. We will also try to recreate Figure 5 ourselves using sentence-transformers.\nThe authors took a sample of 50K instructions from \\({\\hat{X}_{SI}}\\), \\({\\hat{X}_{A}}\\), \\({\\hat{X}_{P3}}\\) & \\(X_{P3}\\) and visualised t-SNE of instruction sentence embeddings that were computed using Sentence Transformer.\nThe t-SNE figure has been shared below.\n\n\n\nFigure 5: The t-SNE visualizations of 50k sample of instruction sentence embeddings.\n\n\nSome comments about the the t-SNE visualisation directly from the paper:\n\nWe observe that \\(\\hat{X}_{SI}\\) exhibits greater diversity than \\(\\hat{X}_A\\) and \\(\\hat{X}_{P3}\\) is slightly more diverse than \\(X_{P3}\\).\n\nBut in no way does having a wider spread in \\(\\hat{X}_{SI}\\) and \\(\\hat{X}_{P3}\\) signify that the instructions are of higher quality. What if the instructions are meaningless?\nFor example one of the instructions from the small 20 instructions that were generated in Section 2.3 is:\n\nRetrieve the contact information for a design bureau specializing in sustainable architecture.\n\nAnd it’s not just the instruction, but rather the response too:\n{\n    'prompt': '<example>Retrieve the contact information for a design bureau specializing in sustainable architecture.</example>',\n    'completion': '<response>Here is the contact information for a design bureau specializing in sustainable architecture:</response>'\n}\nI think by training on such examples that might not be of high quality, we are allowing the model to hallucinate.\n\n\n\n\n\n\nHallucination\n\n\n\nWhen the model tries to answer questions it has no information about, the model is referred to be “hallucinating”.\n\n\n\n\nCode\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom umap import UMAP\nfrom tqdm.notebook import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import load_dataset, load_dataset_builder\n\n\n\n\nCode\nds_builder = load_dataset_builder(\"MBZUAI/LaMini-instruction\")\nds_builder.info.features\n\n\n{'instruction': Value(dtype='string', id=None),\n 'response': Value(dtype='string', id=None),\n 'instruction_source': Value(dtype='string', id=None)}\n\n\n\n\nCode\ndataset = load_dataset(\"MBZUAI/LaMini-instruction\")\ndataset\n\n\nFound cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['instruction', 'response', 'instruction_source'],\n        num_rows: 2585615\n    })\n})\n\n\nTotal of 2582019 samples in the dataset ➡️ 2.58M. Also, we have a column instruction_source that matches Dataset in Figure 4. First, we filter out the datasets based on source. We are trying to replicate Figure 5.\n\n\nCode\nX_alpaca = dataset.filter(lambda example: example[\"instruction_source\"]=='alpaca')\nX_p3     = dataset.filter(lambda example: example[\"instruction_source\"]=='original_p3')\nX_hat_si = dataset.filter(lambda example: example[\"instruction_source\"] in ['self_instruct_with_topic', 'self_instruct_without_topic'])\nX_hat_p3 = dataset.filter(lambda example: example[\"instruction_source\"]=='generated_p3')\nX_alpaca['train'].num_rows, X_p3['train'].num_rows, X_hat_si['train'].num_rows, X_hat_p3['train'].num_rows\n\n\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9195cf0efbc66452.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4d4436fd5c79b44c.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-990830c59dd517ae.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b3523be466a9a289.arrow\n\n\n(51985, 464396, 550137, 297312)\n\n\nNext, let’s keep the 50K sample from each source as per the research paper.\n\n\nCode\nsample_dict = {}\nfor X in tqdm([X_hat_si, X_hat_p3, X_alpaca, X_p3]):\n    np.random.seed(123)\n    idxs = np.random.choice(X['train'].num_rows, 50000)\n    sample_50k = X['train'][idxs]\n    src = np.unique(sample_50k['instruction_source'])[0]\n    if str(src).startswith('self_instruct'): src = 'generated_self_instruct'\n    sample_dict[src] = sample_50k['instruction']\ndf = pd.DataFrame(sample_dict)\ndf.head(2)\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      generated_self_instruct\n      generated_p3\n      alpaca\n      original_p3\n    \n  \n  \n    \n      0\n      How does tobacco use affect the cardiovascular...\n      What do you read in your free time?\\nRead a be...\n      Classify the types of data structures.\n      I know that the answer to the question \"What h...\n    \n    \n      1\n      Watch a sitcom and write down three humorous s...\n      Suppose a survey found that the majority of pa...\n      Determine how this example sentence illustrate...\n      The toddler became cranky. \\n\\nI am hesitating...\n    \n  \n\n\n\n\nNow that we have the 50K sample, we could just use sentence-transformer to create the embeddings. I have already done that using a GPU.\n\n\nCode\n# Convert to Sentence embeddings and then apply `UMAP` to get 2D projections\nif not os.path.exists('../assets/projections_alpaca.npy'):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    for col in tqdm(df.columns):\n        sentence_embeddings = model.encode(df[col], batch_size=256, show_progress_bar=True, device='cuda')\n        umap_2d = UMAP(random_state=0)\n        umap_2d.fit(sentence_embeddings)\n        projections = umap_2d.transform(sentence_embeddings)\n        np.save(f'../assets/projections_{col}.npy', projections)\n\n\nLet’s load the UMAP projections and store in a new DataFrame called df_proj.\n\n\nCode\ndf_proj = pd.DataFrame()\nfor col in df.columns:\n    projections = np.load(f'../assets/projections_{col}.npy')\n    _df = pd.DataFrame(projections, columns=[f'{col}_0', f'{col}_1'])\n    _df[col] = df[col]\n    df_proj = pd.concat([df_proj, _df], axis=1)\n\n\n\n\nCode\nax = sns.scatterplot(data=df_proj, x='generated_self_instruct_0', y='generated_self_instruct_1')\nsns.scatterplot(data=df_proj, x='alpaca_0', y='alpaca_1')\nax.set(xlabel='X', ylabel='Y');\nplt.legend(title='Dataset', loc='upper left', labels=['Self Instruct', 'Alpaca']);\n\n\n\n\n\n\n\n\n\n\n\nThank you authors!\n\n\n\nPreviously, the dataset shared on Huggingface did not contain instruction_source column, but the authors were really kind enough to add it.\n\n\nHi! Thanks for the feedback. You are correct, we'll update the HF repo data with a new column shortly.\n\n— Alham Fikri Aji ((AlhamFikri?)) April 30, 2023\n\n\n\n\n\n\n3.3 Human Evaluation\nFrom the paper:\nWe follow the human evaluation protocol given by Wang et al. (2022a), which categorizes the quality of the generated text into four levels:\n\nRate-A: The generated text is of high quality;\nRate-B: The generated text is acceptable but has minor errors;\nRate-C: The generated text has significant errors in content.\nRate-D: The generated text is completely unacceptable.\n\nWe randomly sample 20 examples from each subset of \\(D_{ALL}\\) and one of the co-authors scores the generated text.\nIn general, both the generated instructions and the generated responses are of high quality as shown in Figure 6.\n\n\n\nFigure 6: Human evaluation results for the generated instruction dataset.\n\n\nAs part of this blog post, let’s look at self_instruct_with_topic and perform human evaluation on 20 samples ourselves.\n\n\nCode\ndataset_si_with_topic = dataset.filter(lambda example: example[\"instruction_source\"] == 'self_instruct_with_topic')\ndataset_si_with_topic = dataset_si_with_topic.shuffle(seed=42)\nsample_20 = dataset_si_with_topic['train'][range(20)]\n\n\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d9007dd1dde13ff9.arrow\nLoading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/MBZUAI___parquet/default-3bf051cc03b2354d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d54e2b73deab01dc.arrow\n\n\nNow let’s score the 20 samples for self_instruct_with_topic, below, I used a simple IpyWidget that I created using ChatGPT. :)\n\n\nCode\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create widgets to display the current example\ninstruction_widget = widgets.HTML(layout=widgets.Layout(width='50%'))\nresponse_widget = widgets.HTML(layout=widgets.Layout(width='25%'))\nscore_widget = widgets.Dropdown(options=[('', 0), ('1', 1), ('2', 2), ('3', 3), ('4', 4)], layout=widgets.Layout(width='25%'))\n\n# Create a container for the example\nexample_container = widgets.HBox([instruction_widget, response_widget, score_widget])\n\n# Create buttons for navigation\nprevious_button = widgets.Button(description='Previous')\nnext_button = widgets.Button(description='Next')\nsubmit_button = widgets.Button(description='Submit')\nclear_button = widgets.Button(description='Clear')\n\n# Keep track of the current example index\ncurrent_index = 0\n\n# Initialize a list to store the scores\nscores = [0] * len(sample_20['instruction'])\n\ndef update_example(index):\n    instruction_widget.value = sample_20['instruction'][index]\n    response_widget.value = sample_20['response'][index]\n    score_widget.value = scores[index]\n\ndef on_previous(button):\n    global current_index\n    scores[current_index] = score_widget.value\n    current_index = max(0, current_index - 1)\n    update_example(current_index)\n\ndef on_next(button):\n    global current_index\n    scores[current_index] = score_widget.value\n    current_index = min(len(sample_20['instruction']) - 1, current_index + 1)\n    update_example(current_index)\n\ndef on_submit(button):\n    scores[current_index] = score_widget.value\n    print('Scores:', scores)\n\ndef on_clear(button):\n    scores[current_index] = 0\n    score_widget.value = 0\n\n# Set button callbacks\nprevious_button.on_click(on_previous)\nnext_button.on_click(on_next)\nsubmit_button.on_click(on_submit)\nclear_button.on_click(on_clear)\n\n# Display the example container and navigation buttons\ndisplay(example_container)\ndisplay(widgets.HBox([previous_button, next_button]))\ndisplay(widgets.HBox([submit_button, clear_button]))\n\n# Initialize the first example\nupdate_example(current_index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: IpyWidget for scoring instructions & responses\n\n\nYou can see how hard it is to score even 20 samples each. Scoring is an intensive task especially when it is about topics that the labeler has no idea about. Above, the instruction is “What are some behavioral patterns exhibited by Zygaenoidea moths?”.\nAs a labeler, I have no idea what “Zygaenoidea moths” are, let alone know their characterstics. I had to search for “Zygaenoidea moths” on google, and that linked me to scholarly articles.\nThrough this simple exercise, I hope I have showcased how difficult it can be to rate responses generated by the LLM."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#model-training",
    "href": "posts/2023-04-30_LaMini-LM.html#model-training",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "5 Model Training",
    "text": "5 Model Training\nFrom the paper:\nWe present LaMini-LM, a family of language models instruction-tuned on our 2.58M instructions dataset \\(D_{ALL}\\). We train two types of models, encoder-decoder and decoder-only, for architectural comparison. The size for both categories of models ranges from 61M to 1.5B to facilitate size comparison. The underlying models for initialization are from five sources, including T5 (Raffel et al., 2020), Flan-T5 (Chung et al. (2022)), Cereberas-GPT (Dey et al. (2023)), GPT-2 (Radford et al., 2019), and GPT-Neo (Gao et al. (2020)).\n\n\n\nFigure 8: LaMini-LM collection.\n\n\nAlso, from the paper:\nWe finetune all models over 5 epochs and a batch size of 1024. For our encoder-decoder models, we use a learning rate of 5 × 10−4 following Chung et al. (2022). For our decoder-only models, we follow the same configuration as Alpaca (Taori et al., 2023) including the learning rate of 2 × 10−5. We use HuggingFace’s transformers for training. Moreover, we use the same prompt wrapper as Alpaca (Taori et al., 2023), hence we also wrap our instruction similarly during inference. We perform all of our experiments on 8×V100 (32G) and 8×A100 (40G) GPUs.\nAs part of this blog post, we will not be re-training the models, but you can see it is supervised finetuning on the Instruction Dataset using Transformers library."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#model-evaluation",
    "href": "posts/2023-04-30_LaMini-LM.html#model-evaluation",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "6 Model Evaluation",
    "text": "6 Model Evaluation\nThe authors have evaluated the performance of their trained models on several NLP tasks using model evaluation harness. (Gao et al. (2021))\nAs part of this blog post we will also be evluating the models using this framework.\nResults of model evaluation provided by the authors are shared in the table below. I have also shared the results from LLAMA.\n\n\n\nFigure 9: LaMini-LM collection.\n\n\nAs shared by anton, and as shown above, the results from LaMini-LM, don’t match LLAMA.\n\n\nWhat's even more interesting is the discrepancy of the numbers on LLaMA 7B vs their paper reported numbers… pic.twitter.com/12TBbuArLb\n\n— anton ((abacaj?)) April 28, 2023\n\n\nThe results shared in the original LLAMA paper are better compared to those shared in the LaMini-LM research paper. The most surprising is OpenBookQA, where in the LLAMA paper the reported accuracy is 57.2% compared to 42.4% in LaMini-LM.\nTo further analyse, let’s run evaluation on BoolQ, the results are reported in LLAMA, but not present in LaMini-LM.\nTo do this, let’s first install the library:\ngit clone https://github.com/EleutherAI/lm-evaluation-harness\ncd lm-evaluation-harness\npip install -e .\nNext, we could just simply run\npython main.py --model hf-causal --model_args pretrained=MBZUAI/LaMini-GPT-1.5B --tasks openbookqa,boolq,piqa --device cuda:0\nto evaluate the 1.5 GPT-2 on OpenBookQA (Mihaylov et al. (2018)), BoolQ (Clark et al. (2019)), PIQA (Bisk et al. (2019)).\n\n\nTable 1: Evaluation Results on BoolQ, PIQA, OpenBookQA\n\n\nTask\nVersion\nMetric\nValue\n\nStderr\n\n\n\n\nboolq\n1\nacc\n0.7725\n±\n0.0073\n\n\npiqa\n0\nacc\n0.7127\n±\n0.0106\n\n\n\n\nacc_norm\n0.7214\n±\n0.0105\n\n\nopenbookqa\n0\nacc\n0.2680\n±\n0.0198\n\n\n\n\nacc_norm\n0.3440\n±\n0.0213\n\n\n\n\nInteresting results to say the least, on BoolQ, it appears as though the results for LaMini-GPT are better than LLAMA. LLAMA’s 7B model is at 76.5% accuracy whereas LaMini-GPT is at 77.25% accuracy.\nAlso, our results on OpenBookQA don’t match those provided in the paper. The accuracy we get is only 26.8% compared to the reported 39.8% in the paper.\n\nAt this point, as a reader, I am not sure if I trust the results from the LaMini-LM paper. I’ve just followed the instructions and results on OpenBookQA are different?\n\nI have asked the authors to check if there is something that they have done differently compared to my understanding.\n\n\nAnother one please, I re-ran Language Model Evaluation Harness for OpenBookQA. (left)And results don't match those provided in paper for GPT-2-LaMini-1.5B. PIQA matches the paper. Provided eval table-4 from paper (right).I get 26.8% acc compared to 39.8% reported in the… pic.twitter.com/ErQYXuVOfQ\n\n— Aman Arora ((amaarora?)) May 1, 2023\n\n\nBased on the above table, let’s re-run for other evaluation datasets too and see if our results match those from the paper.\npython main.py --model hf-causal --model_args pretrained=MBZUAI/LaMini-GPT-1.5B --tasks openbookqa,sciq,race,record,sst,mrpc,rte,wsc,winogrande --device cuda:0"
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#conclusion",
    "href": "posts/2023-04-30_LaMini-LM.html#conclusion",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nTo summarise,\n\nWe recreated a small sample of the dataset using example-guided and topic-guided approach as mentioned in the paper.\nWe used OpenaiAPI to also generate responses for the paper.\nWe replicated Figure 5 which showcases that diversity in Self-Instruct guided \\(\\hat{X}_{SI}\\) is actually more compared to Alpaca \\(\\hat{X}_A\\)\nThe authors were very kind enought to update the HF dataset and add a new column called instruction_source to match Figure 4.\nWe used ChatGPT to create a simple IpyWidget to rate the scores. Through this simple exercise we realised how hard it can be to score text based responses.\nWe ran our own evaluation for LaMini-LM (1.5B GPT) using lm-evaluation-harness by EleutherAI on several NLP datasets. The results were bit different compared to the table as in Table 2. As mentioned before, this is due to difference in prompting, the authors used a specific prompt for inference.\nWe also saw that LLaMA results from the original paper didn’t match those reported in LaMini-LM. The authors were kind enough to run LLaMA benchmarking again, and it led to the same results as in Figure 9. See this tweet for clarification."
  },
  {
    "objectID": "posts/2023-04-30_LaMini-LM.html#sec-model-eval",
    "href": "posts/2023-04-30_LaMini-LM.html#sec-model-eval",
    "title": "Paper Review - ‘LaMini-LM’",
    "section": "6 Model Evaluation",
    "text": "6 Model Evaluation\nThe authors have evaluated the performance of their trained models on several NLP tasks using model evaluation harness. (Gao et al. (2021))\nAs part of this blog post we will also be evluating the models using this framework.\nResults of model evaluation provided by the authors are shared in the table below. I have also shared the results from LLAMA.\n\n\n\nFigure 9: LaMini-LM collection.\n\n\nAs shared by anton, and as shown above, the results from LaMini-LM, don’t match LLAMA.\n\n\nWhat's even more interesting is the discrepancy of the numbers on LLaMA 7B vs their paper reported numbers… pic.twitter.com/12TBbuArLb\n\n— anton ((abacaj?)) April 28, 2023\n\n\nThe results shared in the original LLAMA paper are better compared to those shared in the LaMini-LM research paper. The most surprising is OpenBookQA, where in the LLAMA paper the reported accuracy is 57.2% compared to 42.4% in LaMini-LM.\nTo further analyse, let’s run evaluation on BoolQ, the results are reported in LLAMA, but not present in LaMini-LM.\nTo do this, let’s first install the library:\ngit clone https://github.com/EleutherAI/lm-evaluation-harness\ncd lm-evaluation-harness\npip install -e .\nNext, we could just simply run\npython main.py --model hf-causal --model_args pretrained=MBZUAI/LaMini-GPT-1.5B --tasks openbookqa,boolq,piqa --device cuda:0\nto evaluate the 1.5 GPT-2 on OpenBookQA (Mihaylov et al. (2018)), BoolQ (Clark et al. (2019)), PIQA (Bisk et al. (2019)).\n\n\nTable 1: Evaluation Results on BoolQ, PIQA, OpenBookQA\n\n\nTask\nVersion\nMetric\nValue\n\nStderr\n\n\n\n\nboolq\n1\nacc\n0.7725\n±\n0.0073\n\n\npiqa\n0\nacc\n0.7127\n±\n0.0106\n\n\n\n\nacc_norm\n0.7214\n±\n0.0105\n\n\nopenbookqa\n0\nacc\n0.2680\n±\n0.0198\n\n\n\n\nacc_norm\n0.3440\n±\n0.0213\n\n\n\n\nIt appears as though the results for LaMini-GPT are better than LLAMA. LLAMA’s 7B model is at 76.5% accuracy whereas LaMini-GPT is at 77.25% accuracy.\nAlso, our results on OpenBookQA don’t match those provided in the paper. The authors reported on acc_norm for OpenBookQA using a wrapper for decoder models. We get 34.4% compared to 39.8% reported in the paper.\nThis is because the the authors used a wrapper during inference, which I didn’t. The authors were really kind enough to respond to my query and also share the updated wrapper code.\n\n\nHere is our adapted lm-eval-harness code: https://t.co/fP2y0IcboQ\n\n— Chiyu Zhang ((ChiyuZhang0851?)) May 1, 2023\n\n\nBased on the above table, let’s re-run for other evaluation datasets too and see if our results match those from the paper.\npython main.py --model hf-causal --model_args pretrained=MBZUAI/LaMini-GPT-1.5B --tasks openbookqa,sciq,race,record,sst,mrpc,rte,wsc,winogrande --device cuda:0\n\n\nTable 2: Evaluation Results on MRPC, WinoGrande, WSC, RACE, SST, RTE, Record, SciQ\n\n\nTask\nVersion\nMetric\nValue\n\nStderr\n\n\n\n\nmrpc\n0\nacc\n0.7475\n±\n0.0215\n\n\n\n\nf1\n0.8352\n±\n0.0161\n\n\nwinogrande\n0\nacc\n0.5777\n±\n0.0139\n\n\nwsc\n0\nacc\n0.6635\n±\n0.0466\n\n\nrace\n1\nacc\n0.3742\n±\n0.0150\n\n\nsst\n0\nacc\n0.8933\n±\n0.0105\n\n\nrte\n0\nacc\n0.6354\n±\n0.0290\n\n\nrecord\n0\nf1\n0.8244\n±\n0.0038\n\n\n\n\nem\n0.8177\n±\n0.0039\n\n\nsciq\n0\nacc\n0.9100\n±\n0.0091\n\n\n\n\nacc_norm\n0.8790\n±\n0.0103\n\n\n\n\nAfter running more evaluation on these benchmarks shared in Table 2, looks like the results are different compared to the paper. This maybe due to the same reason as before.\n\n\n\n\n\n\nThank you again authors!\n\n\n\nThe authors have responded regarding the difference between their and original LLaMA benchmarking results. It might be due to difference in prompting. We will probably need to run our own benchmarking using 7B LLaMA model & lm-evaluation-harness.\n\n\n(abacaj?) (amaarora?) The LLaMA results use a different method, so a higher number there doesn’t necessarily mean better than ours. Therefore, the tables shouldn’t be compared. The differences may come from the different prompts they used. Here is the description in the LLaMa paper. pic.twitter.com/B8stCQwyPl\n\n— Chiyu Zhang ((ChiyuZhang0851?)) May 1, 2023\n\n\n\n\n\n6.1 Human Evaluation\nLastly, let’s look at the human evaluation bit. From the paper:\nTo complete the evaluation, we additionally evaluate the practicality of both our LaMini-LM and our baseline models by utilizing the user-oriented instructions from Wang et al. (2022a), which consists of 252 instructions covering 71 commonly used apps use-cases.\n\n\n\n\n\n\nImportant\n\n\n\nThe training set consists of 0.27M instructions+responses that have been generated using “example-guided” approach from Self-Instruction, and 0.28M instructions+responses that have been generated using “topic-guided” approach from Self-Instruction. Doesn’t that mean that the evaluation set is very similar to the training set here?\n\n\nAlso, would have been nice to know what these 252 Instructions look like. The authors have kindly provided the human evaluation results table which I share below in Figure 10, but not the evaluation instructions.\n\n\n\nFigure 10: LaMini-LM collection.\n\n\n\n\nCode\n# pip install -q transformers\nfrom transformers import pipeline\ncheckpoint = \"MBZUAI/LaMini-GPT-1.5B\" \nmodel = pipeline('text-generation', model = checkpoint, device='cuda:0')\n\ninstruction = 'Two large and 1 small pumps can fill a swimming pool in 4 hours. One large and 3 small pumps can also fill the same swimming pool in 4 hours. How many hours will it take 4 large and 4 small pumps to fill the swimming pool?'\ninput_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\n\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nResponse Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nTwo large and 1 small pumps can fill a swimming pool in 4 hours. One large and 3 small pumps can also fill the same swimming pool in 4 hours. How many hours will it take 4 large and 4 small pumps to fill the swimming pool?\n\n### Response:It will take 4 large and 4 small pumps (6 pumps total) 4 hours to fill the swimming pool.\n\n\n\nBy the way, ChatGPT nails it and returns the right answer “1 hour & 36 minutes” but it would be unfair to compare a 1.5B model with ChatGPT.\n\n\n\nCode\ninstruction = 'Today is 30 Apr, 2023. I want to participate in a marathon on July 30, 2023. Please create a training program for me. I can run 5kms easily as of now.'\ninput_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\n\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nResponse Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nToday is 30 Apr, 2023. I want to participate in a marathon on July 30, 2023. Please create a training program for me. I can run 5kms easily as of now.\n\n### Response:Understood. Training program created for participant. Training will be divided into four phases: \n1. Endurance training\n2. Strength and flexibility training \n3. Low-impact exercise (stretching, yoga, etc.) \n4. Functional training (running drills, pace training, etc.)\n\n\nThis is not a statisfactory and I would rate it Rate-C, “The response is relevant and responds to the instruction, but it has significant errors in the content.”\n\n\nCode\ninstruction = 'Write a product description for a sustainable, eco-friendly backpack made from recycled materials.'\ninput_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\n\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nResponse Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWrite a product description for a sustainable, eco-friendly backpack made from recycled materials.\n\n### Response:Sustainable, eco-friendly backpack made from recycled materials.\n\n\nThe result above looks unsatisfactory too."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "",
    "text": "Ahead of Times - Issue 1"
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#what-does-the-image-signify",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#what-does-the-image-signify",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "1.1 What does the image signify?",
    "text": "1.1 What does the image signify?\nSince the inception of Neural Nets in 2012 by the godfather of AI - Geoffrey Hinton, a lot has changed in terms of our capabilities as a human race to process unstructured data - specifically images and text.\nThis is even more apparent and visible with the release of chatbots such as ChatGPT with colleagues and employees from different fields (not particularly software engineers) utilising the tool in their day to day work.\nIn some ways we could say “The world of AI is on fire and changing very rapidly”.\n“Ahead of Times” is meant to serve as a newsletter in this “times of change” to provide you with the latest news and updates by cutting through the noise & help you steer away from TikTok inspired influencers who write for publicity."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#public-commitments",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#public-commitments",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "1.2 Public commitments",
    "text": "1.2 Public commitments\nAs the author of “Ahead of Times”, I publicly commit to:\n\nProviding a fair assessment of the latest technologies by trying them out myself.\nBeing regular in providing you up to date information.\nProvide code examples and snippets where necessary.\nA “hands on” practical approach rather than going deep into mathematics/theory.\nCommunicate clearly if there are times when I am not able to release the newsletter.\nOpen to feedback.\nPromise to not spread misinformation, not be inspired by the “click bait” ideology.\nContinue writing the newsletter even if I am the only reader, trust me, it helps when you note things down in writing.\n\nWith the introductions and some public commitments out of the way, let’s get started with some key updates between “April 24, 2023 - April 30, 2023”."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#redpajama-training-progress-at-440-billion-tokens",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#redpajama-training-progress-at-440-billion-tokens",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.2 RedPajama training progress at 440 billion tokens",
    "text": "2.2 RedPajama training progress at 440 billion tokens\nWhat is RedPajama? Why is this relevant or important?\nFrom the RedPajama announcement post,\nFoundation models such as GPT-4 have driven rapid improvement in AI. However, the most powerful models are closed commercial models or only partially open. RedPajama is a project to create a set of leading, fully open-source models.\nRemember “LLaMA” (Touvron et al. (2023))? Large language models of varying sizes between 7B to 65B released by Meta trained on trillions of tokens on publicly available datasets?\nWell, as great as it may be, The original LLaMA code is GPL licensed which means any project using it must also be released under GPL. What does this mean? This “taints” any other code and prevents meaningful academic and commercial use. You also have to apply for access to LLaMA weights making them inaccessible. I have personally been waiting a few weeks and never heard back from Meta.\nSo why is this RedPajama project important? Quoting directly from their blog,\nOur goal is to train the LLaMA suite of models and make them available under a permissive open-source license so they can be used as a foundation for research and commercial applications.\nGreat, right?!\nThe RedPajama dataset is available on HuggingFace and can be downloaded from here. All pre-processing and quality filters are open soourced here!\nAs part of the announcement on April 24, 2023, Together announced:\nAt 440 billion tokens, we now have a model checkpoint that is better than Pythia-7B (0.416 HELM vs. 0.400 HELM) and StableLM-7B (0.283 HELM).\nBasically, in simpler terms, they conducted a training run with exactly the same model architecture and tokenizer as Pythia-7B (Biderman et al. (2023)), a well-regarded and fully open model from EleutherAI trained on the Pile (originally released by EleutherAI in 2020).\n\n\n\nA mid training checkpoint of the 7B RedPajama base model, using Pythia architecture, achieves higher quality result on HELM scores than Pythia-7B."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#segment-anything-in-medical-images",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#segment-anything-in-medical-images",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.3 Segment Anything in Medical Images",
    "text": "2.3 Segment Anything in Medical Images\nRemember Segment Anything Model from Meta?\n\nBlog | Paper | Code\nTry out the Demo for SAM here. In the video that we see above, the model does a great job at segmenting people and vehicles in the image I provided to it. It did miss however some tiny details, but, hey, that’s okay for a zero shot model performing segmentation!\nFrom the Segment Anything in Medical Images, also called MedSAM (Ma and Wang (2023)),\nMedSAM, is the first attempt at extending the success of SAM to medical images, with the goal of creating a universal tool for the segmentation of various medical targets.\nYou can try out the models in the offical GitHub Repository here.\n\nI have personally not tried MedSAM, but if you do, feel free to share your results with me at (amaarora?).\n\n\n\n\nMedSAM significantly improves the segmentation performance across various modalities and segmentation tasks."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#a-cookbook-of-self-supervised-learning",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#a-cookbook-of-self-supervised-learning",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.4 A Cookbook of Self-Supervised Learning",
    "text": "2.4 A Cookbook of Self-Supervised Learning\nFrom “Self-supervised learning: The dark matter of intelligence”:\nSupervised learning is a bottleneck for building more intelligent generalist models that can do multiple tasks and acquire new skills without massive amounts of labeled data. Practically speaking, it’s impossible to label everything in the world.\nSelf-supervised learning enables AI systems to learn from orders of magnitude more data, which is important to recognize and understand patterns of more subtle, less common representations of the world.\nLast week, Yann LeCun shared “A Cookbook of Self-Supervised Learning” on Twitter.\n\n\n\nContents of “A Cookbook of Self-Supervised Learning”.\n\n\nWhy a Cookbook for Self-Supervised Learning?\nWhile many components of SSL are familiar to researchers, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyperparameters. SSL research has a high barrier to entry due to (i) its computational cost, (ii) the absence of fully transparent papers detailing the intricate implementations required to fully enable SSL’s potential, and (iii) the absence of a unified vocabulary and theoretical view of SSL.\n\nThe above cookbook provides a comprehensive analysis of SSL, hyperparameter tuning, metric learning methods such as SIMCLR (Chen et al. (2020)), evaluation and also deployment!"
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#new-ways-to-manage-your-data-in-chatgpt",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#new-ways-to-manage-your-data-in-chatgpt",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.5 New ways to manage your data in ChatGPT",
    "text": "2.5 New ways to manage your data in ChatGPT\nChatGPT users can now turn off chat history, allowing you to choose which conversations can be used to train OpenAI’s models.\nAnnounced on April 25, 2023, OpenAI has introduced the ability to turn off Chat History in ChatGPT.\nThis is particularly useful for sharing code snippets related to private and confidential code sources."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#parameter-efficient-llm-finetuning-with-low-rank-adaptation-lora",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#parameter-efficient-llm-finetuning-with-low-rank-adaptation-lora",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.6 Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)",
    "text": "2.6 Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)\nAs mentioned in Section 1, I will also be sharing helpful tutorials as part of the newsletter.\nOne such amazing tutorial released last week by Sebastian Raschka showcases how to finetuning Large Language Models using a technique called LoRA (Hu et al. (2021)).\n\n\n\nOverview of LoRA\n\n\n\nWhat I particularly liked about this blog post were the illustrative images matching pseudo PyTorch code. This is an opportunity for someone to build on top of this tutorial and showcase LoRA with complete working PyTorch code from scratch. :)\n\nYou can find the tutorial shared by Sebastian Raschka here."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#a-recap-of-replit-developer-day",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#a-recap-of-replit-developer-day",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.6 A Recap of Replit Developer Day",
    "text": "2.6 A Recap of Replit Developer Day\nAt Develop Day, Replit announced:\n\nProduction-grade Deployments straight from IDE\nA more powerful Workspace\nSecure Workspace Extensions\nReplit’s From-Scratch Trained Code Complete model"
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#replits-from-scratch-trained-code-complete-model",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#replits-from-scratch-trained-code-complete-model",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.7 Replit’s From-Scratch Trained Code Complete model",
    "text": "2.7 Replit’s From-Scratch Trained Code Complete model\nBefore we begin, what is Replit? And why is this relevant?\nReplit is the newest AI unicorn, and they announced on April 28, 2023 that they’ve raised nearly $100M and are valued at $1.16B!\nAlso, at Developer Day, Replit announced:\n\nProduction-grade Deployments straight from IDE\nA more powerful Workspace\nSecure Workspace Extensions\nReplit’s From-Scratch Trained Code Complete model\n\n\nThe one I am most interested in, is the open source replacement for Codex, which Replit has termed “CodeGen”!\n\nSome folks have shared their reviews on CodeGen:\n\n\nDespite the recent hype around Replit's new model, it isn't actually the best open-source code model out thereIn fact, it's not even the best 3-billion parameter code modelThat title belongs to Microsoft's MIM-2.7B… And it was trained on 2x fewer tokens! pic.twitter.com/PiE8NpG5Lc\n\n— Aman Sanger ((amanrsanger?)) April 26, 2023\n\n\nFor further reading:\nThere is also this really cool interview with Replit co-founder Amjad Masad by Dan Shipper."
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#stability.ai-announced-deepfloyd-if",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#stability.ai-announced-deepfloyd-if",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.1 Stability.ai announced DeepFloyd IF",
    "text": "2.1 Stability.ai announced DeepFloyd IF\nThe theme image for this newsletter was also generated using this model. :)"
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#conclusion",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#conclusion",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.8 Conclusion",
    "text": "2.8 Conclusion\nThis brings us to the end of our very first issue of the Newsletter!\nIf you found it helpful, consider subscribing to the blog. You can also buy me a coffee here.\nThank you for your time! See you next week on Monday at 9am AEST!"
  },
  {
    "objectID": "posts/2023-05-02-Ahead_of_times-Issue_1.html#stability-ai-releases-deepfloyd-if",
    "href": "posts/2023-05-02-Ahead_of_times-Issue_1.html#stability-ai-releases-deepfloyd-if",
    "title": "Ahead of Times - Issue 1 (Apr 24 - Apr 30)",
    "section": "2.1 Stability AI releases DeepFloyd IF",
    "text": "2.1 Stability AI releases DeepFloyd IF\n\nThe theme image for this newsletter was also generated using this model. You can try it out yourself here.\n\nOn April 28, 2023, Stability AI announced the research release of DeepFloyd IF, a powerful text-to-image cascaded pixel diffusion model.\nRead more about the release on their official blog here."
  },
  {
    "objectID": "nbs/2023-05-09-Ahead_of_times-Issue_2.html",
    "href": "nbs/2023-05-09-Ahead_of_times-Issue_2.html",
    "title": "Ahead of Times - Issue 2 (May 01 - May 07)",
    "section": "",
    "text": "Ahead of Times - Issue 1\nsubscribe.html"
  },
  {
    "objectID": "nbs/2023-05-09-Ahead_of_times-Issue_2.html#when-a.i.-chatbots-hallucinate",
    "href": "nbs/2023-05-09-Ahead_of_times-Issue_2.html#when-a.i.-chatbots-hallucinate",
    "title": "Ahead of Times - Issue 2 (May 01 - May 07)",
    "section": "1.1 When A.I. Chatbots Hallucinate",
    "text": "1.1 When A.I. Chatbots Hallucinate\nThe article called “When A.I. Chatbots Hallucinate” published in NYT (New York Times) is a great introduction to term called “Hallucination”.\nIn simple words, it could also be referred to ‘incorrect completion of the input query’ by the chatbot.\nI would recommend this article specifically to readers who would like a very gentle introduction to Large Language Models (LLMs), hallucination, prompting, Google Bard, ChatGPT, Microsoft Bing & GPT-4."
  },
  {
    "objectID": "nbs/2023-05-09-Ahead_of_times-Issue_2.html#the-godfather-of-a.i.-leaves-google-and-warns-of-danger-ahead",
    "href": "nbs/2023-05-09-Ahead_of_times-Issue_2.html#the-godfather-of-a.i.-leaves-google-and-warns-of-danger-ahead",
    "title": "Ahead of Times - Issue 2 (May 01 - May 07)",
    "section": "1.2 ‘The Godfather of A.I.’ Leaves Google and Warns of Danger Ahead",
    "text": "1.2 ‘The Godfather of A.I.’ Leaves Google and Warns of Danger Ahead\nGeoffrey Hinton, one of the pioneers in the field of artificial intelligence, quit Google. From the article published in NYT:\nDr. Hinton said he has quit his job at Google, where he has worked for more than a decade and became one of the most respected voices in the field, so he can freely speak out about the risks of A.I.\nSome concerns shared by Geoffrey Hinton in the NYT article:\n\n“He is also worried that A.I. technologies will in time upend the job market.”\n\n\n“The idea that this stuff could actually get smarter than people is not so far off”\n\nAlso, I would refer the reader to Deep learning pioneer Geoffrey Hinton has quit Google for more reading on the same.\nDr. Hinton also later clarified on Twitter:\n\n\nIn the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.\n\n— Geoffrey Hinton ((geoffreyhinton?)) May 1, 2023"
  }
]