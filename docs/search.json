[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSupport bot with Claude 3.5 Sonnet using Claudette and Slack-SDK\n\n\nCreating a support bot that support API calls using Claudette\n\n\n\nLLM\n\n\n\nAs part of this blog post we will build a support bot that can answer support queries from a slack channel using Claudette (a thin python wrapper on top of Anthropic CLI) \n\n\n\n\n\nJun 22, 2024\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain\n\n\n\n\n\nEmbark on an enlightening journey through the world of document-based question-answering chatbots using langchain! With a keen focus on detailed explanations and code walk-throughs, you’ll gain a deep understanding of each component - from creating a vector database to response generation.\n\n\n\n\n\nJul 28, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nDeciphering LangChain: A Deep Dive into Code Complexity\n\n\n\n\n\nAnalyzing LangChain’s source code reveals impressive modularity but also surprising complexity in executing simple text generation. The deep call stack makes tracing execution flow challenging.\n\n\n\n\n\nJul 25, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAhead of Times - Issue 2 (May 01 - May 07)\n\n\nSecond issue of the weekly newsletter to help you stay ahead of the times with latest news & updates in the field of AI.\n\n\n\nNewsletter\n\n\n\nAs part of this newsletter, I share with you key updates, projects, GitHub repos, research trends, research papers in the field of Computer Vision, Large Language Models and Stable Diffusion. \n\n\n\n\n\nMay 8, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAhead of Times - Issue 1 (Apr 24 - Apr 30)\n\n\nFirst issue of the weekly newsletter to help you stay ahead of the times with latest news & updates in the field of AI.\n\n\n\nNewsletter\n\n\n\nAs part of this newsletter, I share with you key updates, projects, GitHub repos, research trends, research papers in the field of Computer Vision, Large Language Models and Stable Diffusion. \n\n\n\n\n\nMay 2, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Review - ‘LaMini-LM’\n\n\nPaper review of “LaMini-LM - A Diverse Herd of Distilled Models from Large-Scale Instructions” and analysis on released 2.58M instruction dataset.\n\n\n\nLarge Language Models\n\n\nPaper Review\n\n\n\nAs part of this blog post, we regenerate a small sample of the 2.58M shared Instruction Dataset and also perform human evaluation on some of the generated models shared in the research paper. \n\n\n\n\n\nMay 1, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated CLIP (Part-2)\n\n\nLearning Transferable Visual Models From Natural Language Supervision\n\n\n\nMultimodal\n\n\nTransformers\n\n\nClip\n\n\n\nThis post is part-2 of the two series blog posts on CLIP (for part-1, please refer to my previous blog post). In this blog, we present the PyTorch code behind CLIP for model building and training. This blog post is in itself a working Jupyter Notebook. \n\n\n\n\n\nMar 11, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated CLIP (Part-1)\n\n\nLearning Transferable Visual Models From Natural Language Supervision\n\n\n\nMultimodal\n\n\nTransformers\n\n\n\nThis post is part-1 of the two series blog posts on CLIP. In this blog, we present an Introduction to CLIP in an easy to digest manner. We also compare CLIP to other research papers and look at the background and inspiration behind CLIP. \n\n\n\n\n\nMar 3, 2023\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSwin Transformer\n\n\nHierarchical Vision Transformer using Shifted Windows\n\n\n\nComputer Vision\n\n\nModel Architecure\n\n\nTransformers\n\n\n\nSwin Transformer Model Architecture explained with PyTorch implementation line-by-line. \n\n\n\n\n\nJul 4, 2022\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated DETR\n\n\nEnd-to-End Object Detection with Transformers\n\n\n\nComputer Vision\n\n\nModel Architecure\n\n\nObject Detection\n\n\nTransformers\n\n\n\nDETR Model Architecture explained with PyTorch implementation line-by-line. \n\n\n\n\n\nJul 26, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe sad state of AI and tech startups in Australia today and what can we do about it\n\n\n\n\n\n\nAI\n\n\nJeremy Howard\n\n\n\n\n\n\n\n\n\nMay 15, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAdam and friends\n\n\nAdam, SGD, RMSProp from scratch in PyTorch.\n\n\n\nComputer Vision\n\n\n\nBasic optimizers from scratch in PyTorch with working notebook. \n\n\n\n\n\nMar 13, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nVision Transformer\n\n\nAn Image is Worth 16x16 Words - Transformers for Image Recognition at Scale\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nTransformers\n\n\n\nIn this blog post, we will be looking at the Vision Transformer architectures in detail, and also re-implement in PyTorch from scratch. \n\n\n\n\n\nJan 18, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe EfficientDet Architecture in PyTorch\n\n\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nObject Detection\n\n\n\nIn this blog post, we will look at how to implement the EfficientDet architecture in PyTorch from scratch. \n\n\n\n\n\nJan 13, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nEfficientDet - Scalable and Efficient Object Detection\n\n\n\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nObject Detection\n\n\n\nAs part of this blog post I will explain how EfficientDets work step-by-step. \n\n\n\n\n\nJan 11, 2021\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nTop 100 solution - SIIM-ACR Pneumothorax Segmentation\n\n\n\n\n\n\nComputer Vision\n\n\nKaggle\n\n\nImage Segmentation\n\n\n\nIn this blog post, we will looking at Image Segmentation based problem in Pytorch with SIIM-ACR Pneumothorax Segmentation competition serving as a useful example and create a solution that will get us to the top-100 leaderboard position on Kaggle. \n\n\n\n\n\nSep 6, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nGeM Pooling Explained with PyTorch Implementation and Introduction to Image Retrieval\n\n\n\n\n\n\nComputer Vision\n\n\n\nAs part of this blog post we will be looking at GeM pooling and also look at the research paper Fine-tuning CNN Image Retrieval with No Human Annotation. We also implement GeM Pooling from scratch in PyTorch. \n\n\n\n\n\nAug 30, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net A PyTorch Implementation in 60 lines of Code\n\n\nU-Net Convolutional Networks for Biomedical Image Segmentation\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\nImage Segmentation\n\n\n\nAs part of this blog post we will implement the U-Net architecture in PyTorch in 60 lines of code. \n\n\n\n\n\nAug 30, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSIIM-ISIC Melanoma Classification - my journey to a top 5% solution and first silver medal on Kaggle\n\n\nWinning solution for SIIM-ISIC Melanoma Classification\n\n\n\nComputer Vision\n\n\nKaggle\n\n\n\nAs part of this blog post I share my winning solution for SIIM-ISIC Melanoma Classification Kaggle Competition. \n\n\n\n\n\nAug 23, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nEfficientNet\n\n\nRethinking Model Scaling for Convolutional Neural Networks\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\nLook at the current SOTA, with top-1 accuracy of 88.5% on ImageNet. \n\n\n\n\n\nAug 13, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nGroup Normalization\n\n\n\n\n\n\nComputer Vision\n\n\n\nIn this blog post, we will look at Group Normalization research paper and also implement Group Normalization in PyTorch from scratch. \n\n\n\n\n\nAug 9, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nDenseNet Architecture Explained with PyTorch Implementation from TorchVision\n\n\nDensely Connected Convolutional Networks\n\n\n\nProgramming\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\nIn this blog post, we introduce dense blocks, transition layers and look at the TorchVision implementation of DenseNet step-by-step. \n\n\n\n\n\nAug 2, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nSqueeze and Excitation Networks Explained with PyTorch Implementation\n\n\nSqueeze-and-Excitation Networks\n\n\n\nComputer Vision\n\n\nModel Architecture\n\n\n\nIn this blogpost, we re-implement the Squeeze-and-Excitation networks in PyTorch step-by-step with very minor updates to ResNet implementation in torchvision. \n\n\n\n\n\nJul 24, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nLabel Smoothing Explained using Microsoft Excel\n\n\nBetter language models and their implications\n\n\n\nComputer Vision\n\n\n\nIn this blogpost, we re-implement Label Smoothing in Microsoft Excel step by step. \n\n\n\n\n\nJul 18, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to PyTorch Lightning with comparisons to PyTorch\n\n\nBetter language models and their implications\n\n\n\nProgramming\n\n\nComputer Vision\n\n\n\nIn this blogpost, we will be going through an introduction to Pytorch Lightning and implement all the cool tricks like - Gradient Accumulation, 16-bit precision training, and also add TPU/multi-gpu support - all in a few lines of code. We will use Pytorch Lightning to work on SIIM-ISIC Melanoma Classification challenge on Kaggle. \n\n\n\n\n\nJul 12, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Focal Loss and when should you use it?\n\n\nBetter language models and their implications\n\n\n\nComputer Vision\n\n\nLoss Function\n\n\n\nIn this blogpost, we will understand what Focal Loss and when is it used. We will also take a dive into its math and implement step-by-step in PyTorch. \n\n\n\n\n\nJun 29, 2020\n\n\nAman Arora\n\n\n\n\n\n\n\n\n\n\n\n\nThe Annotated GPT-2\n\n\nBetter language models and their implications\n\n\n\nNLP\n\n\nTransformers\n\n\n\nThis post presents an annotated version of the paper in the form of a line-by-line implementation in PyTorch. This document itself is a working notebook, and should be a completely usable implementation. \n\n\n\n\n\nFeb 18, 2020\n\n\nAman Arora\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n    \n        \n        Subscribe\n        * indicates required\n\n    Email Address  *"
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "",
    "text": "Recently I presented at REA Unstack’d on Large Language Models. It was mostly a demo about a ChatBot that I’ve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - PropTrack.\nLater on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment & all things LLM.\n\n\n\nREA Unstack’d\n\n\nMeet Sachin Abeywardana & Ned Letcher, our panelists.\nThere are many tutorials available today that showcase how to build a Q/A ChatBot, and most (if not all) use LangChain. Over the past few months, this framework has become extremely popular among all who want to use LLMs. But, its source code is hard to read and if you are trying to do something that’s not within the capabilities of the framework, it becomes extremely difficult.\n\n\nHere's a few thoughts on (LangChainAI?), the problems I see with it currently, and how I think it could improve. This was originally formatted as a message to (hwchase17?):Here's a few things off the top of my head – 1. Heavy use of OOP. Having multiple layers of abstraction…\n\n— Sam Hogan ((0xSamHogan?)) July 12, 2023\n\n\nI recently wrote about LLMChains in langchain too, and found the same to true. You can find the previous blog post here. I would highly recommend the readers to give the previous blog post a read, it will explain LLMChains and Chains in langchain, that will be instrumental in understanding conversational chatbot that we are building today.\n\n\n\n\n\n\nNote\n\n\n\n👉 This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. The blog post in itself is a completely working jupyter notebook with code-snippets."
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#introduction",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#introduction",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "",
    "text": "Recently I presented at REA Unstack’d on Large Language Models. It was mostly a demo about a ChatBot that I’ve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - PropTrack.\nLater on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment & all things LLM.\n\n\n\nREA Unstack’d\n\n\nMeet Sachin Abeywardana & Ned Letcher, our panelists.\nThere are many tutorials available today that showcase how to build a Q/A ChatBot, and most (if not all) use LangChain. Over the past few months, this framework has become extremely popular among all who want to use LLMs. But, its source code is hard to read and if you are trying to do something that’s not within the capabilities of the framework, it becomes extremely difficult.\n\n\nHere's a few thoughts on (LangChainAI?), the problems I see with it currently, and how I think it could improve. This was originally formatted as a message to (hwchase17?):Here's a few things off the top of my head – 1. Heavy use of OOP. Having multiple layers of abstraction…\n\n— Sam Hogan ((0xSamHogan?)) July 12, 2023\n\n\nI recently wrote about LLMChains in langchain too, and found the same to true. You can find the previous blog post here. I would highly recommend the readers to give the previous blog post a read, it will explain LLMChains and Chains in langchain, that will be instrumental in understanding conversational chatbot that we are building today.\n\n\n\n\n\n\nNote\n\n\n\n👉 This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. The blog post in itself is a completely working jupyter notebook with code-snippets."
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#chatbot-implementation-in-langchain",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#chatbot-implementation-in-langchain",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "2 Chatbot: Implementation in langchain",
    "text": "2 Chatbot: Implementation in langchain\nLet’s say you have a number of documents, in my case, I have a bunch of markdown documents. And we want to build a question answering chatbot that can take in a question, and find the answer based on the documents.\n\n\n\n\n\n\nFigure 1: Chatbot architecture\n\n\n\nIn essence, the chatbot looks something like above. We pass the documents through an “embedding model”. It is easy enough to use OpenAI’s embedding API to convert documents, or chunks of documents to embeddings. These embeddings can be stored in a vector database such as Chroma, Faiss or Lance.\nThe user interacts through a “chat interface” and enters a question/query. This query can also be converted to an embedding using the embedding model. Next, we can find the nearest chunks (similar to the query) using similarity search, then pass these nearest chunks (referred to as “context”) to a Large Language Model such as ChatGPT.\nFinally, we retrieve an answer and this answer get’s passed back to the user in the chat interfact. We store this interaction in chat history and continue.\nThat is all in theory, in code, using langchain, above would look like:\n\n%load_ext autoreload\n%autoreload 2\n\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\n# directory to store vector database\npersist_directory = \".db/\"\nopenai_api_key = os.environ['OPENAI_API_KEY']\n# loader that loads `markdown` documents\nloader = DirectoryLoader(\"./output/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n# text splitter converts documents to chunks\ndocs = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nchunks = text_splitter.split_documents(docs)\n# embedding model to convert chunks to embeddings\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n\n# load vector database, uncomment below two lines if you'd like to create it\n\n#################### run only once at beginning ####################\n# db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\n# db.persist()\n####################################################################\ndb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", output_key='answer', return_messages=False)\n\n# create QA chain using `langchain`, database is used as vector store retriever to find \"context\" (using similarity search)\nqa = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    get_chat_history=lambda o:o,\n    memory=memory,\n    return_generated_question=True,\n    verbose=False,\n)\n\n\n# let's ask a question\nqa({\"question\": \"Why is it so hard to find a rental property in Australia in June 2023?\", \"chat_history\": []})\n\n{'question': 'Why is it so hard to find a rental property in Australia in June 2023?',\n 'chat_history': '',\n 'answer': 'In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.',\n 'generated_question': 'Why is it so hard to find a rental property in Australia in June 2023?'}\n\n\nLooking at the answer above, it really answers the question - “Why is it so hard to find a rental property in Australia in June 2023?” very well. Above might only be a few lines of code, but there is actually quite a lot going on underneath. Refer to Figure 1 for everything that’s going on underneath.\nBut, as a recap, and matching our steps with code shared above:\n\nLoad markdown files in a list loader = DirectoryLoader(\"./output/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\nCreate a splitter that can split documents to chunks text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nConvert each chunk and store as Embeddings in a Chroma DB Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\nUse the database as retriever to get relevant text (context), and based on ‘question’, use OpenAI’s gpt-3.5-turbo (ChatGPT) model to answer question based on context.\n\nConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    memory=memory,\n    verbose=False,\n)\n\nAlso store conversation as chat history in memory memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n\n\n2.1 Text splitter\nFor our simple usecase, we are using a text splitter of type CharacterTextSplitter.\ntext_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nWe are using a chunk_size of 1024, which means that the document will be divided into chunks of size 1024, and there will be 128 character overlap between each of the chunks.\nThe CharacterTextSplitter used above splits texts based using regex and a separator. The separator in this case is '\\n\\n'. Thus, anytime there are two line breaks, our text splitter will split documents. Internally, in LangChain to split a text, _split_text_with_regex is being called.\n# simplified version without `keep_separator`\ndef _split_text_with_regex(\n    text: str, separator: str, keep_separator: bool\n) -&gt; List[str]:\n    # Now that we have the separator, split the text\n    if separator:\n                splits = re.split(separator, text)\n    else:\n        splits = list(text)\n    return [s for s in splits if s != \"\"]\nThere are many other text splitters that we could have also used. For a complete list - refer here.\n\n\n\n\n\n\nNote\n\n\n\nOne good one to further try would be - MarkdownHeaderTextSplitter. This particular splitter splits based on markdown headings, and it might be more useful for our usecase.\n\nRemember, the idea of chunking is to keep text with common context together.\n\n\n\nNow, that we have created our first bit, a text splitter that can split documents to chunks, let’s move on to the embedding model.\n\n\n2.2 Embedding model\nAlso, for our embedding model - we are using OpenAIEmbeddings. The main idea for the embedding model is to convert the chunks from before to embeddings.\nRemember, an embedding is only a vector representation of the text.\nSo, how do we convert our chunks (few sentences long) to a bunch of numbers? We can use openai’s embeddings API. Without langchain, this looks something like:\n\nimport openai\nchunk = \"This is a sample chunk consisting of few sentences.\"\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\nemb = get_embedding(chunk, model='text-embedding-ada-002')\nlen(emb)\n\n1536\n\n\nIn langchain, to achieve the same we instantiate from OpenAIEmbeddings.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\ntext = \"This is a test document.\"\nquery_result = embeddings.embed_query(text)\nlen(query_result)\n\n1536\n\n\nNow, to embed all chunks at once, OpenAIEmbeddings has a method called embed_documents.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nimport numpy as np\nembeddings = OpenAIEmbeddings()\ndocs = [\"This is test document 1.\", \"This is test document 2.\"]\nembs = embeddings.embed_documents(docs)\nnp.array(embs).shape\n\n(2, 1536)\n\n\nGreat, now that we have a way to embed all documents, let’s look at vector database next.\n\n\n2.3 Vector database\nConsider the vector database to a repository of knowledge. All our chunks get converted to embeddings and get stored in a vector-db. In our case, we are using chroma-db.\nLooking at the documentation, we start by creating a client, and then a collection. Once we have a collection ready, it is very simple to query the collection to get back the results.\nresults = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2\n)\nWhat goes under the hood inside langchain, is that we first instantiate a chroma-db collection. Next, we use collection’s upsert method passing in embeddings and texts. And this way, we have created our vector database that can be used to find nearest chunks from our documents based on “query” using similarity-search.\n\n\n\n\n\n\nNote\n\n\n\n❓ Some questions here to ask would be\n\nWould results look different or better if we used Cohere Embeddings? What would be the price difference?\nWhat would the quality of results be like if we used open source models like Llama-v2 released a few days ago?\nWhat if we used sentence-transformers?\nDo we really need a vector database? Can we store the embeddings as a np.array and use cosine-similarity to find nearest embeddings?\n\n\n\n\n\n2.4 Q&A ChatBot\nSo far we have looked at text-splitter, embedding model and vector database. These are the building blocks of the chatbot. But, how do we bring the building blocks together?\nIn langchain, all the pieces come together in ConversationalRetrievalChain which is the main topic of this blog post too. We instantiate an instance of the class using @classmethod called from_llm.\nqa = ConversationalRetrievalChain.from_llm(\n    llm=OpenAIChat(temperature=0, max_tokens=-1),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    memory=memory,\n    get_chat_history=lambda x: x,\n    verbose=True,\n)\nresponse = qa({\n    \"question\": \"Why is it so hard to find a rental property in Australia in June 2023?\", \n    \"chat_history\": []\n})\nThere are two main things that go on inside a conversational retrieval chain.\nA conversational retrieval chain can take in a query, and based on the input query (question) and chat-history, it updates it to a new question.\nThis new question is then passed to a second document chain, to find the nearest chunks (based on question) - referred to as “context”, and this context alongside the new question get’s passed to a large language model (such as gpt-3.5-turbo or ChatGPT), to retrieve the answer.\nSo, internally - ConversationalRetrievalChain consists of two chains:\n\nA question generator chain, which updates input query/question based on chat history (LLMChain)\nAnd a document chain to join retrieved documents/chunks together (StuffDocumentsChain)\n\n\n\n\n\n\n\nOn LLMChains\n\n\n\nGood news! We have already covered LLMChains in our previous blog post before here. In essence, given a prompt, the LLMChain can be used to generate an answer based on the prompt.\nGoing forward, I am going to assume that the reader has read the previous blog post and has a solid understanding of LLMChains & Chains in general.\n\n\nFrom our previous blog post, we know that anytime we call any chain in langchain, the __call__ method from Chain class gets invoked which in turn makes a call to _call method of derived class.\nThe ConversationalRetrievalChain is a subclass of BaseConversationalRetrievalChain which in turn is a subclass of Chain.\nThe _call method is implemented inside BaseConversationalRetrievalChain and it looks like below:\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; Dict[str, Any]:\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        question = inputs[\"question\"]\n        get_chat_history = self.get_chat_history or _get_chat_history\n        chat_history_str = get_chat_history(inputs[\"chat_history\"])\n\n        if chat_history_str:\n            callbacks = _run_manager.get_child()\n            new_question = self.question_generator.run(\n                question=question, chat_history=chat_history_str, callbacks=callbacks\n            )\n        else:\n            new_question = question\n        accepts_run_manager = (\n            \"run_manager\" in inspect.signature(self._get_docs).parameters\n        )\n        if accepts_run_manager:\n            docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\n        else:\n            docs = self._get_docs(new_question, inputs)  # type: ignore[call-arg]\n        new_inputs = inputs.copy()\n        if self.rephrase_question:\n            new_inputs[\"question\"] = new_question\n        new_inputs[\"chat_history\"] = chat_history_str\n        answer = self.combine_docs_chain.run(\n            input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\n        )\n        output: Dict[str, Any] = {self.output_key: answer}\n        if self.return_source_documents:\n            output[\"source_documents\"] = docs\n        if self.return_generated_question:\n            output[\"generated_question\"] = new_question\n        return output\nIn simple terms, first, the question_generator chain is called that updates the input question/query based on chat history.\nNext, we retrieve the documents based on our new_question using similarity search.\nThese retrieved docs, then get passed to combine_docs_chain which combines the retrieved chunks and passes them over to a large language model (in this case gpt-3.5-turbo) to get back the answer.\nLet’s understand both chains one by one in the next two sections. That way, we will be able to have a solid understanding of our conversational retrieval chain.\n\n2.4.1 Question generator chain\nLet’s start out with the question generator. Remeber, the question generator takes in the user question and a chat history, and based on chat history, it updates the question to a new question.\nWhy does it do that? The question generator rephrases the original question to be a standalone question. So if it is a follow up question like “Why did that happen?” from the user, remember, we do not know what “that” is in this particular question.\nSo, what the question generator will do, is that it will look at the chat history, and fill information for the word “that” to update the question to be a standalone question. So the new question could be “Why did the rental prices increase in Australia?” based on chat history.\nWe will also be looking at a working example of this in our code in this section.\nFrom a code perspective, in langchain, the question_generator is an instance of LLMChain.\nIn this case the prompt for the question generator (LLMChain) is CONDENSE_QUESTION_PROMPT which looks like:\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n{chat_history}\n\nFollow Up Input: {question}\nStandalone question:\"\"\"\n\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\nSo taking in a chat_history and the original question (from the user), internally a new question get’s generated! This new question is a standalone question as discussed at the start of this section.\nLet’s see it in action. Let’s see how the original question get’s updated to a new question based on chat_history. Remember, the first time we interact with the question answer bot, chat history is NULL, so no new question is generated. But, it works from the second time forward.\nWe can get langchain to return the newly generated question by passing in return_generated_question=True to the ConversationRetrievalChain.\n\nqa.memory.chat_memory.messages\n\n[HumanMessage(content='Why is it so hard to find a rental property in Australia in June 2023?', additional_kwargs={}, example=False),\n AIMessage(content='In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.', additional_kwargs={}, example=False)]\n\n\nSo far, we have the above chat history. Let’s now ask a follow up question about the home price index and say “How has the pandemic affected this?” and we can see the question generator in action.\n\nqa(\"How has the pandemic affected this?\")\n\n{'question': 'How has the pandemic affected this?',\n 'chat_history': 'Human: Why is it so hard to find a rental property in Australia in June 2023?\\nAI: In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.',\n 'answer': 'The given context does not provide specific information about the rental property market in Australia in June 2023.',\n 'generated_question': 'How has the pandemic affected the rental property market in Australia in June 2023?'}\n\n\nAs can be seen above the original question was “How has the pandemic affected this?” which got updated to the generated_question - “How has the pandemic impacted the difficulty in finding a rental property in Australia in June 2023?”. This was done based on the chat history.\nAnd that’s all that there is to know about the question generator! We can now move on the document chain which is StuffDocumentsChain.\n\n\n2.4.2 Document chain\nThe stuff documents chain is available as combine_docs_chain attribute from the conversational retrieval chain.\nThe StuffDocumentsChain itself has a LLMChain of it’s own with the prompt\nsystem_template = \"\"\"Use the following pieces of context to answer the users question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}\"\"\"\nmessages = [\n    SystemMessagePromptTemplate.from_template(system_template),\n    HumanMessagePromptTemplate.from_template(\"{question}\"),\n]\nCHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n\n\nPROMPT_SELECTOR = ConditionalPromptSelector(\n    default_prompt=PROMPT, conditionals=[(is_chat_model, CHAT_PROMPT)]\n)\nSo, we to our prompt, we pass in the context and a follow up question. It specifically says “just say that you don’t know, don’t try to make up an answer.” This is good to limit hallucination.\nWhen we call the StuffDocumentsChain, it does two things - first it calls combine_docs. This method first combines the given input chunks by using separator \\n\\n to generate context.\n    def _get_inputs(self, docs: List[Document], **kwargs: Any) -&gt; dict:\n        # Format each document according to the prompt\n        doc_strings = [format_document(doc, self.document_prompt) for doc in docs]\n        # Join the documents together to put them in the prompt.\n        inputs = {\n            k: v\n            for k, v in kwargs.items()\n            if k in self.llm_chain.prompt.input_variables\n        }\n        inputs[self.document_variable_name] = self.document_separator.join(doc_strings)\n        return inputs\n\n\n    def combine_docs(\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\n    ) -&gt; Tuple[str, dict]:\n        inputs = self._get_inputs(docs, **kwargs)\n        # Call predict on the LLM.\n        return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\nGiven our question, remember, we first find the closest chunks to the question. These chunks are then joined together using \\n\\n separator.\n\n\n\n\n\n\nNote\n\n\n\n❓ I wonder how things would look like if we numbered the various chunks and passed in the context as bullet points?\n\n\nNext, we just call LLMChain’s predict method, this generates an answer using a prompt and returns the answer.\nYou know what? That’s really it! I hope that now you understand completely how context based question answering chatbots work when using langchain. :)"
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#conclusion",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#conclusion",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nIn langchain, once we have a vector database, below lines of code are enough to create a chatbot, that can answer user questions based on some “context”.\nimport os\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\npersist_directory = \"./db/\"\nopenai_api_key = os.environ['OPENAI_API_KEY']\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\ndb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", output_key='answer', return_messages=False)\n\n# create QA chain using `langchain`, database is used as vector store retriever to find \"context\" (using similarity search)\nqa = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    get_chat_history=lambda o:o,\n    memory=memory,\n    return_generated_question=True,\n    verbose=False,\n)\nWe saw all the steps in detail as part of this blog post. We also saw that the ConversationalRetrievalChain consists of two chains:\n\nQuestion generator chain (to generate a new standalone question based on chat history)\nDocuments chain (to combine chunks as context and answer question based on context)\n\nWe saw that both chains consist of llm_chain with different prompts. We even saw the two prompts in detail.\nAnd thus, we uncovered all the magic behind a conversational retrieval chain in langchain. I hope you enjoyed reading this blog post.\nPlease feel to reach out to me on twitter for any follow-up questions!"
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html",
    "href": "posts/2024-06-22 Support bot.html",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "",
    "text": "Problem Statement: You are the owner of a graphical and tech company called “DRAMA77IC” that creates dramatic and graphical visualisations for games for users all over the world. You have an API that contains information about various games such as genre, date of release, description and so on. You want to create a support channel, so your users can directly ask questions about your offerings through this Slack channel.\nNow that we have a well defined problem statement, let’s go about creating a solution using Claudette!\nRecently Answer.AI team released Claudette. It is built on top of Claude 3.5 Sonnet - the most powerful language model at the time of writing this blog post.\nAs part of this blog post, I will show you how to use Claudette to create a support bot built on top of Slack. You should be able to easily integrate the steps shown below to respond to user queries by calling any function.\nThere are two parts to this blog post.\nFinally, we might to test this out and showcase a demo."
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html#creating-a-slack-app",
    "href": "posts/2024-06-22 Support bot.html#creating-a-slack-app",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "1 Creating a slack APP",
    "text": "1 Creating a slack APP\nOn Slack, I created a new workspace for this blog post.\nNext, you want to go to https://api.slack.com/apps and create a new app.\n\nLet’s say DRAMA77IC is a tech company that works on games specifically creating dramatic and graphical games. This company has an API that can give out information about the various games that this company has built. The users of this company interact with the support team using a Slack channel (not discord, because we want to showcase slack-sdk as part of this blog post).\nNext, create a new slack app - “support-bot”.\nHere’s what you want to do:\n\nEnable socket mode, when you do this, a new APP level token will also be created with scope connections:write.\nGo over to OAuth & Permissions and add the following scopes:\n\napp_mentions:read\nchannels:history\nchannels:read\nchat:write\nim:history\nim:read\nim:write\nreactions:write\n\nEnable event subscriptions.\nInstall your bot to workspace and add it to your support channel.\n\n\nWe are now ready to start sending messages to public slack channels using our bot. Copy over your SLACK_APP_TOKEN and SLACK_BOT_TOKEN to a .env file and let’s use dotenv to load them.\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nTrue\n\n\nNow let’s make some imports and get our BOT_USER_ID. Each user in Slack has a user_id. To read more about the slack client, refer here.\n\nimport os\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\n\n\nclient = WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\nBOT_USER_ID = client.auth_test()[\"user_id\"]\nBOT_USER_ID\n\n'U07932L0L5U'\n\n\nYou can also add the channel ID to your dotenv, and we can load it like below:\n\nchannel_id = os.environ['LISTEN_CHANNEL_ID']\nchannel_id\n\n'C078V28044F'\n\n\nTo post the message to your channel, simply use client.chat_postMessage. But, before you do that, make sure to add the support-bot to your channel.\nI created a new channel called #blog and added support-bot to it.\n\nresponse = client.chat_postMessage(\n        channel=channel_id, \n        text=\"Bonjour! My name is Claudia, I am your support-bot for DRAMA77IC, a made-up company name for blogging purposes.\", \n)\n\n\nA little bit more about slack - each message in Slack a timestamp represented by ts.\n\nmessage = client.conversations_history(channel=channel_id)['messages'][0]\nmessage\n\n{'user': 'U07932L0L5U',\n 'type': 'message',\n 'ts': '1719101633.566529',\n 'bot_id': 'B079C5SV99A',\n 'app_id': 'A079C53DT9A',\n 'text': 'Bonjour! My name is Claudia, I am your support-bot for DRAMA77IC, a made-up company name for blogging purposes.',\n 'team': 'T079C2R49GC',\n 'bot_profile': {'id': 'B079C5SV99A',\n  'deleted': False,\n  'name': 'support-bot',\n  'updated': 1719017903,\n  'app_id': 'A079C53DT9A',\n  'icons': {'image_36': 'https://a.slack-edge.com/80588/img/plugins/app/bot_36.png',\n   'image_48': 'https://a.slack-edge.com/80588/img/plugins/app/bot_48.png',\n   'image_72': 'https://a.slack-edge.com/80588/img/plugins/app/service_72.png'},\n  'team_id': 'T079C2R49GC'},\n 'blocks': [{'type': 'rich_text',\n   'block_id': 'Xwfz',\n   'elements': [{'type': 'rich_text_section',\n     'elements': [{'type': 'text',\n       'text': 'Bonjour! My name is Claudia, I am your support-bot for DRAMA77IC, a made-up company name for blogging purposes.'}]}]}]}\n\n\nTo respond to this very message, we can pass in the timestamp as a thread_ts parameter.\n\nresponse = client.chat_postMessage(\n        channel=channel_id, \n        text=\"I was just told to respond to my own message. So I am doing that.\", \n        thread_ts=message['ts']\n)\n\n\nNow we have the basics in place to start working on our support-bot using Claudette."
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html#support-bot-using-claudette",
    "href": "posts/2024-06-22 Support bot.html#support-bot-using-claudette",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "2 Support Bot using Claudette",
    "text": "2 Support Bot using Claudette\nNext up, let’s get Claudette to respond to one of the user messages automatically. First things first, let’s install the library.\npip install claudette\n\nimport os\nfrom slack_sdk.web import WebClient\nfrom slack_sdk.socket_mode import SocketModeClient\nfrom slack_sdk.socket_mode.response import SocketModeResponse\nfrom slack_sdk.socket_mode.request import SocketModeRequest\nimport dotenv\nimport logging\nfrom datetime import datetime, timedelta\nimport time\nfrom claudette import *\n\n\n# claude's latest and most powerful version\nmodel='claude-3-5-sonnet-20240620'\n\n\nchat = Chat(model=model, \n            sp=\"You are Claudia. Do not share what tools you use to respond to user requests.\")\nchat(\"Hi, I'm Alice.\")\n\nHello Alice! It’s nice to meet you. I’m Claudia. How are you doing today? Is there anything in particular you’d like to chat about?\n\n\nid: msg_01HzdXGnHQFFtHAGMZfqU8Fj\ncontent: [{‘text’: “Hello Alice! It’s nice to meet you. I’m Claudia. How are you doing today? Is there anything in particular you’d like to chat about?”, ‘type’: ‘text’}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {‘input_tokens’: 31, ‘output_tokens’: 36}\n\n\n\n\nNow, the best part about claudette is that it allows function calling and it has been made really simple. Let’s take the example of the following games. Let’s say the company has the following five games - G1 to G5 and two customers C1 & C2.\n\ncustomers = {\n    \"C1\": dict(name=\"Alice Johnson\", email=\"alice@example.com\", phone=\"123-456-7890\",\n               games=[\"G1\", \"G2\", \"G3\"]),\n    \"C2\": dict(name=\"Bob Smith\", email=\"bob@example.com\", phone=\"987-654-3210\",\n               games=[\"G4\", \"G5\"])\n}\n\ngames = {\n    \"G1\": dict(id=\"G1\", name=\"Shadow Realms\", release_date=\"2023-03-15\", description=\"Navigate enchanted forests and haunted castles.\", status=\"Shipped\"),\n    \"G2\": dict(id=\"G2\", name=\"Solar Winds\", release_date=\"2023-07-22\", description=\"Explore space with stunning visuals and alien planets.\", status=\"Shipped\"),\n    \"G3\": dict(id=\"G3\", name=\"Mystic Legends\", release_date=\"2023-11-10\", description=\"Epic fantasy RPG with beautiful landscapes.\", status=\"Shipped\"),\n    \"G4\": dict(id=\"G4\", name=\"Cyber Revolution\", release_date=\"2024-02-28\", description=\"Dystopian future with advanced technology and cyber warfare.\", status=\"Shipped\"),\n    \"G5\": dict(id=\"G5\", name=\"Desert Storm\", release_date=\"2024-05-05\", description=\"Tactical shooter in a war-torn desert.\", status=\"Processing\")\n}\n\nLet’s say we have a database of games and customers. Let’s now define some functions.\n\ndef get_customer_info(\n    customer_id: str  # ID of the customer\n):  # Customer's name, email, phone number, and list of games\n    \"Retrieves a customer's information and their orders based on the customer ID\"\n    print(f'- Retrieving customer {customer_id}')\n    return customers.get(customer_id, \"Customer not found\")\n\ndef get_game_details(\n    game_id: str  # ID of the game\n):  # Game's ID, name, release date, description & status\n    \"Retrieves the details of a specific game based on the game ID\"\n    print(f'- Retrieving game {game_id}')\n    return games.get(game_id, \"Game not found\")\n\ndef return_game(\n    game_id:str # ID of the order to cancel\n)-&gt;bool: # True if the return is successful\n    \"Returns a game to the cmpany based on game ID.\"\n    print(f'- Returning game {game_id}')\n    if game_id not in games: return False\n    games[game_id]['status'] = 'Returned'\n    return True\n\nNow we can simply define these tools with claudette. Note, we no longer need to provide the chunky json version, claudette automatically handles that for us using docments.\n\ntools = [get_customer_info, get_game_details, return_game]\nchat = Chat(model, tools=tools)\n\nLet’s now do a function call as customer C1 and return one of the games.\n\nr = chat('Hi! How are you? This is Alice Johnson. (Customer ID: \"C1\")')\nprint(r.stop_reason)\nr.content\n\nend_turn\n\n\n[TextBlock(text=\"Hello Alice Johnson! It's great to hear from you. I'm doing well, thank you for asking. I hope you're doing well too. \\n\\nI see that you've provided your Customer ID. That's very helpful! Would you like me to retrieve your customer information and order details? I can do that for you using the Customer ID you've provided. This will allow me to assist you better with any questions or concerns you might have. \\n\\nShall I go ahead and fetch your customer information?\", type='text')]\n\n\n\nr = chat('Can you tell me more about the games I currently have? Just give me a list of games I own.')\nprint(r.stop_reason)\nr.content\n\n- Retrieving customer C1\ntool_use\n\n\n[TextBlock(text=\"Certainly, Alice! I'd be happy to help you with that. To get the information about the games you currently have, I'll need to retrieve your customer information first. I'll use the Customer ID you provided to do this.\", type='text'),\n ToolUseBlock(id='toolu_01TrdXW3C3VfJpcLi9UMeyYp', input={'customer_id': 'C1'}, name='get_customer_info', type='tool_use')]\n\n\nClaude recognises that we are doing a function call to retrieve information about C1. Claudette let’s you call the function automatically by simply calling it again.\n\nr = chat()\nprint(contents(r))\n\nThank you for your patience, Alice. I've retrieved your customer information, including the list of games you currently own. Here's a list of the games associated with your account:\n\n1. Game ID: G1\n2. Game ID: G2\n3. Game ID: G3\n\nThese are the games you currently have in your possession. Would you like more detailed information about any of these games? I can provide you with specific details for each game if you're interested. Just let me know which game(s) you'd like to know more about, and I'll be happy to fetch that information for you.\n\n\n\nr = chat(\"No, that's fine. Can you just return my game G2? I don't want it anymore.\")\nprint(r.stop_reason)\nr.content\n\n- Returning game G2\ntool_use\n\n\n[TextBlock(text=\"Certainly, Alice. I understand that you want to return the game with ID G2. I'd be happy to help you process that return. I'll use the return_game function to do this for you right away.\", type='text'),\n ToolUseBlock(id='toolu_018X45fRcYQ69TL4MUZS5rXg', input={'game_id': 'G2'}, name='return_game', type='tool_use')]\n\n\n\nr = chat()\nprint(contents(r))\n\nGreat news, Alice! The return for game G2 has been successfully processed. The system confirms that the return was completed successfully.\n\nTo summarize:\n- You've returned the game with ID G2.\n- The return has been recorded in our system.\n- You should no longer have this game in your possession.\n\nIs there anything else you'd like me to help you with regarding your games or account? Perhaps you'd like to know more about the remaining games you have, or if you have any other questions, I'm here to assist.\n\n\n\nr = chat(\"That's it. Thank you Claudia.\")\nprint(r.stop_reason)\nr.content\n\nend_turn\n\n\n[TextBlock(text=\"You're welcome, Alice! I'm glad I could help you with returning game G2. \\n\\nJust a small correction: my name isn't Claudia. I'm an AI assistant without a specific name. But I'm always here to help you with any questions or concerns you might have about your games or account.\\n\\nIs there anything else you need assistance with today? If not, I hope you have a wonderful day!\", type='text')]\n\n\nNow, that’s a good looking customer support conversation but we want to have this in Slack instead. For that case, we will write a small Python script that constantly monitors the channel and looks for new messages. Claudette only responds if the bot has been mentioned with “(support-bot?)”. Let’s go ahead and write that script now and show it in action.\n\n# we can also check the total tokens used in our conversation\nchat.use\n\nIn: 5609; Out: 658; Total: 6267\n\n\nThis is not very convenient, is it? To have to run chat() again every time when Claude wants to do a function call. In this case, we have another method called toolloop(). This continues to call functions until we are finished. Let’s take the example of having to delete all orders.\n\nchat.toolloop(\"Hey Claudia. Can you return all the games for me?\")\n\n- Returning game G1\n- Returning game G3\n\n\nGreat news! I’ve successfully processed the returns for both of your remaining games. Here’s a summary:\n\nGame G1: Successfully returned\nGame G3: Successfully returned\n\nAll of your games have now been returned to the company. Your account should no longer have any active game rentals.\nIs there anything else you would like me to help you with regarding your account or our services?\n\n\nid: msg_01SKrBoyXMbFNUF2uWGFErZK\ncontent: [{‘text’: “Great news! I’ve successfully processed the returns for both of your remaining games. Here’s a summary:. Game G1: Successfully returned. Game G3: Successfully returnedof your games have now been returned to the company. Your account should no longer have any active game rentals.there anything else you would like me to help you with regarding your account or our services?”, ‘type’: ‘text’}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {‘input_tokens’: 1633, ‘output_tokens’: 88}\n\n\n\n\nThere you go! Now, we were able to call multiple functions in a loop. Which is great. To confirm let’s check the games dict and confirm that the order status has changed.\n\ngames\n\n{'G1': {'id': 'G1',\n  'name': 'Shadow Realms',\n  'release_date': '2023-03-15',\n  'description': 'Navigate enchanted forests and haunted castles.',\n  'status': 'Returned'},\n 'G2': {'id': 'G2',\n  'name': 'Solar Winds',\n  'release_date': '2023-07-22',\n  'description': 'Explore space with stunning visuals and alien planets.',\n  'status': 'Returned'},\n 'G3': {'id': 'G3',\n  'name': 'Mystic Legends',\n  'release_date': '2023-11-10',\n  'description': 'Epic fantasy RPG with beautiful landscapes.',\n  'status': 'Returned'},\n 'G4': {'id': 'G4',\n  'name': 'Cyber Revolution',\n  'release_date': '2024-02-28',\n  'description': 'Dystopian future with advanced technology and cyber warfare.',\n  'status': 'Shipped'},\n 'G5': {'id': 'G5',\n  'name': 'Desert Storm',\n  'release_date': '2024-05-05',\n  'description': 'Tactical shooter in a war-torn desert.',\n  'status': 'Processing'}}\n\n\nAs can be seen from the dictionary above, we can see that games G1, G2 & G3 have been returned."
  },
  {
    "objectID": "posts/2024-06-22 Support bot.html#claudette-in-slack",
    "href": "posts/2024-06-22 Support bot.html#claudette-in-slack",
    "title": "Support bot with Claude 3.5 Sonnet using Claudette and Slack-SDK",
    "section": "3 Claudette in Slack",
    "text": "3 Claudette in Slack\nNow that we have a good idea on how to use claudette for function calling, let’s integrate it with Slack so that we can allow our support-bot to respond to user queries in a thread.\nMostly all, we need is a process function like below:\ndef process(client: SocketModeClient, req: SocketModeRequest):\n    print(req.payload)\n    if req.type == \"events_api\" or req.type == \"event_callback\":\n        response = SocketModeResponse(envelope_id=req.envelope_id)\n        client.send_socket_mode_response(response)\n        if (\n            req.payload[\"event\"][\"type\"] == \"message\"\n            and req.payload[\"event\"].get(\"subtype\") is None\n            and \"bot_profile\" not in req.payload[\"event\"].keys()\n        ):\n            thread_ts = req.payload[\"event\"][\"ts\"]\n            if \"thread_ts\" in req.payload[\"event\"].keys():\n                thread_ts = req.payload[\"event\"][\"thread_ts\"]\n            text = req.payload[\"event\"][\"text\"]\n            r = chat.toolloop(text, maxtok=200)\n            response = _client.chat_postMessage(\n                channel=CHANNEL_ID, text=contents(r), thread_ts=thread_ts\n            )\nUsing claudette has made this function really easy. Because claudette already takes care of state, and past messages, that is something we don’t have to worry about and can simply delegate to Claudette to take care of it all.\nOnce we get a request, we can get a timestamp, and if the user responds in a thread itself, then we get the timestamp from the thread. Next, we extract the message as a string and pass it over to claudette.\nUsing toolloop allows claudette to make function calls directly to Claude and return the answer. A sample conversation on Slack using this setup looks something like below."
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html",
    "href": "posts/2023-07-25-llmchain.html",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "",
    "text": "With large language models taking the world by storm ever since the release of ChatGPT, one framework that has been ubiquitous has been LangChain. Recently, I was myself working on building an economic chatbot using the framework and wanted to look into the source code of what goes inside this complex framework. As part of this blog post, we start small. We pick the simplest use-case of LLMChain and look at the source code to understand what goes inside the framework.\nLet’s say that we want to hear a joke about any product. We can use the LLMChain for this.\n# https://python.langchain.com/docs/modules/chains/foundational/llm_chain#get-started\nfrom langchain import LLMChain, OpenAI, PromptTemplate\nprompt_template = \"Tell me a joke that includes {product}?\"\nllm = OpenAI(temperature=0, openai_api_key=&lt;openai_api_key&gt;)\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate.from_template(prompt_template),\n    return_final_only=True,\n)\nprint(llm_chain(\"colorful socks\")['text'])\nThe above code is internally calls the OpenAI chat completion API to tell a joke about “colorful socks”. Here is the output from the model.\nQ: What did the sock say to the other sock when it was feeling blue?\nA: \"Cheer up, it could be worse, at least we're not white socks!\"\nAnd with that joke, let’s start looking into the source code of LangChain and understand everything that there is to know about LLMChain.\n\n\n\n\n\n\nNote\n\n\n\nAll code below has been copied from from LangChain. At the time of writing, this was the GIT commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. LangChain’s code might change in the future."
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#introduction",
    "href": "posts/2023-07-25-llmchain.html#introduction",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "",
    "text": "With large language models taking the world by storm ever since the release of ChatGPT, one framework that has been ubiquitous has been LangChain. Recently, I was myself working on building an economic chatbot using the framework and wanted to look into the source code of what goes inside this complex framework. As part of this blog post, we start small. We pick the simplest use-case of LLMChain and look at the source code to understand what goes inside the framework.\nLet’s say that we want to hear a joke about any product. We can use the LLMChain for this.\n# https://python.langchain.com/docs/modules/chains/foundational/llm_chain#get-started\nfrom langchain import LLMChain, OpenAI, PromptTemplate\nprompt_template = \"Tell me a joke that includes {product}?\"\nllm = OpenAI(temperature=0, openai_api_key=&lt;openai_api_key&gt;)\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate.from_template(prompt_template),\n    return_final_only=True,\n)\nprint(llm_chain(\"colorful socks\")['text'])\nThe above code is internally calls the OpenAI chat completion API to tell a joke about “colorful socks”. Here is the output from the model.\nQ: What did the sock say to the other sock when it was feeling blue?\nA: \"Cheer up, it could be worse, at least we're not white socks!\"\nAnd with that joke, let’s start looking into the source code of LangChain and understand everything that there is to know about LLMChain.\n\n\n\n\n\n\nNote\n\n\n\nAll code below has been copied from from LangChain. At the time of writing, this was the GIT commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. LangChain’s code might change in the future."
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#code-deep-dive",
    "href": "posts/2023-07-25-llmchain.html#code-deep-dive",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "2 Code: Deep-dive",
    "text": "2 Code: Deep-dive\nCalling any class in Python requires the __call__ method to be implemented. LLMChain in itself is a subclass of Chain which has the __call__ method implemented that looks like below:\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/chains/base.py#L185-L250\ndef __call__(\n        self,\n        inputs: Union[Dict[str, Any], Any],\n        return_only_outputs: bool = False,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        include_run_info: bool = False,\n    ) -&gt; Dict[str, Any]:\n        inputs = self.prep_inputs(inputs)\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\n        run_manager = callback_manager.on_chain_start(\n            dumpd(self),\n            inputs,\n        )\n        try:\n            outputs = (\n                self._call(inputs, run_manager=run_manager)\n                if new_arg_supported\n                else self._call(inputs)\n            )\nLooking at the above code we can see that it calls self.prep_inputs(inputs) and then calls self._call method. We wil ignore the self.prep_inputs part for now, as otherwise, this blog post will become too long. The self._call method inside the Chain class is an abstract method. Therefore, it must be implemented in LLMChain.\nLet’s look at the definition of it in LLMChain.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/chains/llm.py#L87-L93\ndef _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; Dict[str, str]:\n        response = self.generate([inputs], run_manager=run_manager)\n        return self.create_outputs(response)[0]\nOkay, great! Now, we see that the _call method calling self.generate passing in the [inputs]. Remember, the inputs were prepared in self.prep_inputs(inputs) step inside __call__ of Chain.\nBelow I have shared the source code of generate method from LLMChain.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/chains/llm.py#L95-L107\ndef generate(\n        self,\n        input_list: List[Dict[str, Any]],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; LLMResult:\n        \"\"\"Generate LLM result from inputs.\"\"\"\n        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n        return self.llm.generate_prompt(\n            prompts,\n            stop,\n            callbacks=run_manager.get_child() if run_manager else None,\n            **self.llm_kwargs,\n        )\nSo, it’s calling self.llm.generate_prompt. Great! At this point, I am starting to wonder, what’s the point of LLMChain at all? Also, let’s skip the self.prep_prompts part. Otherwise the blog post will be too long. Rather than looking at the source code of self.prep_prompts, let’s just look at the output of it.\n&gt;&gt; input_list\n[{'product': 'colorful socks'}]\n\n&gt;&gt; prompts, stop\n([StringPromptValue(text='Tell me a joke that includes colorful socks?')], None)\nSo now all that has happened is that the above code has added product value to out input prompt. Why is this not an f-string I wonder?\nBefore we go any further, because, now we are starting to look at self.llm’s source code and not so much on Chains let’s just look at the reponse from response = self.generate([inputs], run_manager=run_manager). Below is what the response looks like:\n&gt;&gt; response\nLLMResult(generations=[[Generation(text='\\n\\nQ: What did the sock say to the other sock when it was feeling blue?\\nA: \"Cheer up, it could be worse, at least we\\'re not white socks!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 9, 'total_tokens': 49, 'completion_tokens': 40}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('5afa5d25-802a-49eb-b147-0f708d207a33'))])\nNow we are ready to look into self.llm.generate_prompt, how did this method create the response that we see above?"
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#openai-llm",
    "href": "posts/2023-07-25-llmchain.html#openai-llm",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "3 OpenAI LLM",
    "text": "3 OpenAI LLM\nSo far we were looking at the LLMChain source code. But, internally that in-itself is calling self.llm.generate_prompt. If you remember from the top of the blog post, self.llm was an instance of OpenAI class.\nThe OpenAI class is a subclass of BaseOpenAI and that in itself is a subclass of BaseLLM and the generate_prompt method that was called from inside the generate method of ChainLLM is implemented in BaseLLM. A bit complicated, isn’t it?\n# \n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/base.py#L178-L186\ndef generate_prompt(\n        self,\n        prompts: List[PromptValue],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        prompt_strings = [p.to_string() for p in prompts]\n        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\nSo, now we can see that the generate_prompt method calls self.generate again. Remember, the last time we called self.generate it was for the LLMChain, but this time, it is for the OpenAI LLM. The p.to_string() part? That’s just converting our prompt to string. This is the output of prompt_strings looks like below:\n&gt;&gt; prompt_strings\n['Tell me a joke that includes colorful socks?']\n\n\n\n\n\n\nNote\n\n\n\n🤔 So far so good? Yes and No. I am a bit baffled at the amount of complexity in the code. I still don’t know why we had a Chain and a LLMChain. I guess we are looking at just one use-case of LLMChain which is generation, LLMChains might also be supporting other use cases where this complexity might be needed.\n\n\nTime to look at the generate method. It is again implemented in BaseLLM.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/base.py#L233-L302\ndef generate(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n        if not isinstance(prompts, list):\n            raise ValueError(\n                \"Argument 'prompts' is expected to be of type List[str], received\"\n                f\" argument of type {type(prompts)}.\"\n            )\n        params = self.dict()\n        params[\"stop\"] = stop\n        options = {\"stop\": stop}\n        (\n            existing_prompts,\n            llm_string,\n            missing_prompt_idxs,\n            missing_prompts,\n        ) = get_prompts(params, prompts)\n        disregard_cache = self.cache is not None and not self.cache\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n        new_arg_supported = inspect.signature(self._generate).parameters.get(\n            \"run_manager\"\n        )\n        if langchain.llm_cache is None or disregard_cache:\n            if self.cache is not None and self.cache:\n                raise ValueError(\n                    \"Asked to cache, but no cache found at `langchain.cache`.\"\n                )\n            run_managers = callback_manager.on_llm_start(\n                dumpd(self), prompts, invocation_params=params, options=options\n            )\n            output = self._generate_helper(\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            return output\n        if len(missing_prompts) &gt; 0:\n            run_managers = callback_manager.on_llm_start(\n                dumpd(self), missing_prompts, invocation_params=params, options=options\n            )\n            new_results = self._generate_helper(\n                missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            llm_output = update_cache(\n                existing_prompts, llm_string, missing_prompt_idxs, new_results, prompts\n            )\n            run_info = (\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\n                if run_managers\n                else None\n            )\n        else:\n            llm_output = {}\n            run_info = None\n        generations = [existing_prompts[i] for i in range(len(prompts))]\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)\nOkay, well, let’s look at the outputs step-by-step. This again is a majorly over-complicated piece of code. But, for what? Why do we need such complication?\n&gt;&gt; params\n{'model_name': 'text-davinci-003', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}, '_type': 'openai', 'stop': None}\nHonestly, looking at the above source code, I am still a bit confused at which point do we even call the model and call the create API. I can’t count how many layers down we are in code, and we still haven’t called the openai.Completion.create method when generating an output from the prompt should be as simple as:\nimport openai\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)\nI believe the part where the generations actually happen is inside the following piece of code.\noutput = self._generate_helper(\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n\n# Values of `output` (This is the same as `response` from before)\n&gt;&gt; output\nLLMResult(generations=[[Generation(text='\\n\\nQ: What did the sock say to the other sock when it was feeling blue?\\nA: \"Cheer up, it could be worse, at least we\\'re not white socks!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 9, 'total_tokens': 49, 'completion_tokens': 40}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('b32869ec-43a3-4780-805e-be482f1fe05b'))])\nSo, now we need to look at another method self._generate_helper and who knows what other methods that method will call. Let’s dig down more into the source code a bit further and look at self._generate_helper.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/base.py#L200-L231\ndef _generate_helper(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]],\n        run_managers: List[CallbackManagerForLLMRun],\n        new_arg_supported: bool,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        try:\n            output = (\n                self._generate(\n                    prompts,\n                    stop=stop,\n                    # TODO: support multiple run managers\n                    run_manager=run_managers[0] if run_managers else None,\n                    **kwargs,\n                )\n                if new_arg_supported\n                else self._generate(prompts, stop=stop)\n            )\n        except (KeyboardInterrupt, Exception) as e:\n            for run_manager in run_managers:\n                run_manager.on_llm_error(e)\n            raise e\n        flattened_outputs = output.flatten()\n        for manager, flattened_output in zip(run_managers, flattened_outputs):\n            manager.on_llm_end(flattened_output)\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\nUnbelievable! The self._generate_helper is further calling self._generate method which is an abstract method in BaseLLM but implemented in BaseOpenAI!\n\n\n\n\n\n\nNote\n\n\n\n😡 Are you still with me yet? We got to finish this. But, yes I am a bit frustrated. This whole part seems so complex and is so hard to follow and explain. Let’s recap what has happened so far. We wanted to look at what goes inside LangChain to create outputs from a prompt. What we have discovered is extremely puzzling. And I am being euphemistic here.\nWe started with LLMChain and called it using __call__ method that was implemented in Chain which called_call method of LLMChain that in-turn called self.generate which further called self.llm.generate_prompt.\nself.llm is an instance of OpenAI class which is subclass of BaseOpenAI which is subclass of BaseLLM and generate_prompt method is implemented there. We are not done yet.\nThe generate_prompt implemented in BaseLLM calls self.generate which in turn calls self._generate_helper that in turn calls self._generate of BaseOpenAI!\nIsn’t that a lot of code? And we still haven’t called the OpenAI API yet.\n\n\nOkay, I am normally breathing again. Let’s continue. We are still not done yet and haven’t generated our results. Let’s continue and look at the self._generate method which is implemented in BaseOpenAI.\n# https://github.com/langchain-ai/langchain/blob/24c165420827305e813f4b6d501f93d18f6d46a4/langchain/llms/openai.py#L272-L325\ndef _generate(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        params = self._invocation_params\n        params = {**params, **kwargs}\n        sub_prompts = self.get_sub_prompts(params, prompts, stop)\n        choices = []\n        token_usage: Dict[str, int] = {}\n        # Get the token usage from the response.\n        # Includes prompt, completion, and total tokens used.\n        _keys = {\"completion_tokens\", \"prompt_tokens\", \"total_tokens\"}\n        for _prompts in sub_prompts:\n            if self.streaming:\n                if len(_prompts) &gt; 1:\n                    raise ValueError(\"Cannot stream results with multiple prompts.\")\n                params[\"stream\"] = True\n                response = _streaming_response_template()\n                for stream_resp in completion_with_retry(\n                    self, prompt=_prompts, **params\n                ):\n                    if run_manager:\n                        run_manager.on_llm_new_token(\n                            stream_resp[\"choices\"][0][\"text\"],\n                            verbose=self.verbose,\n                            logprobs=stream_resp[\"choices\"][0][\"logprobs\"],\n                        )\n                    _update_response(response, stream_resp)\n                choices.extend(response[\"choices\"])\n            else:\n                response = completion_with_retry(self, prompt=_prompts, **params)\n                choices.extend(response[\"choices\"])\n            if not self.streaming:\n                # Can't update token usage if streaming\n                update_token_usage(_keys, response, token_usage)\n        return self.create_llm_result(choices, prompts, token_usage)\nDo we get to the part where we have results yet? Yes! The completion_with_retry function looks like it! Let’s look at the inputs to this function. It looks like the _generate also supports streaming.\n&gt;&gt; _prompts\n['Tell me a joke that includes colorful socks?']\n\n&gt;&gt; params\n{'model': 'text-davinci-003', 'api_key': '&lt;openai_api_key&gt;', 'api_base': '', 'organization': '', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\nWe are now ready to call the completion_with_retry passing in above as inputs. The _prompts is just the Prompt Template converted to a string. And the params are defined as defaults in BaseOpenAI.\ndef completion_with_retry(llm: Union[BaseOpenAI, OpenAIChat], **kwargs: Any) -&gt; Any:\n    \"\"\"Use tenacity to retry the completion call.\"\"\"\n    retry_decorator = _create_retry_decorator(llm)\n\n    @retry_decorator\n    def _completion_with_retry(**kwargs: Any) -&gt; Any:\n        return llm.client.create(**kwargs)\n\n    return _completion_with_retry(**kwargs)\nNow, self get’s passed as an argument to the function and finally we call llm.client.create(kwargs).\nBelow is what the kwargs look like. Remember, they are just _prompts & params mergfed together into a single dictionary.\n&gt;&gt; kwargs\n{'prompt': ['Tell me a joke that includes colorful socks?'], 'model': 'text-davinci-003', 'api_key': '&lt;api_key&gt;', 'api_base': '', 'organization': '', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}"
  },
  {
    "objectID": "posts/2023-07-25-llmchain.html#complexity-and-readability-in-langchain",
    "href": "posts/2023-07-25-llmchain.html#complexity-and-readability-in-langchain",
    "title": "Deciphering LangChain: A Deep Dive into Code Complexity",
    "section": "4 Complexity and Readability in LangChain",
    "text": "4 Complexity and Readability in LangChain\nGoing through the LangChain code, I find myself filled with a mix of admiration and confusion. It’s clear that the creators of LangChain put a lot of thought into building a flexible architecture. The design, which involves multiple layers of abstraction and many separate components, suggests that the tool is built to handle a wide range of use cases beyond the one we’ve examined today. However, in this particular scenario, we’ve seen how complexity can make code harder to follow and understand.\nLet’s start with the positives. The modular design of LangChain allows for easy extension and modification.\nOn the other hand, the complexity of the code made it difficult to trace the flow of execution. We found ourselves diving deeper and deeper into the call stack, and it took a considerable amount of time just to locate the point where the OpenAI API is actually called.\nAnother point of confusion was the usage of the self.dict() method. It seems that this method is intended to create a dictionary representation of an object’s attributes, but it’s not immediately clear why this is necessary. In some cases, it seemed that a simpler approach, such as using f-strings for prompt generation, could have achieved the same result with less code.\nIn conclusion, examining the LangChain code has provided valuable insights into the design decisions that go into creating a complex tool like this. While the abstraction and modularity are commendable, the complexity of the code can potentially make it harder for anyone to understand.\nWhat are your thoughts?"
  }
]