<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.21">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Arora">
<meta name="dcterms.date" content="2024-07-09">
<meta name="description" content="In this post, we take a deep dive into the architectural components of Gemma 2 such as Grouped Query Attention, Sliding Window Attention, RoPE Embeddings, Logit soft-capping &amp; Model-merging!">

<title>Gemma 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href=".././images/logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea1d7ac60288e0f1efdbc993fd8432ae.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-158677010-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="Gemma 2">
<meta name="twitter:description" content="In this post, we take a deep dive into the architectural components of Gemma 2 such as Grouped Query Attention, Sliding Window Attention, RoPE Embeddings, Logit soft-capping &amp; Model-merging!">
<meta name="twitter:image" content="../images/gemma2-intro.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Aman Arora’s Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">Aman Arora</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/amaarora"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/amaarora"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aroraaman/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Gemma 2</h1>
            <p class="subtitle lead">Improving Open Language Models at a Practical Size</p>
                  <div>
        <div class="description">
          <p>In this post, we take a deep dive into the architectural components of Gemma 2 such as Grouped Query Attention, Sliding Window Attention, RoPE Embeddings, Logit soft-capping &amp; Model-merging!</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Arora </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 9, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gemma-2-architectural-details" id="toc-gemma-2-architectural-details" class="nav-link active" data-scroll-target="#gemma-2-architectural-details"><span class="header-section-number">1</span> Gemma 2 architectural details</a></li>
  <li><a href="#sec-gqa" id="toc-sec-gqa" class="nav-link" data-scroll-target="#sec-gqa"><span class="header-section-number">2</span> Group Query Attention</a></li>
  <li><a href="#sec-swa" id="toc-sec-swa" class="nav-link" data-scroll-target="#sec-swa"><span class="header-section-number">3</span> Sliding Window Attention</a></li>
  <li><a href="#sec-rope" id="toc-sec-rope" class="nav-link" data-scroll-target="#sec-rope"><span class="header-section-number">4</span> Rotary Positional Embeddings (RoPE)</a></li>
  <li><a href="#sec-logits" id="toc-sec-logits" class="nav-link" data-scroll-target="#sec-logits"><span class="header-section-number">5</span> Logit soft-capping</a></li>
  <li><a href="#sec-merge" id="toc-sec-merge" class="nav-link" data-scroll-target="#sec-merge"><span class="header-section-number">6</span> Model merging</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">7</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>I am sure by now you would have seen <a href="https://blog.google/technology/developers/google-gemma-2/">Gemma 2’s announcement</a> or <a href="https://aistudio.google.com/app/prompts/new_chat?model=gemma-2-27b-it">played around with the model</a>. If you haven’t yet, I highly recommend that you do.</p>
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/gemma2-bench.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Comparison of Gemma 2 models on a variety of benchmarks
</figcaption>
</figure>
</div>
<p>Going by the benchmarks shared in the official Gemma 2 report, the model is extremely competitive and outperforming other models relative to it’s size.</p>
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/gemma2-chatarena.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Evaluation of Gemma 2 9B and 27B Instruction Tuned models on the Chatbot Arena
</figcaption>
</figure>
</div>
<p>Also, as can be seen in <a href="#fig-2" class="quarto-xref">Figure&nbsp;2</a>, the results from Chatbot arena for Gemma 2 models look pretty impressive, given the model sizes.</p>
<p>Below, I try to summarise <strong>“Why is Gemma 2 such a big deal?”</strong>:</p>
<ul>
<li>The model comes in <strong>practical sizes</strong> 3B, 9B &amp; 27B that can fit on a single GPU (at the time of writing this blog post, the 3B version is yet to be released)</li>
<li>Performance of the Gemma 2 models is on par with models twice or more it’s size!</li>
<li>Model weights are open-source - thank you Google Deepmind!</li>
</ul>
<p>As part of this blog post, we will be going deeper into some of the architectural components of Gemma 2 <strong>along with their implementation in PyTorch</strong>. Specifically we will be looking into:</p>
<ol type="1">
<li><em>Grouped Query Attention</em> (<a href="#sec-gqa" class="quarto-xref">Section&nbsp;2</a>)</li>
<li><em>Sliding Window Attention</em> (<a href="#sec-swa" class="quarto-xref">Section&nbsp;3</a>)</li>
<li><em>Rotary Position Embeddings (RoPE)</em> (<a href="#sec-rope" class="quarto-xref">Section&nbsp;4</a>)</li>
<li><em>Logit soft-capping</em> (<a href="#sec-logits" class="quarto-xref">Section&nbsp;5</a>)</li>
<li><em>Model merging</em> (<a href="#sec-merge" class="quarto-xref">Section&nbsp;6</a>)</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Gentle introduction to Gemma 2
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a more gentle introduction, I would like to refer the readers to <a href="https://huggingface.co/blog/gemma2">Welcome Gemma 2 - Google’s new open LLM</a> by Huggingface.</p>
</div>
</div>
<section id="gemma-2-architectural-details" class="level2 page-columns page-full" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="gemma-2-architectural-details"><span class="header-section-number">1</span> Gemma 2 architectural details</h2>
<p>In this section, we look into the architecture details as shared in the report - <a href="https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf">Gemma 2: Improving Open Language Models at a Practical Size</a>.</p>
<p>From the report:</p>
<div class="page-columns page-full"><p><em>In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. The 9 billion and 27 billion parameter models are available today, with a 2 billion parameter model to be released shortly. In this new version, we provide several technical modifications to our architecture, such as interleaving local-global attentions (<span class="citation" data-cites="longformer">Beltagy, Peters, and Cohan (<a href="#ref-longformer" role="doc-biblioref">2020</a>)</span>) and group-query attention (<span class="citation" data-cites="gqa">Ainslie et al. (<a href="#ref-gqa" role="doc-biblioref">2023</a>)</span>). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger.</em></p><div class="no-row-height column-margin column-container"></div></div>
<div class="callout callout-style-default callout-note callout-titled" data-collapsible="True">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I guess the key point that me hooked to Gemma 2 was the last line shared in the abstract:</p>
<p><em>The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger.</em></p>
<p>This is pretty big news, and very important for the projects that I have been recently working on. Smaller models in productions means - lower latency, lower memory requirements, faster runtime, thus, an overall reduction in computing costs.</p>
</div>
</div>
<p>The recent large language models <span class="citation" data-cites="llama3">AI@Meta (<a href="#ref-llama3" role="doc-biblioref">2024</a>)</span>, have been known to have dataset sizes as big as 15T tokens! It is the longer training on bigger datasets that has been key towards LLMs having continued improvements in performance. The models are trained to predict the next tokens in a left-to-right manner.</p>
<div class="no-row-height column-margin column-container"><div id="ref-llama3" class="csl-entry" role="listitem">
AI@Meta. 2024. <span>“Llama 3 Model Card.”</span> <a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a>.
</div></div><p>In Gemma 2, the authors trained the smaller 2.6B and 9B models using knowledge distillation. This, alongside other architecture details, has allowed Gemma 2 to have the best in class performance given it’s size. Let’s look into each one of the components in the following sections.</p>
</section>
<section id="sec-gqa" class="level2 page-columns page-full" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-gqa"><span class="header-section-number">2</span> Group Query Attention</h2>
<p>Grouped query attention was introduced by <span class="citation" data-cites="gqa">Ainslie et al. (<a href="#ref-gqa" role="doc-biblioref">2023</a>)</span> in 2023. The key difference as compared to the standard Multi-headed attention has been highlighted in <a href="#fig-1" class="quarto-xref">Figure&nbsp;3</a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gqa" class="csl-entry" role="listitem">
Ainslie, Joshua, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. <span>“GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.”</span> <a href="https://arxiv.org/abs/2305.13245">https://arxiv.org/abs/2305.13245</a>.
</div></div><p>In this part of the blog post, we understand more about Group Query Attention and implement in in PyTorch code from scratch.</p>
<p>For an introduction and in-depth understand to multi-head attention, I would like to refer the reader to my previous blog post on <a href="https://amaarora.github.io/posts/2021-01-18-ViT.html">Vision Transformer</a> where we implement attention from scratch in PyTorch in <a href="https://amaarora.github.io/posts/2021-01-18-ViT.html#the-vision-transformer-in-pytorch">Section 8</a>.</p>
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/gqa.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Group Query Attention Overview
</figcaption>
</figure>
</div>
<p>From the <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> paper, attention mechanism was introduced using the formula:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>In Grouped Query Attention, we reduce the number of key and value heads (thus, in a way, grouping heads together as shown in <a href="#fig-1" class="quarto-xref">Figure&nbsp;3</a>). If the number of keys &amp; value heads is reduced to 1, it is equivalent to Multi-Query Attention <span class="citation" data-cites="mqa">Shazeer (<a href="#ref-mqa" role="doc-biblioref">2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mqa" class="csl-entry" role="listitem">
Shazeer, Noam. 2019. <span>“Fast Transformer Decoding: One Write-Head Is All You Need.”</span> <a href="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a>.
</div></div><p>Thus, Group Query Attention (GQA) is somewhere in the middle between MHA &amp; MQA. Let’s now implement it in PyTorch.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>GQA implementation in PyTorch
</div>
</div>
<div class="callout-body-container callout-body">
<p>We modify the implementation from Meta’s Llama-3 repo <a href="https://github.com/meta-llama/llama3/blob/main/llama/model.py#L90">here</a>. Basically, we removed rotary embeddings, KV caching, and model parallelization to keep the implementation to a bare minimum.</p>
</div>
</div>
<div id="0bf1de4d" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Having made the imports, let’s define the model arguments. We assume that the input and output dimensions inside the Decoder layer are <span class="math inline">\(4096\)</span>.</p>
<p>Below, the <code>n_kv_heads</code> defines the number of key &amp; value heads. If the number is equal to 1, the below Attention implementation follows Multi-Query Attention. When the number is greater than 1 and less than <code>n_heads</code>, then we follow Group Query Attention as in <a href="#fig-1" class="quarto-xref">Figure&nbsp;3</a>.</p>
<div id="31d9b269" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelArgs:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4096</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    n_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    n_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    n_kv_heads: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    vocab_size: <span class="bu">int</span> <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>  <span class="co"># defined later by tokenizer</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    multiple_of: <span class="bu">int</span> <span class="op">=</span> <span class="dv">256</span>  <span class="co"># make SwiGLU hidden layer size multiple of large power of 2</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    ffn_dim_multiplier: Optional[<span class="bu">float</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    norm_eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    max_batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    max_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For our implementation, we assume 8 key &amp; value heads whereas 32 query heads.</p>
<div id="711c2745" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>args <span class="op">=</span> ModelArgs()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>args</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=2048)</code></pre>
</div>
</div>
<p>Below, follows a standard implementation of Attention (without positional embeddings to keep it simple). We have our weight matrices for q,k &amp; v layers defined as Linear layers. These weight matrices transform an input tensor <span class="math inline">\(X\)</span> to query <span class="math inline">\(q\)</span>, key <span class="math inline">\(k\)</span> &amp; value <span class="math inline">\(v\)</span> respectively.</p>
<p>Taking in an input of shape <span class="math inline">\((2, 32, 4096)\)</span> which represents a batch of 2 sequences of length 32, each represented by a 4096 long vector.</p>
<p>Upon taking the transform, given the weight matrices <code>self.wq</code>, <code>self.wk</code> &amp; <code>self.wv</code>, the dimensions for our <span class="math inline">\(q\)</span>, <span class="math inline">\(k\)</span> &amp; <span class="math inline">\(v\)</span> matrices will be:</p>
<p><span class="math inline">\(q\)</span> <span class="math inline">\(-&gt;\)</span> <span class="math inline">\((2, 32, 4096)\)</span></p>
<p><span class="math inline">\(k\)</span> <span class="math inline">\(-&gt;\)</span> <span class="math inline">\((2, 32, 1024)\)</span></p>
<p><span class="math inline">\(v\)</span> <span class="math inline">\(-&gt;\)</span> <span class="math inline">\((2, 32, 1024)\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Time to take a break and “think” 💭
</div>
</div>
<div class="callout-body-container callout-body">
<p>This would be a great time for you to take a break and think about the dimensions. Can you reason in your head as to why <span class="math inline">\(k\)</span> and <span class="math inline">\(v\)</span> are of dimensions <span class="math inline">\((2,32,1024)\)</span>?</p>
<p>Hint: We have fewer number of k,v heads by an order of magnitude of “4”.</p>
</div>
</div>
<div id="36e6c1d9" class="cell" data-code_folding="[31]" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Attention(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, args: ModelArgs):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_kv_heads <span class="op">=</span> args.n_heads <span class="cf">if</span> args.n_kv_heads <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> args.n_kv_heads</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_local_heads <span class="op">=</span> args.n_heads</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_local_kv_heads <span class="op">=</span> <span class="va">self</span>.n_kv_heads</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_rep <span class="op">=</span> <span class="va">self</span>.n_local_heads <span class="op">//</span> <span class="va">self</span>.n_local_kv_heads</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> args.dim <span class="op">//</span> args.n_heads</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wq <span class="op">=</span> nn.Linear(args.dim, args.n_heads <span class="op">*</span> <span class="va">self</span>.head_dim, bias<span class="op">=</span><span class="va">False</span>,)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wk <span class="op">=</span> nn.Linear(args.dim, args.n_kv_heads <span class="op">*</span> <span class="va">self</span>.head_dim, bias<span class="op">=</span><span class="va">False</span>,)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wv <span class="op">=</span> nn.Linear(args.dim, args.n_kv_heads <span class="op">*</span> <span class="va">self</span>.head_dim, bias<span class="op">=</span><span class="va">False</span>,)            </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wo <span class="op">=</span> nn.Linear(args.n_heads <span class="op">*</span> <span class="va">self</span>.head_dim, args.dim, bias<span class="op">=</span><span class="va">False</span>,)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        x: torch.Tensor,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        bsz, seqlen, _ <span class="op">=</span> x.shape</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        xq, xk, xv <span class="op">=</span> <span class="va">self</span>.wq(x), <span class="va">self</span>.wk(x), <span class="va">self</span>.wv(x)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        xq <span class="op">=</span> xq.view(bsz, seqlen, <span class="va">self</span>.n_local_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        xk <span class="op">=</span> xk.view(bsz, seqlen, <span class="va">self</span>.n_local_kv_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        xv <span class="op">=</span> xv.view(bsz, seqlen, <span class="va">self</span>.n_local_kv_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># repeat k/v heads if n_kv_heads &lt; n_heads</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        xk <span class="op">=</span> repeat_kv(xk, <span class="va">self</span>.n_rep)  <span class="co"># (bs, seqlen, n_local_heads, head_dim)</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        xv <span class="op">=</span> repeat_kv(xv, <span class="va">self</span>.n_rep)  <span class="co"># (bs, seqlen, n_local_heads, head_dim)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        xq <span class="op">=</span> xq.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (bs, n_local_heads, seqlen, head_dim)</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        xk <span class="op">=</span> xk.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        xv <span class="op">=</span> xv.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(xq, xk.transpose(<span class="dv">2</span>, <span class="dv">3</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.head_dim)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> F.softmax(scores.<span class="bu">float</span>(), dim<span class="op">=-</span><span class="dv">1</span>).type_as(xq)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.matmul(scores, xv)  <span class="co"># (bs, n_local_heads, seqlen, head_dim)</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(bsz, seqlen, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.wo(output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The above implementation really follows <a href="#fig-1" class="quarto-xref">Figure&nbsp;3</a> very closely. First, we get the dimension per head <code>self.head_dim</code>, by simply doing <code>args.dim // args.n_heads</code>. Given the values, in this case, each head has a dimension of <span class="math inline">\(128\)</span>.</p>
<p>Now, after the matrix multiplication with weight matrices, we do a reshape to get our <span class="math inline">\(xq\)</span>, <span class="math inline">\(xk\)</span> &amp; <span class="math inline">\(xv\)</span> values.</p>
<p>Can you think what their dimensions would be?</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Time to take a break and “think” 💭
</div>
</div>
<div class="callout-body-container callout-body">
<p>The dimensions for <span class="math inline">\(xq\)</span>, <span class="math inline">\(xk\)</span> &amp; <span class="math inline">\(xv\)</span> are <span class="math inline">\([2, 32, 32, 128]\)</span>, <span class="math inline">\([2, 32, 8, 128]\)</span> &amp; <span class="math inline">\([2, 32, 32, 128]\)</span> respectively. Thereby, we are doing a “grouped” attention, because 4 queries get grouped to work a single key &amp; value pair.</p>
</div>
</div>
<p>In practice, we just repeat the <span class="math inline">\(k\)</span> &amp; <span class="math inline">\(v\)</span> values, in this case <code>n_rep</code> is 4 to get <span class="math inline">\(k\)</span> and <span class="math inline">\(v\)</span> to have tensors of shape <span class="math inline">\([2, 32, 32, 128]\)</span>. We do this using the <code>repeat_kv</code> function below.</p>
<div id="2e8ef1b1" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> repeat_kv(x: torch.Tensor, n_rep: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""torch.repeat_interleave(x, dim=2, repeats=n_rep)"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    bs, slen, n_kv_heads, head_dim <span class="op">=</span> x.shape</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_rep <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        x[:, :, :, <span class="va">None</span>, :]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        .expand(bs, slen, n_kv_heads, n_rep, head_dim)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        .reshape(bs, slen, n_kv_heads <span class="op">*</span> n_rep, head_dim)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For a detailed explanation of this <code>repeat_kv</code> function, refer <a href="https://github.com/meta-llama/llama/issues/384#issuecomment-1641359877">here</a>.</p>
<p>And that’s really it. After that, we calculate our attention scores as usual, using the attention formula:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<div id="e9203004" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">32</span>, <span class="dv">4096</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> Attention(args)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>attn(X).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>torch.Size([2, 32, 4096])</code></pre>
</div>
</div>
<p>And that’s really all the magic that there is behind Group Query Attention (GQA)! You have just succesfully implemented it from scratch using PyTorch yourself!</p>
</section>
<section id="sec-swa" class="level2 page-columns page-full" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-swa"><span class="header-section-number">3</span> Sliding Window Attention</h2>
<p>For a detailed explanation &amp; implementation in PyTorch of Sliding Window Attention <span class="citation" data-cites="longformer">Beltagy, Peters, and Cohan (<a href="#ref-longformer" role="doc-biblioref">2020</a>)</span>, I would like to refer the readers to my <a href="https://amaarora.github.io/posts/2024-07-04%20SWA.html">previous blog post</a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-longformer" class="csl-entry" role="listitem">
Beltagy, Iz, Matthew E. Peters, and Arman Cohan. 2020. <span>“Longformer: The Long-Document Transformer.”</span> <a href="https://arxiv.org/abs/2004.05150">https://arxiv.org/abs/2004.05150</a>.
</div></div><p>The authors interleaved local and global attentions in alternating layers, which helped reduce number of parameters (for compact model-size) while mantaining performance. This is pretty unique! From the paper:</p>
<p><em>The sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens.</em></p>
</section>
<section id="sec-rope" class="level2 page-columns page-full" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-rope"><span class="header-section-number">4</span> Rotary Positional Embeddings (RoPE)</h2>
<p>RoPE were introduced as part of the RoFormer architecture <span class="citation" data-cites="roformer">Su et al. (<a href="#ref-roformer" role="doc-biblioref">2023</a>)</span>. From the paper itself:</p>
<div class="no-row-height column-margin column-container"><div id="ref-roformer" class="csl-entry" role="listitem">
Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. <span>“RoFormer: Enhanced Transformer with Rotary Position Embedding.”</span> <a href="https://arxiv.org/abs/2104.09864">https://arxiv.org/abs/2104.09864</a>.
</div></div><p><em>The proposed Rotary Position Embedding (RoPE) encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. We evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives.</em></p>
<p>The Roformer has been integrated in the transformers library and can be used like so:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, RoFormerModel</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"junnyu/roformer_chinese_base"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RoFormerModel.from_pretrained(<span class="st">"junnyu/roformer_chinese_base"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"Hello, my dog is cute"</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>model(<span class="op">**</span>inputs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div id="01d2e807" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The below are the Positional Encodings from the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> by <span class="citation" data-cites="attention">Vaswani et al. (<a href="#ref-attention" role="doc-biblioref">2017</a>)</span> paper:</p>
<div class="no-row-height column-margin column-container"><div id="ref-attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>CoRR</em> abs/1706.03762. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.
</div></div><p><span class="math display">\[
PE_{(pos, 2i)} = \sin \left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right)
\]</span></p>
<p><span class="math display">\[
PE_{(pos, 2i+1)} = \cos \left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right)
\]</span></p>
<p>They work with absolute positions, but, not with relative positions. From Huggingface’s implementation of the RoFormer architecture, this is how one could implement them in PyTorch code:</p>
<div id="cb5373b2-5b3a-4bdb-8fde-eefc5498bcc0" class="cell" data-code_folding="[]" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RoFormerSinusoidalPositionalEmbedding(nn.Embedding):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This module produces sinusoidal positional embeddings of any length."""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_positions: <span class="bu">int</span>, embedding_dim: <span class="bu">int</span>, padding_idx: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(num_positions, embedding_dim)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> <span class="va">self</span>._init_weight(<span class="va">self</span>.weight)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_weight(out: nn.Parameter) <span class="op">-&gt;</span> nn.Parameter:</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">        the 2nd half of the vector. [dim // 2:]</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        n_pos, dim <span class="op">=</span> out.shape</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        position_enc <span class="op">=</span> np.array(</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>            [[pos <span class="op">/</span> np.power(<span class="dv">10000</span>, <span class="dv">2</span> <span class="op">*</span> (j <span class="op">//</span> <span class="dv">2</span>) <span class="op">/</span> dim) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(dim)] <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(n_pos)]</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        out.requires_grad <span class="op">=</span> <span class="va">False</span>  <span class="co"># set early to avoid an error in pytorch-1.8+</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        sentinel <span class="op">=</span> dim <span class="op">//</span> <span class="dv">2</span> <span class="cf">if</span> dim <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> (dim <span class="op">//</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        out[:, <span class="dv">0</span>:sentinel] <span class="op">=</span> torch.FloatTensor(np.sin(position_enc[:, <span class="dv">0</span>::<span class="dv">2</span>]))</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        out[:, sentinel:] <span class="op">=</span> torch.FloatTensor(np.cos(position_enc[:, <span class="dv">1</span>::<span class="dv">2</span>]))</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        out.detach_()</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids_shape: torch.Size, past_key_values_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""`input_ids_shape` is expected to be [bsz x seqlen]."""</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        bsz, seq_len <span class="op">=</span> input_ids_shape[:<span class="dv">2</span>]</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>            past_key_values_length, past_key_values_length <span class="op">+</span> seq_len, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span><span class="va">self</span>.weight.device</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().forward(positions)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>I don’t go much into the detail of the implementation of <code>RoFormerSinusoidalPositionalEmbedding</code>, since it is pretty self-explanatory when we compare the implementation with the formula.</p>
<div class="{callout-note}">
<p>I would like to refer the readers to <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding">The Annotated Transformer</a> for another resource on positional encodings.</p>
</div>
<p>There are some challenges to using absolute position encodings as above that I highlight below:</p>
<ol type="1">
<li>The self-attention architecture has shown to be position agnostic. Thus, by adding positional information to the context representation, it renders them unsuitable for the linear self-attention architecture. <span class="citation" data-cites="yun2020">Yun et al. (<a href="#ref-yun2020" role="doc-biblioref">2020</a>)</span></li>
<li>These encodings do-not follow the intuition that tokens close to each other should have more importance compared to tokens further away from each other.</li>
<li>The sequences at test-time might be of different length to trainining-time, thus, leading to train-test discrepency.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-yun2020" class="csl-entry" role="listitem">
Yun, Chulhee, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. 2020. <span>“Are Transformers Universal Approximators of Sequence-to-Sequence Functions?”</span> <a href="https://arxiv.org/abs/1912.10077">https://arxiv.org/abs/1912.10077</a>.
</div></div><p>Thus, there is a need for positional encodings that overcome the above two challenges. From the RoPE paper:</p>
<p><em>We introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. Specifically, RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Note that the proposed RoPE is prioritized over the existing methods through valuable properties, including the sequence length flexibility, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding.</em></p>
<p>By utilising a derived rotation matrix, through RoPE, the authors were able to overcome the challenges and come up with a solution that not only solves the problem in theory but these embeddings are also easy to implement in practice! Thus, the widespread use of RoPE throughout multiple LLMs.</p>
<div class="{callout-note}">
<p>In this blog post, we do not go into the derivation of RoPE. I would like the readers to refer to another wonderful blog post by Eleuther AI that goes into the mathematical details - <a href="https://blog.eleuther.ai/rotary-embeddings/">Rotary Embeddings: A Relative Revolution</a>.</p>
</div>
<p>Rotary Position Embeddings can be implemented easily using the following matrix multiplication, where</p>
<p><span class="math inline">\(x_{i}\)</span>: contextual representation of token <span class="math inline">\(x\)</span> at position <span class="math inline">\(i\)</span>. (<code>nn.Embedding</code>)</p>
<p><span id="eq-1"><span class="math display">\[
R_{\Theta,m}^d x = \begin{pmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
\vdots \\
x_{d-1} \\
x_d
\end{pmatrix} \otimes \begin{pmatrix}
\cos m\theta_1 \\
\cos m\theta_1 \\
\cos m\theta_2 \\
\cos m\theta_2 \\
\vdots \\
\cos m\theta_{d/2} \\
\cos m\theta_{d/2}
\end{pmatrix} + \begin{pmatrix}
-x_2 \\
x_1 \\
-x_4 \\
x_3 \\
\vdots \\
-x_d \\
x_{d-1}
\end{pmatrix} \otimes \begin{pmatrix}
\sin m\theta_1 \\
\sin m\theta_1 \\
\sin m\theta_2 \\
\sin m\theta_2 \\
\vdots \\
\sin m\theta_{d/2} \\
\sin m\theta_{d/2}
\end{pmatrix}
\tag{1}\]</span></span></p>
<p>We can get the sinusoidal and cosine values of the matrix multiplication from <code>RoFormerSinusoidalPositionalEmbedding</code>.</p>
<div id="b9317457" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> RoFormerSinusoidalPositionalEmbedding(<span class="dv">100</span>, <span class="dv">64</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>sinusoidal_pos <span class="op">=</span> embedding_layer([<span class="dv">1</span>, <span class="dv">9</span>])[<span class="va">None</span>, <span class="va">None</span>, :, :]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>sinusoidal_pos.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>torch.Size([1, 1, 9, 64])</code></pre>
</div>
</div>
<p>Assuming 12 attention heads, each with a dimension of 64, we can randomly initialise our query and key layer like so:</p>
<div id="7e4f6d59" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>query_layer <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">64</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>key_layer   <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">64</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>query_layer.shape, key_layer.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>(torch.Size([1, 12, 9, 64]), torch.Size([1, 12, 9, 64]))</code></pre>
</div>
</div>
<div id="5d6404b1" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://kexue.fm/archives/8265</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sin [batch_size, num_heads, sequence_length, embed_size_per_head//2]</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cos [batch_size, num_heads, sequence_length, embed_size_per_head//2]</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    sin, cos <span class="op">=</span> sinusoidal_pos.chunk(<span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sin [θ0,θ1,θ2......θd/2-1] -&gt; sin_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    sin_pos <span class="op">=</span> torch.stack([sin, sin], dim<span class="op">=-</span><span class="dv">1</span>).reshape_as(sinusoidal_pos)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cos [θ0,θ1,θ2......θd/2-1] -&gt; cos_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    cos_pos <span class="op">=</span> torch.stack([cos, cos], dim<span class="op">=-</span><span class="dv">1</span>).reshape_as(sinusoidal_pos)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rotate_half_query_layer [-q1,q0,-q3,q2......,-qd-1,qd-2]</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    rotate_half_query_layer <span class="op">=</span> torch.stack([<span class="op">-</span>query_layer[..., <span class="dv">1</span>::<span class="dv">2</span>], query_layer[..., ::<span class="dv">2</span>]], dim<span class="op">=-</span><span class="dv">1</span>).reshape_as(</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        query_layer</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    query_layer <span class="op">=</span> query_layer <span class="op">*</span> cos_pos <span class="op">+</span> rotate_half_query_layer <span class="op">*</span> sin_pos</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rotate_half_key_layer [-k1,k0,-k3,k2......,-kd-1,kd-2]</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    rotate_half_key_layer <span class="op">=</span> torch.stack([<span class="op">-</span>key_layer[..., <span class="dv">1</span>::<span class="dv">2</span>], key_layer[..., ::<span class="dv">2</span>]], dim<span class="op">=-</span><span class="dv">1</span>).reshape_as(key_layer)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    key_layer <span class="op">=</span> key_layer <span class="op">*</span> cos_pos <span class="op">+</span> rotate_half_key_layer <span class="op">*</span> sin_pos</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> value_layer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># rotate_half_value_layer [-v1,v0,-v3,v2......,-vd-1,vd-2]</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        rotate_half_value_layer <span class="op">=</span> torch.stack([<span class="op">-</span>value_layer[..., <span class="dv">1</span>::<span class="dv">2</span>], value_layer[..., ::<span class="dv">2</span>]], dim<span class="op">=-</span><span class="dv">1</span>).reshape_as(</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>            value_layer</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        value_layer <span class="op">=</span> value_layer <span class="op">*</span> cos_pos <span class="op">+</span> rotate_half_value_layer <span class="op">*</span> sin_pos</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> query_layer, key_layer, value_layer</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> query_layer, key_layer</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now, one could simply apply the rotary position embeddings using the above function <code>apply_rotary_position_embeddings</code>. Note that <code>rotate_half_query_layer</code> is just the following matrix:</p>
<span class="math display">\[\begin{pmatrix}
-x_2 \\
x_1 \\
-x_4 \\
x_3 \\
\vdots \\
-x_d \\
x_{d-1}
\end{pmatrix}\]</span>
<p>Finally, by doing <code>query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos</code>, we are replicating the matrix multiplication as in <a href="#eq-1" class="quarto-xref">Equation&nbsp;1</a>.</p>
<div id="953b3ee0" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>query_layer, key_layer <span class="op">=</span> apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>query_layer.shape, key_layer.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>(torch.Size([1, 12, 9, 64]), torch.Size([1, 12, 9, 64]))</code></pre>
</div>
</div>
<p>And that is all that there is to Rotary Position Embeddings. We have successfully re-implemented RoPE in PyTorch.</p>
</section>
<section id="sec-logits" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sec-logits"><span class="header-section-number">5</span> Logit soft-capping</h2>
<p>Another trick that was used by the authors of Gemma 2 was logit soft capping. Generally we use <code>torch.clip</code> or <code>torch.clamp</code> which is more like hard clipping. Instead the authors utilised soft-capping which can be formulated as:</p>
<p><span class="math display">\[\text{logits} \leftarrow \text{soft\_cap} * \tanh\left(\frac{\text{logits}}{\text{soft\_cap}}\right)\]</span></p>
<p>Let’s have a look at the <strong>tanh</strong> function and plot it using <code>matplotlib</code>.</p>
<div id="ba981016" class="cell" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">3</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_tanh():</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">400</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.tanh(x)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, y)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Tanh Function'</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'tanh(x)'</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>plot_tanh()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2024-07-07 Gemma_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking at the <code>tanh</code> function, one can notice that it limits the upper and lower bounds between -1 &amp; 1, with <span class="math inline">\(+∞\)</span> approaching 1, and <span class="math inline">\(-∞\)</span> approaching -1. It’s pretty easy to implement logit soft capping in PyTorch.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> soft_cap_logits(logits, soft_cap):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    scaled_logits <span class="op">=</span> logits <span class="op">/</span> soft_cap</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> torch.tanh(scaled_logits)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> soft_cap <span class="op">*</span> t</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>])</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>soft_cap <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>capped_logits <span class="op">=</span> soft_cap_logits(logits, soft_cap)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The authors capped the attention logits at 50.0 and final logits at 30.0.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>From the paper:</p>
<p><em>Note that attention logit soft-capping is, at the time of publication, incompatible with common FlashAttention implementations, and we have removed this feature from libraries that use FlashAttention, namely, the HuggingFace transformers library and the vLLM implementation.</em></p>
</div>
</div>
</section>
<section id="sec-merge" class="level2 page-columns page-full" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="sec-merge"><span class="header-section-number">6</span> Model merging</h2>
<p>From the paper:</p>
<div class="page-columns page-full"><p><em>Model merging. We average models from experiments run with different hyperparameters <span class="citation" data-cites="warp">Ramé et al. (<a href="#ref-warp" role="doc-biblioref">2024</a>)</span> .</em></p><div class="no-row-height column-margin column-container"><div id="ref-warp" class="csl-entry" role="listitem">
Ramé, Alexandre, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, and Olivier Bachem. 2024. <span>“WARP: On the Benefits of Weight Averaged Rewarded Policies.”</span> <a href="https://arxiv.org/abs/2406.16768">https://arxiv.org/abs/2406.16768</a>.
</div></div></div>
<p>I would like to refer the readers to mergekit (<span class="citation" data-cites="mergekit">Goddard et al. (<a href="#ref-mergekit" role="doc-biblioref">2024</a>)</span>), which is an open-source library for merging pre-trained Large Language Models.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mergekit" class="csl-entry" role="listitem">
Goddard, Charles, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. 2024. <span>“Arcee’s MergeKit: A Toolkit for Merging Large Language Models.”</span> <em>arXiv Preprint arXiv:2403.13257</em>.
</div></div><p>From the <a href="https://huggingface.co/blog/gemma2#model-merging">Gemma 2 introduction blog by Huggingface</a>:</p>
<p>*According to the Technical Report, Gemma 2 used Warp, a new merging technique that merges models in three distinct stages:</p>
<ul>
<li><em>Exponential Moving Average (EMA): This is applied during the reinforcement learning (RL) fine-tuning process.</em></li>
<li><em>Spherical Linear intERPolation (SLERP): This is applied after the RL fine-tuning of multiple policies.</em></li>
<li><em>Linear Interpolation Towards Initialization (LITI): This stage is applied after the SLERP stage.</em></li>
</ul>
<p>Please refer to <a href="https://wandb.ai/wandb_fc/pytorch-image-models/reports/Revisiting-ResNets-Improved-Training-and-Scaling-Strategies--Vmlldzo2NDE3NTM#ema-of-weights">one of my previous blogs</a> for an in-depth explanation and implementation in PyTorch on <strong>Exponential Moving Average</strong>.</p>
<p>Going by the <strong>mergekit</strong> repository, merging models is as simple as running this one line of code:</p>
<p><code>mergekit-yaml path/to/your/config.yml ./output-model-directory [--cuda] [--lazy-unpickle] [--allow-crimes] [... other options]</code></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I haven’t personally tried model-merging yet, but will share results shortly in a future blog post. Intuitively it feels very similar to model ensembling.</p>
</div>
</div>
</section>
<section id="conclusion" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">7</span> Conclusion</h2>
<p>As part of the following blog post we took a deep dive into grouped query qttention, sliding window attention, RoPE embeddings, logits soft-capping &amp; also model-merging.</p>
<p>We did it all with the motivation from Gemma 2. The idea was to dig deeper into the Gemma 2 architecture. I hope that through this blog post, the reader is able to understand more about the Gemma 2 architecture in detail.</p>
<p>Thank you for your time!</p>



</section>

<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css"><div id="mc_embed_signup">
    <form action="https://github.us4.list-manage.com/subscribe/post?u=e847230346a7c78d4745ae796&amp;id=7a63b2b273&amp;f_id=005f58e8f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2 class="anchored">Subscribe to Aman Arora's blog:</h2>
        <div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
<div hidden="true"><input type="hidden" name="tags" value="7232948"></div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e847230346a7c78d4745ae796_7a63b2b273" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
                <p class="brandingLogo"><a href="http://eepurl.com/il3baM" title="Mailchimp - email marketing made easy and fun"><img src="https://eep.io/mc-cdn-images/template_images/branding_logo_text_dark_dtp.svg"></a></p>
            </div>
        </div>
    </div>
</form>
</div><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>