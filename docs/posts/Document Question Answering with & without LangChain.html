<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Arora">
<meta name="dcterms.date" content="2023-07-25">

<title>Q&amp;A</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href=".././images/logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-158677010-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<meta name="twitter:title" content="Q&amp;A">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="../images/langchain.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Aman Arora’s Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">Aman Arora</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/amaarora" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/amaarora" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aroraaman/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Q&amp;A</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Arora </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 25, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Recently I presented at <a href="https://www.meetup.com/rea-unstackd/events/294318323">REA Unstack’d</a> on Large Language Models. It was mostly a demo about a ChatBot that I’ve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - <a href="https://www.proptrack.com.au/">PropTrack</a>.</p>
<p>Later on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment &amp; all things LLM.</p>
<p><img src="../images/IMG_8001.jpg" class="img-fluid"></p>
<p>Meet <a href="https://www.linkedin.com/in/sachinabeywardana?originalSubdomain=au">Sachin Abeywardana</a> &amp; <a href="https://au.linkedin.com/in/nletcher">Ned Letcher</a>, our co-hosts.</p>
<p>There are many tutorials available today that showcase how to build a Q/A ChatBot, but most (if not all) use <a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a>. Over the past few months, this framework has become extremely popular among all who want to play with LLMs. But, it’s <a href="https://twitter.com/0xSamHogan/status/1679192480565309441">source code is hard to read</a> and if you are trying to do something that’s not within the capabilities of the framework, it becomes extremely difficult.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>👉 This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4</p>
</div>
</div>
<p>In essence, if you want to build a chatbot with LangChain, this is how it looks like:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'OPENAI_API_KEY'</span>] <span class="op">=</span> <span class="st">'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'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.vectorstores.chroma <span class="im">import</span> Chroma</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.embeddings.openai <span class="im">import</span> OpenAIEmbeddings</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.text_splitter <span class="im">import</span> CharacterTextSplitter</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.document_loaders <span class="im">import</span> DirectoryLoader, UnstructuredMarkdownLoader</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.llms <span class="im">import</span> OpenAI, OpenAIChat</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chains <span class="im">import</span> ConversationalRetrievalChain</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.memory <span class="im">import</span> ConversationBufferMemory</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>persist_directory <span class="op">=</span> <span class="st">"db"</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>openai_api_key <span class="op">=</span> os.environ[<span class="st">'OPENAI_API_KEY'</span>]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> DirectoryLoader(<span class="st">"../../rea-crawler/reacrawl/output/"</span>, glob<span class="op">=</span><span class="st">"**/*.md"</span>, loader_cls<span class="op">=</span>UnstructuredMarkdownLoader)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> CharacterTextSplitter(chunk_size<span class="op">=</span><span class="dv">1024</span>, chunk_overlap<span class="op">=</span><span class="dv">128</span>).split_documents(loader.load())</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> OpenAIEmbeddings(openai_api_key<span class="op">=</span>openai_api_key)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># db = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># db.persist()</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>db <span class="op">=</span> Chroma(persist_directory<span class="op">=</span>persist_directory, embedding_function<span class="op">=</span>embeddings)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> ConversationBufferMemory(memory_key<span class="op">=</span><span class="st">"chat_history"</span>, return_messages<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>qa <span class="op">=</span> ConversationalRetrievalChain.from_llm(</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    llm<span class="op">=</span>OpenAIChat(temperature<span class="op">=</span><span class="dv">0</span>, max_tokens<span class="op">=-</span><span class="dv">1</span>, model_name<span class="op">=</span><span class="st">'gpt-3.5-turbo'</span>),</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    chain_type<span class="op">=</span><span class="st">"stuff"</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    retriever<span class="op">=</span>db.as_retriever(),</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    memory<span class="op">=</span>memory,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>qa.run({<span class="st">"question"</span>: <span class="st">"How has the pandemic affected the rental prices in Australia?"</span>, <span class="st">"chat_history"</span>: []})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/base.py(223)__call__()
    221 
    222         pdb.set_trace()
--&gt; 223         inputs = self.prep_inputs(inputs)
    224         callback_manager = CallbackManager.configure(
    225             callbacks,

ipdb&gt; c
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(122)_call()
    120 
    121         pdb.set_trace()
--&gt; 122         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
    123         question = inputs["question"]
    124         get_chat_history = self.get_chat_history or _get_chat_history

ipdb&gt; self.get_chat_history 
ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(123)_call()
    121         pdb.set_trace()
    122         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
--&gt; 123         question = inputs["question"]
    124         get_chat_history = self.get_chat_history or _get_chat_history
    125         chat_history_str = get_chat_history(inputs["chat_history"])

ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(124)_call()
    122         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
    123         question = inputs["question"]
--&gt; 124         get_chat_history = self.get_chat_history or _get_chat_history
    125         chat_history_str = get_chat_history(inputs["chat_history"])
    126 

ipdb&gt; inputs
{'question': 'How has the pandemic affected the rental prices in Australia?', 'chat_history': ''}
ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(125)_call()
    123         question = inputs["question"]
    124         get_chat_history = self.get_chat_history or _get_chat_history
--&gt; 125         chat_history_str = get_chat_history(inputs["chat_history"])
    126 
    127         if chat_history_str:

ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(127)_call()
    125         chat_history_str = get_chat_history(inputs["chat_history"])
    126 
--&gt; 127         if chat_history_str:
    128             callbacks = _run_manager.get_child()
    129             new_question = self.question_generator.run(

ipdb&gt; chat_history_str 
''
ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(133)_call()
    131             )
    132         else:
--&gt; 133             new_question = question
    134         accepts_run_manager = (
    135             "run_manager" in inspect.signature(self._get_docs).parameters

ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(135)_call()
    133             new_question = question
    134         accepts_run_manager = (
--&gt; 135             "run_manager" in inspect.signature(self._get_docs).parameters
    136         )
    137         if accepts_run_manager:

ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(134)_call()
    132         else:
    133             new_question = question
--&gt; 134         accepts_run_manager = (
    135             "run_manager" in inspect.signature(self._get_docs).parameters
    136         )

ipdb&gt; n
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(137)_call()
    135             "run_manager" in inspect.signature(self._get_docs).parameters
    136         )
--&gt; 137         if accepts_run_manager:
    138             docs = self._get_docs(new_question, inputs, run_manager=_run_manager)
    139         else:

ipdb&gt; c
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/base.py(223)__call__()
    221 
    222         pdb.set_trace()
--&gt; 223         inputs = self.prep_inputs(inputs)
    224         callback_manager = CallbackManager.configure(
    225             callbacks,

ipdb&gt; inputs
{'input_documents': [], 'question': 'How has the pandemic affected the rental prices in Australia?', 'chat_history': ''}
ipdb&gt; c


&gt; Entering new StuffDocumentsChain chain...
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/base.py(223)__call__()
    221 
    222         pdb.set_trace()
--&gt; 223         inputs = self.prep_inputs(inputs)
    224         callback_manager = CallbackManager.configure(
    225             callbacks,

ipdb&gt; inputs
{'question': 'How has the pandemic affected the rental prices in Australia?', 'context': ''}
ipdb&gt; c


&gt; Entering new LLMChain chain...
Prompt after formatting:
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.



Question: How has the pandemic affected the rental prices in Australia?
Helpful Answer:

&gt; Finished chain.

&gt; Finished chain.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>'The pandemic has had a mixed impact on rental prices in Australia. In some areas, particularly in major cities like Sydney and Melbourne, rental prices have decreased due to a decrease in demand caused by job losses and reduced immigration. On the other hand, in regional areas and popular holiday destinations, rental prices have increased as people seek to escape the cities and work remotely. Overall, the rental market has become more competitive, with some areas experiencing a decline in prices while others see an increase.'</code></pre>
</div>
</div>
<p>While it’s only a few lines of code, there is really a lot that’s going underneath. In terms of what’s going on:</p>
<ol type="1">
<li>Load markdown files in a list</li>
<li>Create a splitter that can split files to chunks</li>
<li>Convert each chunk and store as Embeddings in a Chroma DB</li>
<li>Use the database as retriever to get relevant text (context), and based on ‘question’, use OpenAI’s gpt-3.5-turbo (ChatGPT) model to answer question based on context.</li>
<li>Also store conversation in memory for reference</li>
</ol>
<p>Going line by line in code, <code>DirectoryLoader</code> loads markdown articles and partitions them using <code>partition_md</code> from library called <code>unstructured</code>.</p>
<p>What <code>UnstructuredMarkdownLoader.load()</code> does can be achieved by following code also:</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> unstructured</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.document_loaders.markdown <span class="im">import</span> UnstructuredMarkdownLoader</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>fpath <span class="op">=</span> <span class="st">'../../rea-crawler/reacrawl/output/10_The renters most impacted by the rental crunch.md'</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>chunks <span class="op">=</span> UnstructuredMarkdownLoader(fpath, mode<span class="op">=</span><span class="st">'elements'</span>).load()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(chunks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>41</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unstructured.partition.md <span class="im">import</span> partition_md</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>_chunks <span class="op">=</span> partition_md(fpath)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(_chunks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>41</code></pre>
</div>
</div>
<p>One key difference between the two is that <code>UnstructuredMarkdownLoader</code> loads it as a special <code>Document</code> class and adds metadata such as <code>source</code>, <code>filename</code>, <code>file_directory</code>.</p>
<p><code>DirectoryLoader</code> that we have used here loads all documents using <code>loader_cls</code> and stores them as a list.</p>
<p>So far so good?</p>
<p>The <code>CharacterTextSplitter</code> used above splits texts based using regex and a separator. The separator in this case is <code>'\n\n'</code>. Thus, anytime there are two line breaks, our text splitter will split documents. Internally, in LangChain to split a text, <code>_split_text_with_regex</code> is being called.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simplified version without `keep_separator`</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _split_text_with_regex(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    text: <span class="bu">str</span>, separator: <span class="bu">str</span>, keep_separator: <span class="bu">bool</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> List[<span class="bu">str</span>]:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now that we have the separator, split the text</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> separator:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>                splits <span class="op">=</span> re.split(separator, text)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        splits <span class="op">=</span> <span class="bu">list</span>(text)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [s <span class="cf">for</span> s <span class="kw">in</span> splits <span class="cf">if</span> s <span class="op">!=</span> <span class="st">""</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we have to define our embedding functiont that can convert texts to embeddings. LangChain has a class called <code>OpenAIEmbeddings</code>.</p>
<p>This class get’s invoked in <code>db = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)</code>. Essentially we are creating a Chroma database from our chunks.</p>
<p>What goes under the hood is that after instantiating a ChromaDB <code>collection</code>, we use collection’s <code>upsert</code> method passing in embeddings and texts. Now you might ask how are these embeddings being calculated? LangChain internalls calls <code>openai.Embedding.create</code>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>❓ Some questions here to ask would be 1. Would results look different or better if we used Cohere Embeddings? What would be the price difference? 2. What would the quality of results be like if we used open source models like Llama-v2 released a few days ago? 3. What if we used free <code>sentence-transformers</code>?</p>
</div>
</div>
<p>Now, that we have stored our texts and embeddings in ChromaDB, for any query, we can find the most similar texts. All of this happens inside <code>ConversationalRetrievalChain</code>.</p>
<p>Essentially we created our chain as:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>qa <span class="op">=</span> ConversationalRetrievalChain.from_llm(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    llm<span class="op">=</span>OpenAIChat(temperature<span class="op">=</span><span class="dv">0</span>, max_tokens<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    chain_type<span class="op">=</span><span class="st">"stuff"</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    retriever<span class="op">=</span>db.as_retriever(),</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    memory<span class="op">=</span>memory,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    get_chat_history<span class="op">=</span><span class="kw">lambda</span> x: x,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>There are two more chains inside this one chain. First is a <code>doc_chain</code> &amp; the second is a <code>condense_question_chain</code>.</p>
<p>Also, previously, we did question answering by using <code>qa.run</code> like so - <code>qa.run({"question": "How has the pandemic affected the rental prices in Australia?", "chat_history": []})</code>. Instead, we could have called the Document Retrieval Chain directly too.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>qa(<span class="st">"How has the pandemic affected the rental prices in Australia?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>

&gt; Entering new LLMChain chain...
Prompt after formatting:
Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.

Chat History:
Human: How has the pandemic affected the rental prices in Australia?
AI: The pandemic has had a mixed impact on rental prices in Australia. In some areas, particularly in major cities like Sydney and Melbourne, rental prices have decreased due to a decrease in demand caused by job losses and reduced immigration. However, in regional areas and some coastal towns, rental prices have increased as people have sought to move away from densely populated areas. Overall, the rental market in Australia has become more competitive, with some areas experiencing a decline in prices while others have seen an increase.
Follow Up Input: How has the pandemic affected the rental prices in Australia?
Standalone question:

&gt; Finished chain.


&gt; Entering new StuffDocumentsChain chain...


&gt; Entering new LLMChain chain...
Prompt after formatting:
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.



Question: What has been the overall impact of the pandemic on rental prices in Australia?
Helpful Answer:

&gt; Finished chain.

&gt; Finished chain.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>{'question': 'How has the pandemic affected the rental prices in Australia?',
 'chat_history': 'Human: How has the pandemic affected the rental prices in Australia?\nAI: The pandemic has had a mixed impact on rental prices in Australia. In some areas, particularly in major cities like Sydney and Melbourne, rental prices have decreased due to a decrease in demand caused by job losses and reduced immigration. However, in regional areas and some coastal towns, rental prices have increased as people have sought to move away from densely populated areas. Overall, the rental market in Australia has become more competitive, with some areas experiencing a decline in prices while others have seen an increase.',
 'answer': 'The overall impact of the pandemic on rental prices in Australia has been a decrease in some areas, particularly in major cities like Sydney and Melbourne. This can be attributed to factors such as reduced demand due to job losses and economic uncertainty, as well as an increase in available rental properties. However, it is important to note that rental prices can vary significantly depending on the location and type of property, so the impact may not be uniform across the entire country.'}</code></pre>
</div>
</div>
<p>Now, we need to look into the details as to how this all works magically and the large language model is able to provide an answer for the question at hand.</p>
<p>Well, as mentioned before, the <code>ConversationalRetrievalChain</code> consists of two chains: 1. Doc chain (Chain used to combine retrieved documents: <code>StuffDocumentsChain</code>) 2. Condense question chain (Chain used to update the question: <code>LLMChain</code>)</p>
<div class="callout callout-style-default callout-tip callout-titled" title="On `LLMChain`">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
On <code>LLMChain</code>
</div>
</div>
<div class="callout-body-container callout-body">
<p>We have already covered <code>LLMChain</code>s in our previous blog post before <a href="https://amaarora.github.io/posts/2023-07-25-llmchain.html">here</a>. It is a pre-requisite before moving further that the readers give this blog post a read for a complete understanding of what exactly goes inside <code>LLMChain</code>. In essence, given a prompt template, the <code>LLMChain</code> can be used for text completion. <code>LLMChain</code>s are a subclass of <code>Chain</code> and we also take a deep dive into everything that goes inside langchain’s <code>Chain</code>s.</p>
</div>
</div>
<p>Okay, so far so good. Now that we know that the <code>ConversationalRetrievalChain</code> consists of two chains, how do they get called? Well, from our <a href="https://amaarora.github.io/posts/2023-07-25-llmchain.html">previous blog post</a> on langchain, we figured that <code>__call__</code> method from <code>Chain</code> (parent class) in turn calls <code>_call</code> method of child class - in this case <code>ConversationalRetrievalChain</code>. The <code>_call</code> method in this case has been implemented inside <code>BaseConversationalRetrievalChain</code> class. Let’s look at it’s definition.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _call(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        inputs: Dict[<span class="bu">str</span>, Any],</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        run_manager: Optional[CallbackManagerForChainRun] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        _run_manager <span class="op">=</span> run_manager <span class="kw">or</span> CallbackManagerForChainRun.get_noop_manager()</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        question <span class="op">=</span> inputs[<span class="st">"question"</span>]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        get_chat_history <span class="op">=</span> <span class="va">self</span>.get_chat_history <span class="kw">or</span> _get_chat_history</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        chat_history_str <span class="op">=</span> get_chat_history(inputs[<span class="st">"chat_history"</span>])</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> chat_history_str:</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>            callbacks <span class="op">=</span> _run_manager.get_child()</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>            new_question <span class="op">=</span> <span class="va">self</span>.question_generator.run(</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>                question<span class="op">=</span>question, chat_history<span class="op">=</span>chat_history_str, callbacks<span class="op">=</span>callbacks</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>            new_question <span class="op">=</span> question</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        accepts_run_manager <span class="op">=</span> (</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"run_manager"</span> <span class="kw">in</span> inspect.signature(<span class="va">self</span>._get_docs).parameters</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accepts_run_manager:</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>            docs <span class="op">=</span> <span class="va">self</span>._get_docs(new_question, inputs, run_manager<span class="op">=</span>_run_manager)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>            docs <span class="op">=</span> <span class="va">self</span>._get_docs(new_question, inputs)  <span class="co"># type: ignore[call-arg]</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        new_inputs <span class="op">=</span> inputs.copy()</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.rephrase_question:</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>            new_inputs[<span class="st">"question"</span>] <span class="op">=</span> new_question</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        new_inputs[<span class="st">"chat_history"</span>] <span class="op">=</span> chat_history_str</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> <span class="va">self</span>.combine_docs_chain.run(</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>            input_documents<span class="op">=</span>docs, callbacks<span class="op">=</span>_run_manager.get_child(), <span class="op">**</span>new_inputs</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        output: Dict[<span class="bu">str</span>, Any] <span class="op">=</span> {<span class="va">self</span>.output_key: answer}</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.return_source_documents:</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>            output[<span class="st">"source_documents"</span>] <span class="op">=</span> docs</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.return_generated_question:</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>            output[<span class="st">"generated_question"</span>] <span class="op">=</span> new_question</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As can be seen above, if <code>chat_history</code> exists, then the <code>question_generator</code> which is an <code>LLMChain</code> is used to create a <code>new_question</code> by taking in user’s original question and also by passing in <code>chat_history_str</code>.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'OPENAI_API_KEY'</span>] <span class="op">=</span> <span class="st">'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'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.vectorstores.chroma <span class="im">import</span> Chroma</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.embeddings.openai <span class="im">import</span> OpenAIEmbeddings</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.text_splitter <span class="im">import</span> CharacterTextSplitter</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.document_loaders <span class="im">import</span> DirectoryLoader, UnstructuredMarkdownLoader</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.llms <span class="im">import</span> OpenAI, OpenAIChat</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chains <span class="im">import</span> ConversationalRetrievalChain</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.memory <span class="im">import</span> ConversationBufferMemory</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>persist_directory <span class="op">=</span> <span class="st">"db"</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>openai_api_key <span class="op">=</span> os.environ[<span class="st">'OPENAI_API_KEY'</span>]</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> DirectoryLoader(<span class="st">"../../rea-crawler/reacrawl/output/"</span>, glob<span class="op">=</span><span class="st">"**/*.md"</span>, loader_cls<span class="op">=</span>UnstructuredMarkdownLoader)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> CharacterTextSplitter(chunk_size<span class="op">=</span><span class="dv">1024</span>, chunk_overlap<span class="op">=</span><span class="dv">128</span>).split_documents(loader.load())</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> OpenAIEmbeddings(openai_api_key<span class="op">=</span>openai_api_key)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># db = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># db.persist()</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>db <span class="op">=</span> Chroma(persist_directory<span class="op">=</span>persist_directory, embedding_function<span class="op">=</span>embeddings)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> ConversationBufferMemory(memory_key<span class="op">=</span><span class="st">"chat_history"</span>, return_messages<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>qa <span class="op">=</span> ConversationalRetrievalChain.from_llm(</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    llm<span class="op">=</span>OpenAIChat(temperature<span class="op">=</span><span class="dv">0</span>, max_tokens<span class="op">=-</span><span class="dv">1</span>, model_name<span class="op">=</span><span class="st">'gpt-3.5-turbo'</span>),</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    chain_type<span class="op">=</span><span class="st">"stuff"</span>,</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    retriever<span class="op">=</span>db.as_retriever(),</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    memory<span class="op">=</span>memory,</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>qa.run({<span class="st">"question"</span>: <span class="st">"How has the pandemic affected the rental prices in Australia?"</span>, <span class="st">"chat_history"</span>: []})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/ubuntu/GIT_REPOS/langchain/langchain/llms/openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

&gt; Entering new StuffDocumentsChain chain...


&gt; Entering new LLMChain chain...
Prompt after formatting:
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.



Question: How has the pandemic affected the rental prices in Australia?
Helpful Answer:

&gt; Finished chain.
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/base.py(341)prep_outputs()
    339 
    340         pdb.set_trace()
--&gt; 341         if self.memory is not None:
    342             self.memory.save_context(inputs, outputs)
    343         if return_only_outputs:

ipdb&gt; a
self = LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template="Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:", template_format='f-string', validate_template=True), llm=OpenAIChat(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=&lt;class 'openai.api_resources.chat_completion.ChatCompletion'&gt;, model_name='gpt-3.5-turbo', model_kwargs={'temperature': 0, 'max_tokens': -1}, openai_api_key=None, openai_api_base=None, openai_proxy=None, max_retries=6, prefix_messages=[], streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={})
inputs = {'question': 'How has the pandemic affected the rental prices in Australia?', 'context': ''}
outputs = {'text': 'The pandemic has had a mixed impact on rental prices in Australia. In some areas, particularly in major cities like Sydney and Melbourne, rental prices have decreased due to a decrease in demand caused by job losses and reduced immigration. On the other hand, in regional areas and popular holiday destinations, rental prices have increased as people seek to escape the cities and work remotely. Overall, the rental market has become more competitive, with some areas experiencing a decline in prices while others see an increase.'}
return_only_outputs = False
ipdb&gt; q</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>qa.memory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='How has the pandemic affected the rental prices in Australia?', additional_kwargs={}, example=False), AIMessage(content='The pandemic has had a mixed impact on rental prices in Australia. In some areas, particularly in major cities like Sydney and Melbourne, rental prices have decreased due to a decrease in demand caused by job losses and reduced immigration. However, in regional areas and popular holiday destinations, rental prices have increased as people seek to escape the cities and work remotely. Overall, the rental market in Australia has become more competitive, with some areas experiencing a decline in prices while others see an increase.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='chat_history')</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>qa(<span class="st">"How has the pandemic affected the rental prices in Sydney?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/base.py(223)__call__()
    221 
    222         pdb.set_trace()
--&gt; 223         inputs = self.prep_inputs(inputs)
    224         callback_manager = CallbackManager.configure(
    225             callbacks,

ipdb&gt; inputs
'How has the pandemic affected the rental prices in Sydney?'
ipdb&gt; c
&gt; /home/ubuntu/GIT_REPOS/langchain/langchain/chains/conversational_retrieval/base.py(122)_call()
    120 
    121         pdb.set_trace()
--&gt; 122         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
    123         question = inputs["question"]
    124         get_chat_history = self.get_chat_history or _get_chat_history

ipdb&gt; inputs
{'question': 'How has the pandemic affected the rental prices in Sydney?', 'chat_history': 'Human: How has the pandemic affected the rental prices in Australia?\nAI: The pandemic has had a mixed impact on rental prices in Australia. In some areas, particularly in major cities like Sydney and Melbourne, rental prices have decreased due to a decrease in demand caused by job losses and reduced immigration. On the other hand, in regional areas and popular holiday destinations, rental prices have increased as people seek to escape the cities and work remotely. Overall, the rental market has become more competitive, with some areas experiencing a decline in prices while others see an increase.'}
ipdb&gt; q</code></pre>
</div>
</div>



<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css"><div id="mc_embed_signup">
    <form action="https://github.us4.list-manage.com/subscribe/post?u=e847230346a7c78d4745ae796&amp;id=7a63b2b273&amp;f_id=005f58e8f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2 class="anchored">Subscribe to Aman Arora's blog:</h2>
        <div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
<div hidden="true"><input type="hidden" name="tags" value="7232948"></div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e847230346a7c78d4745ae796_7a63b2b273" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
                <p class="brandingLogo"><a href="http://eepurl.com/il3baM" title="Mailchimp - email marketing made easy and fun"><img src="https://eep.io/mc-cdn-images/template_images/branding_logo_text_dark_dtp.svg"></a></p>
            </div>
        </div>
    </div>
</form>
</div><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>