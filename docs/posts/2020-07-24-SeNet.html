<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Arora">
<meta name="dcterms.date" content="2020-07-24">
<meta name="description" content="In this blogpost, we re-implement the Squeeze-and-Excitation networks in PyTorch step-by-step with very minor updates to ResNet implementation in torchvision.">

<title>Squeeze and Excitation Networks Explained with PyTorch Implementation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Aman Arora’s Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#intuition-behind-squeeze-and-excitation-networks" id="toc-intuition-behind-squeeze-and-excitation-networks" class="nav-link" data-scroll-target="#intuition-behind-squeeze-and-excitation-networks"><span class="toc-section-number">2</span>  Intuition behind Squeeze-and-Excitation Networks</a>
  <ul class="collapse">
  <li><a href="#main-idea-behind-se-nets" id="toc-main-idea-behind-se-nets" class="nav-link" data-scroll-target="#main-idea-behind-se-nets"><span class="toc-section-number">2.1</span>  Main Idea behind Se-Nets:</a></li>
  </ul></li>
  <li><a href="#squeeze-global-information-embedding" id="toc-squeeze-global-information-embedding" class="nav-link" data-scroll-target="#squeeze-global-information-embedding"><span class="toc-section-number">3</span>  Squeeze: Global Information Embedding</a></li>
  <li><a href="#excitation-adaptive-recalibration" id="toc-excitation-adaptive-recalibration" class="nav-link" data-scroll-target="#excitation-adaptive-recalibration"><span class="toc-section-number">4</span>  Excitation: Adaptive Recalibration</a></li>
  <li><a href="#squeeze-and-excitation-block-in-pytorch" id="toc-squeeze-and-excitation-block-in-pytorch" class="nav-link" data-scroll-target="#squeeze-and-excitation-block-in-pytorch"><span class="toc-section-number">5</span>  Squeeze and Excitation Block in PyTorch</a></li>
  <li><a href="#se-block-with-existing-sota-architectures" id="toc-se-block-with-existing-sota-architectures" class="nav-link" data-scroll-target="#se-block-with-existing-sota-architectures"><span class="toc-section-number">6</span>  SE Block with Existing SOTA Architectures</a></li>
  <li><a href="#se-resnet-in-pytorch" id="toc-se-resnet-in-pytorch" class="nav-link" data-scroll-target="#se-resnet-in-pytorch"><span class="toc-section-number">7</span>  SE-ResNet in PyTorch</a>
  <ul class="collapse">
  <li><a href="#seresnet-18" id="toc-seresnet-18" class="nav-link" data-scroll-target="#seresnet-18"><span class="toc-section-number">7.1</span>  SEResNet-18</a></li>
  <li><a href="#seresnet-34" id="toc-seresnet-34" class="nav-link" data-scroll-target="#seresnet-34"><span class="toc-section-number">7.2</span>  SEResNet-34</a></li>
  <li><a href="#seresnet-50" id="toc-seresnet-50" class="nav-link" data-scroll-target="#seresnet-50"><span class="toc-section-number">7.3</span>  SEResNet-50</a></li>
  <li><a href="#seresnet-101" id="toc-seresnet-101" class="nav-link" data-scroll-target="#seresnet-101"><span class="toc-section-number">7.4</span>  SEResNet-101</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">8</span>  Conclusion</a></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits"><span class="toc-section-number">9</span>  Credits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Squeeze and Excitation Networks Explained with PyTorch Implementation</h1>
<p class="subtitle lead">Squeeze-and-Excitation Networks</p>
  <div class="quarto-categories">
    <div class="quarto-category">Computer Vision</div>
    <div class="quarto-category">Model Architecture</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>In this blogpost, we re-implement the Squeeze-and-Excitation networks in PyTorch step-by-step with very minor updates to ResNet implementation in torchvision.</p>
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Aman Arora </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 24, 2020</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>In this blog post, we will be looking at the <a href="https://arxiv.org/abs/1709.01507">Squeeze-and-Excitation</a> networks. We will refer to the <a href="https://arxiv.org/abs/1709.01507">research paper by Hu et al</a> and first understand what <strong>Squeeze-and-Excitation</strong> networks are before implementing the novel architecture in PyTorch with very few modifications to the popular <code>ResNet</code> architecture.</p>
<p>First, we develop an intuition for what SE-Nets are and the novel idea behind their success. Next, we will look at the <strong>Squeeze</strong> and <strong>Excitation</strong> operations in a little more detail. Finally, we implement the <strong>Squeeze-and-Excitation</strong> networks in PyTorch with very minor updates to <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py">ResNet implementation in torchvision</a>.</p>
</section>
<section id="intuition-behind-squeeze-and-excitation-networks" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="intuition-behind-squeeze-and-excitation-networks"><span class="header-section-number">2</span> Intuition behind Squeeze-and-Excitation Networks</h2>
<p>So, what’s new in the <strong>Squeeze-and-Excitation</strong> networks? How are they different from the <strong>ResNet</strong> architecture?</p>
<p>Let’s consider an RGB image as an input. Then the convolution operation with a 3x3 kernel on the input image can be visualised as below:</p>
<p><img src="../images/cnn.gif" title="fig-1: Convolution operation on RGB image; src: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1" class="img-fluid"></p>
<p>A feature map is generated per-channel (RGB) and then summed together to form one channel or the final output of the convolution operation as below:</p>
<p><img src="../images/output.gif" title="fig-2: Convolution output; src: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1" class="img-fluid"></p>
<p>Implicitly, the convolution kernel would have different weights for different channels and these are learned weights through backpropagation. If it’s an RGB image, then generally the kernels are also cubic to map channel dependencies.</p>
<section id="main-idea-behind-se-nets" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="main-idea-behind-se-nets"><span class="header-section-number">2.1</span> Main Idea behind Se-Nets:</h3>
<blockquote class="blockquote">
<p>We expect the learning of convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able to increase its sensitivity to informative features which can be exploited by subsequent transformations. Consequently, we would like to provide it with access to global information and recalibrate filter responses in two steps, <em>squeeze</em> and <em>excitation</em>, before they are fed into the next transformation.</p>
</blockquote>
<p>In other words, with the <strong>squeeze-and-excitation</strong> block, the neural nets are better able to map the channel dependency along with access to global information. Therefore, they are better able to recalibrate the filter outputs and thus, this leads to performance gains.</p>
<p><img src="../images/senet_block.png" title="fig-3: Squeeze-and-Excitation block" class="img-fluid"></p>
<p>This main idea can be further explained using the <strong>Squeeze-and-Excitation block</strong> image above from the paper. First, a feature transformation (such as a convolution operation) is performed on the input image <code>X</code> to get features <code>U</code>. Next, we perform a <strong>squeeze</strong> operation to get a single value for each channel of output <code>U</code>. After, we perform an <strong>excitation</strong> operation on the output of the <strong>squeeze</strong> operation to get per-channel weights.</p>
<p>Finally, once we have the per-channel weights, the final output of the block is obtained by rescaling the feature map <code>U</code> with these activations.</p>
<p>From the paper: &gt; The role this operation performs at different depths differs throughout the network. In earlier layers, it excites informative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to different inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network.</p>
<p>Next, we will look at the <strong>Squeeze</strong> and <strong>Excitation</strong> operations in a little more detail.</p>
</section>
</section>
<section id="squeeze-global-information-embedding" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="squeeze-global-information-embedding"><span class="header-section-number">3</span> Squeeze: Global Information Embedding</h2>
<p>The main purpose of the Squeeze operation is to extract global information from each of the channels of an image. Since, <a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1">convolution is a local operation</a> (that is, at a particular time, it is only able to see a part of the image), it might be beneficial to also extract information outside the receptive field of the convolution filter.</p>
<p>The <strong>Squeeze</strong> operation is meant to do exactly that and the authors keep it as simple as possible.</p>
<p>The authors perform a Global Average Pooling operation to reduce the <code>C x H x W</code> image to <code>C x 1 x 1</code> to get a global statistic for each channel.</p>
<p>Formally, the <a href="https://principlesofdeeplearning.com/index.php/a-tutorial-on-global-average-pooling/#:~:text=The%20global%20average%20pooling%20mechanism,all%20digits%20summing%20to%201.0.">Global Average Pooling</a> or the <strong>Squeeze</strong> operation can be formally represented as:</p>
<p><img src="../images/squeeze.png" title="eq-1: Global Average Pooling" class="img-fluid"></p>
<p>In other words, all we do is that we take the mean of each channel across <code>H x W</code> spatial dimensions.</p>
</section>
<section id="excitation-adaptive-recalibration" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="excitation-adaptive-recalibration"><span class="header-section-number">4</span> Excitation: Adaptive Recalibration</h2>
<p>Now that we have a vector of length <code>C</code> from the <a href="https://amaarora.github.io/2020/07/24/SeNet.html#squeeze-global-information-embedding">Squeeze</a> operation, the next step is to generate a set of weights for each channel. This is done with the help of <strong>Excitation</strong> operation explained in this section.</p>
<p>Formally, the excitation operation can be represented by:</p>
<p><img src="../images/gating_operation.png" title="eq-2: Excitation" class="img-fluid"></p>
<p>where:<br>
- <code>δ</code> refers to ReLU operation<br>
- <code>σ</code> refers to Sigmoid operation<br>
- <code>W1</code> and <code>W2</code> are two fully-connected layers<br>
- <code>z</code> is the output from the Squeeze block</p>
<p>The two FC layers form a bottleneck architecture, that is, the first <code>W1</code> layer is used for dimensionality reduction by a ratio <code>r</code> and the second <code>W2</code> layer is a dimensionality increasing layer returning to the channel dimension of <code>U</code>.</p>
<p>Since, the Sigmoid layer would return numbers between 0 and 1, these are the channel weights and the final output of the block can be respresented as:</p>
<p><img src="../images/senet_output.png" title="fig4: SeNet output" class="img-fluid"></p>
<p>From the paper: &gt; The excitation operator maps the input-specific descriptor z to a set of channel weights. In this regard, SE blocks intrinsically introduce dynamics conditioned on the input, which can be regarded as a self-attention function on channels whose relationships are not confined to the local receptive field the convolutional filter are responsive to.</p>
<p>The authors in the paper also mentioned the thinking and reasoning behind coming up with this excitation function. They write: &gt; To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfil this objective, the function must meet two criteria: first, it must be flexible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised.</p>
<p>That is why using a <code>Sigmoid</code> layer makes so much sense rather than <code>Softmax</code>(which would generally impose importance on only one of the channels). A <code>Sigmoid</code> function (which is also used in multi-label classification) allows multiple channels to have higher importance.</p>
</section>
<section id="squeeze-and-excitation-block-in-pytorch" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="squeeze-and-excitation-block-in-pytorch"><span class="header-section-number">5</span> Squeeze and Excitation Block in PyTorch</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SE_Block(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c, r<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.squeeze <span class="op">=</span> nn.AdaptiveAvgPool2d(<span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.excitation <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(c, c <span class="op">//</span> r, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(c <span class="op">//</span> r, c, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        bs, c, _, _ <span class="op">=</span> x.shape</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.squeeze(x).view(bs, c)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.excitation(y).view(bs, c, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> y.expand_as(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As mentioned the <strong>Squeeze</strong> operation is a global Average Pooling operation and in PyTorch this can be represented as <code>nn.AdaptiveAvgPool2d(1)</code> where 1, represents the output size.</p>
<p>Next, the <strong>Excitation</strong> network is a bottle neck architecture with two FC layers, first to reduce the dimensions and second to increase the dimensions back to original. We reduce the dimensions by a reduction ratio <code>r=16</code>. This is as simple as creating a <code>nn.Sequential</code> with two FC layers, with a <code>nn.ReLU()</code> in between and followed by a <code>nn.Sigmoid()</code>.</p>
<p>The outputs of the <strong>Excitation</strong> operation are the channel weights which are then multiplied element-wise to input feature <code>X</code> to get the final output of the <code>SE_Block</code>.</p>
</section>
<section id="se-block-with-existing-sota-architectures" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="se-block-with-existing-sota-architectures"><span class="header-section-number">6</span> SE Block with Existing SOTA Architectures</h2>
<p>From the paper: &gt; The structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced. SE blocks are also computationally lightweight and impose only a slight increase in model complexity and computational burden.</p>
<p>In other words, it is really simple to integrate SE blocks with existing state-of-art architectures. The authors provide two examples for SE-ResNet and SE-Inception as below:</p>
<p><img src="../images/se-resnet.png" title="fig5: SE-Inception and SE-ResNet Architectures" class="img-fluid"></p>
<p>In this blogpost we will implement the <strong>SE-ResNet</strong> architecture and the <strong>SE-Inception</strong> architecture is left as an exercise to the reader.</p>
<p>The authors experimented by inserting <strong>SE block</strong> in various positions and found that the performance improvements produced by SE units are fairly robust to their location, provided that they are applied prior to branch aggregation. In this post, we will construct a <strong>SE-ResNet</strong> architecture using the <strong>Standard SE block</strong> integration as below:</p>
<p><img src="../images/SE_block_integ.png" title="fig-6: SE block integrations ablation study" class="img-fluid"></p>
<p><strong>Key point to note:</strong> &gt; As can be seen from the Standard SE block (b) integration, the SE block is preceded directly by the Residual operation in ResNet before summation with the identity branch.</p>
</section>
<section id="se-resnet-in-pytorch" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="se-resnet-in-pytorch"><span class="header-section-number">7</span> SE-ResNet in PyTorch</h2>
<p>As we saw from fig-6, we simply insert the SE block after the Residual operation. To create SE-Resnet34 and below, we simply copy the <code>BasicBlock</code> from torchvision from <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L35">here</a>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicBlock(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    expansion <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inplanes, planes, stride<span class="op">=</span><span class="dv">1</span>, downsample<span class="op">=</span><span class="va">None</span>, groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                 base_width<span class="op">=</span><span class="dv">64</span>, dilation<span class="op">=</span><span class="dv">1</span>, norm_layer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BasicBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            norm_layer <span class="op">=</span> nn.BatchNorm2d</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> groups <span class="op">!=</span> <span class="dv">1</span> <span class="kw">or</span> base_width <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'BasicBlock only supports groups=1 and base_width=64'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> dilation <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Dilation &gt; 1 not supported in BasicBlock"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv3x3(inplanes, planes, stride)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> norm_layer(planes)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3(planes, planes)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> norm_layer(planes)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample <span class="op">=</span> downsample</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> identity</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we update the <code>forward</code> to insert the SE block operation as in fig-6:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SEBasicBlock(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    expansion <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inplanes, planes, stride<span class="op">=</span><span class="dv">1</span>, downsample<span class="op">=</span><span class="va">None</span>, groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                 base_width<span class="op">=</span><span class="dv">64</span>, dilation<span class="op">=</span><span class="dv">1</span>, norm_layer<span class="op">=</span><span class="va">None</span>, r<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SEBasicBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            norm_layer <span class="op">=</span> nn.BatchNorm2d</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> groups <span class="op">!=</span> <span class="dv">1</span> <span class="kw">or</span> base_width <span class="op">!=</span> <span class="dv">64</span>:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'BasicBlock only supports groups=1 and base_width=64'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> dilation <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Dilation &gt; 1 not supported in BasicBlock"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv3x3(inplanes, planes, stride)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> norm_layer(planes)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3(planes, planes)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> norm_layer(planes)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample <span class="op">=</span> downsample</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add SE block</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.se <span class="op">=</span> SE_Block(planes, r)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add SE operation</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.se(out)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> identity</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And that’s it! As easy as it may sound, we have just implemented the <code>SEBasicBlock</code> in PyTorch.</p>
<p>Next, for ResNet-50 and above, we perform the same steps for <code>Bottleneck</code> architecture. First, we copy the <code>Bottleneck</code> class <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L75">from torchvision</a>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Bottleneck(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># while original implementation places the stride at the first 1x1 convolution(self.conv1)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This variant is also known as ResNet V1.5 and improves accuracy according to</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    expansion <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inplanes, planes, stride<span class="op">=</span><span class="dv">1</span>, downsample<span class="op">=</span><span class="va">None</span>, groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                 base_width<span class="op">=</span><span class="dv">64</span>, dilation<span class="op">=</span><span class="dv">1</span>, norm_layer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Bottleneck, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            norm_layer <span class="op">=</span> nn.BatchNorm2d</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        width <span class="op">=</span> <span class="bu">int</span>(planes <span class="op">*</span> (base_width <span class="op">/</span> <span class="fl">64.</span>)) <span class="op">*</span> groups</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv1x1(inplanes, width)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> norm_layer(width)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3(width, width, stride, groups, dilation)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> norm_layer(width)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv1x1(width, planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn3 <span class="op">=</span> norm_layer(planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample <span class="op">=</span> downsample</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv3(out)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn3(out)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> identity</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we add the <code>SE_Block</code> operation similar to what we did for the <code>BasicBlock</code> as below:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SEBottleneck(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># while original implementation places the stride at the first 1x1 convolution(self.conv1)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This variant is also known as ResNet V1.5 and improves accuracy according to</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    expansion <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inplanes, planes, stride<span class="op">=</span><span class="dv">1</span>, downsample<span class="op">=</span><span class="va">None</span>, groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                 base_width<span class="op">=</span><span class="dv">64</span>, dilation<span class="op">=</span><span class="dv">1</span>, norm_layer<span class="op">=</span><span class="va">None</span>, r<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SEBottleneck, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            norm_layer <span class="op">=</span> nn.BatchNorm2d</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        width <span class="op">=</span> <span class="bu">int</span>(planes <span class="op">*</span> (base_width <span class="op">/</span> <span class="fl">64.</span>)) <span class="op">*</span> groups</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv1x1(inplanes, width)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> norm_layer(width)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3(width, width, stride, groups, dilation)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> norm_layer(width)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv1x1(width, planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn3 <span class="op">=</span> norm_layer(planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample <span class="op">=</span> downsample</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add SE block</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.se <span class="op">=</span> SE_Block(planes, r)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv3(out)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn3(out)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add SE operation</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.se(out)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> identity</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s it! Now that we have implemented <code>SEBasicBlock</code> and <code>SEBottleneck</code> in PyTorch, we are ready to construct <code>SE-ResNet</code> architectures. As was mentioned in the paper,</p>
<blockquote class="blockquote">
<p>The structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced.</p>
</blockquote>
<p>Let’s do exactly this. We simply replace the components of ResNet architecture with the SE counterparts.</p>
<p>But, one last step is to copy some helper functions from torchvision. These functions are present <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L24">here</a>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv3x3(in_planes, out_planes, stride<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>, dilation<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""3x3 convolution with padding"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Conv2d(in_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span>stride,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                     padding<span class="op">=</span>dilation, groups<span class="op">=</span>groups, bias<span class="op">=</span><span class="va">False</span>, dilation<span class="op">=</span>dilation)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv1x1(in_planes, out_planes, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""1x1 convolution"""</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Conv2d(in_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span>stride, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _resnet(arch, block, layers, pretrained, progress, <span class="op">**</span>kwargs):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ResNet(block, layers, <span class="op">**</span>kwargs)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="seresnet-18" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="seresnet-18"><span class="header-section-number">7.1</span> SEResNet-18</h3>
<p>From torchvision <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L232">here</a>, we update the implementation of <code>resnet18</code> to get <code>se_resnet18</code>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> se_resnet18(pretrained<span class="op">=</span><span class="va">False</span>, progress<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>kwargs):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _resnet(<span class="st">'resnet18'</span>, SEBasicBlock, [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>], pretrained, progress,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                   <span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="seresnet-34" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="seresnet-34"><span class="header-section-number">7.2</span> SEResNet-34</h3>
<p>From torchvision <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L244">here</a>, we update the implementation of <code>resnet34</code> to get <code>se_resnet34</code>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> se_resnet34(pretrained<span class="op">=</span><span class="va">False</span>, progress<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>kwargs):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _resnet(<span class="st">'resnet34'</span>, SEBasicBlock, [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">3</span>], pretrained, progress,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>                   <span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="seresnet-50" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="seresnet-50"><span class="header-section-number">7.3</span> SEResNet-50</h3>
<p>From torchvision <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L256">here</a>, we update the implementation of <code>resnet50</code> to get <code>se_resnet50</code>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> se_resnet50(pretrained<span class="op">=</span><span class="va">False</span>, progress<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>kwargs):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _resnet(<span class="st">'resnet50'</span>, SEBottleneck, [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">3</span>], pretrained, progress,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                   <span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="seresnet-101" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="seresnet-101"><span class="header-section-number">7.4</span> SEResNet-101</h3>
<p>From torchvision <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L268">here</a>, we update the implementation of <code>resnet101</code> to get <code>se_resnet101</code>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> se_resnet101(pretrained<span class="op">=</span><span class="va">False</span>, progress<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>kwargs):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _resnet(<span class="st">'resnet101'</span>, SEBottleneck, [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">23</span>, <span class="dv">3</span>], pretrained, progress,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                   <span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="conclusion" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">8</span> Conclusion</h2>
<p>In this blogpost, first, we looked at what <strong>SE blocks</strong> are and the novel idea that they introduce. Next, we looked at the <strong>Squeeze and Excitation</strong> operation that is used to generate per-channel weights which are then used to return the final output of the SE block.</p>
<p>Finally, we looked at integration of SE block in the ResNet architecture to construct <code>se_resnet18</code>, <code>se_resnet34</code>, <code>se_resnet50</code> and <code>se_resnet101</code>.</p>
<p>I hope that my explaination of <strong>SE Blocks</strong> was clear and as always - constructive feedback is always welcome at <a href="https://twitter.com/amaarora"><span class="citation" data-cites="amaarora">@amaarora</span></a>.</p>
<p>Also, feel free to <a href="https://amaarora.github.io/subscribe">subscribe</a> to receive regular updates regarding new blog posts. Thanks for reading!</p>
</section>
<section id="credits" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="credits"><span class="header-section-number">9</span> Credits</h2>
<p>The implementation of <code>SE_Block</code> has been adapted from senet.pytorch repo <a href="https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4">here</a>.</p>
<p>Also, thanks to my friends - <a href="https://twitter.com/abanerjee99">Atmadeep Banerjee</a> and <a href="https://twitter.com/akashpalrecha98">Akash Palrecha</a> - both research interns at Harvard - for proof reading the draft version of this blog post and providing me with an honest and constructive feedback.</p>
<p>Both Akash and Atmadeep found a technical error in my blog in the <a href="https://amaarora.github.io/2020/07/24/SeNet.html#main-idea-behind-se-nets">main idea behind SENet</a> section where I had skipped the part that channel-dependencies are implicitly present when performing a convolution operation. Also, they helped me make this section better by helping me further improve my understanding of SeNets.</p>
<p>Akash, was also very kind to point out a grammatical error and also it was his suggestion to add the two reasons mentioned in the paper for choosing the <strong>excitation</strong> function.</p>
<p>Both Akash and Atmadeep helped me in making this blog post better and more accurate. Thanks guys!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>