<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Arora">
<meta name="dcterms.date" content="2023-07-25">
<meta name="description" content="Embark on an enlightening journey through the world of document-based question-answering chatbots using langchain! With a keen focus on detailed explanations and code walk-throughs, you‚Äôll gain a deep understanding of each component - from creating a vector database to response generation.">

<title>Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href=".././images/logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-158677010-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<meta name="twitter:title" content="Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain">
<meta name="twitter:description" content="Embark on an enlightening journey through the world of document-based question-answering chatbots using langchain! With a keen focus on detailed explanations and code walk-throughs, you‚Äôll gain a deep understanding of each component - from creating a vector database to response generation.">
<meta name="twitter:image" content="../images/langchain.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Aman Arora‚Äôs Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">Aman Arora</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/amaarora" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/amaarora" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aroraaman/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain</h1>
                  <div>
        <div class="description">
          Embark on an enlightening journey through the world of document-based question-answering chatbots using langchain! With a keen focus on detailed explanations and code walk-throughs, you‚Äôll gain a deep understanding of each component - from creating a vector database to response generation.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Arora </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 25, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#question-generator-chain-inside-conversationalretrievalchain" id="toc-question-generator-chain-inside-conversationalretrievalchain" class="nav-link active" data-scroll-target="#question-generator-chain-inside-conversationalretrievalchain"><span class="header-section-number">0.1</span> Question generator chain inside <code>ConversationalRetrievalChain</code></a></li>
  <li><a href="#document-chain" id="toc-document-chain" class="nav-link" data-scroll-target="#document-chain"><span class="header-section-number">0.2</span> Document Chain</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Recently I presented at <a href="https://www.meetup.com/rea-unstackd/events/294318323">REA Unstack‚Äôd</a> on Large Language Models. It was mostly a demo about a ChatBot that I‚Äôve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - <a href="https://www.proptrack.com.au/">PropTrack</a>.</p>
<p>Later on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment &amp; all things LLM.</p>
<p><img src="../images/IMG_8001.jpg" class="img-fluid"></p>
<p>Meet <a href="https://www.linkedin.com/in/sachinabeywardana?originalSubdomain=au">Sachin Abeywardana</a> &amp; <a href="https://au.linkedin.com/in/nletcher">Ned Letcher</a>, our panelists.</p>
<p>There are many tutorials available today that showcase how to build a Q/A ChatBot, and most (if not all) use <a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a>. Over the past few months, this framework has become extremely popular among all who want to play with LLMs. But, its <a href="https://twitter.com/0xSamHogan/status/1679192480565309441">source code is hard to read</a> and if you are trying to do something that‚Äôs not within the capabilities of the framework, it becomes extremely difficult.</p>
<p>I recently wrote about <code>LLMChain</code>s in langchain too, and found the same to true. You can find the previous blog post <a href="https://amaarora.github.io/posts/2023-07-25-llmchain.html">here</a>. I would highly recommend the readers to give the previous blog post a read, it will explain <code>LLMChain</code>s and <code>Chain</code>s in langchain, that will be instrumental in understanding conversational chatbot that we are building today.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>üëâ This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4</p>
</div>
</div>
<p>Let‚Äôs say you have a number of documents, in my case, I have a bunch of markdown documents. And we want to build a question answering chatbot that can take in a question, and find the answer based on the documents.</p>
<div id="fig-chatbot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/chatbot.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Chatbot architecture</figcaption>
</figure>
</div>
<p>In essence, the chatbot looks something like above. We pass the documents through an ‚Äúembedding model‚Äù. It is easy enough to use <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI‚Äôs embedding API</a> to convert documents, or chunks of documents to embeddings. These embeddings can be stored in a vector database such as <a href="https://www.trychroma.com/">Chroma</a>, <a href="https://faiss.ai/index.html">Faiss</a> or <a href="https://lancedb.com/">Lance</a>.</p>
<p>The user interacts through a ‚Äúchat interface‚Äù and enters a question/query. This query can also be converted to an embedding using the embedding model. Next, we can find the nearest chunks (similar to the query) using similarity search, then pass these nearest chunks (referred to as ‚Äúcontext‚Äù) to a Large Language Model such as ChatGPT.</p>
<p>Finally, we retrieve an answer and this answer get‚Äôs passed back to the user in the chat interfact. We store this interaction in chat history and continue.</p>
<p>That is all in theory, in code, using <a href="https://python.langchain.com/">langchain</a>, above would look like:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'OPENAI_API_KEY'</span>] <span class="op">=</span> <span class="st">'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'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.vectorstores.chroma <span class="im">import</span> Chroma</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.embeddings.openai <span class="im">import</span> OpenAIEmbeddings</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.text_splitter <span class="im">import</span> CharacterTextSplitter</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.document_loaders <span class="im">import</span> DirectoryLoader, UnstructuredMarkdownLoader</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chat_models <span class="im">import</span> ChatOpenAI</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chains <span class="im">import</span> ConversationalRetrievalChain</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.memory <span class="im">import</span> ConversationBufferMemory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># directory to store vector database</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>persist_directory <span class="op">=</span> <span class="st">"/home/ubuntu/GIT_REPOS/rea-crawler/db/"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>openai_api_key <span class="op">=</span> os.environ[<span class="st">'OPENAI_API_KEY'</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># loader that loads `markdown` documents</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> DirectoryLoader(<span class="st">"../../rea-crawler/reacrawl/output/"</span>, glob<span class="op">=</span><span class="st">"**/*.md"</span>, loader_cls<span class="op">=</span>UnstructuredMarkdownLoader)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># text splitter converts documents to chunks</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>docs <span class="op">=</span> loader.load()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>text_splitter <span class="op">=</span> CharacterTextSplitter(chunk_size<span class="op">=</span><span class="dv">1024</span>, chunk_overlap<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>chunks <span class="op">=</span> text_splitter.split_documents(docs)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># embedding model to convert chunks to embeddings</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> OpenAIEmbeddings(openai_api_key<span class="op">=</span>openai_api_key)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load vector database, uncomment below two lines if you'd like to create it</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># db.persist()</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>db <span class="op">=</span> Chroma(persist_directory<span class="op">=</span>persist_directory, embedding_function<span class="op">=</span>embeddings)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> ConversationBufferMemory(memory_key<span class="op">=</span><span class="st">"chat_history"</span>, return_messages<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create QA chain using `langchain`, database is used as vector store retriever to find "context" (using similarity search)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>qa <span class="op">=</span> ConversationalRetrievalChain.from_llm(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    llm<span class="op">=</span>ChatOpenAI(temperature<span class="op">=</span><span class="fl">0.2</span>, model_name<span class="op">=</span><span class="st">'gpt-3.5-turbo'</span>),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    chain_type<span class="op">=</span><span class="st">"stuff"</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    retriever<span class="op">=</span>db.as_retriever(),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    memory<span class="op">=</span>memory,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's ask a question</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>qa.run({<span class="st">"question"</span>: <span class="st">"Why is it so hard to find a rental property in Australia in June 2023?"</span>, <span class="st">"chat_history"</span>: []})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>'In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. Additionally, there has been increased demand for rentals as people have moved out of sharehouses into smaller households. In regional areas, the demand from city-movers has made housing difficult to find, leading to plummeting vacancy rates and soaring prices. In cities like Melbourne and Sydney, rental market conditions have tightened considerably over the past year, resulting in strong rent growth. The departure of investors from the market and the lack of new investors entering has also impacted rental supply. On the other hand, demand for rentals has been strong, particularly in inner-city areas, with the return of international students, migrants, and office workers to CBDs. These factors have contributed to the difficulty in finding a rental property in Australia in June 2023.'</code></pre>
</div>
</div>
<p>Looking at the answer above, it really answers the question - ‚ÄúWhy is it so hard to find a rental property in Australia in June 2023?‚Äù very well. Above might only be a few lines of code, but there is actually quite a lot going on underneath. Refer to <a href="#fig-chatbot">Figure&nbsp;1</a> for everything that‚Äôs going on underneath.</p>
<p>But, as a recap, and matching our steps with code shared above:</p>
<ol type="1">
<li>Load markdown files in a list <code>loader = DirectoryLoader("../../rea-crawler/reacrawl/output/", glob="**/*.md", loader_cls=UnstructuredMarkdownLoader)</code></li>
<li>Create a splitter that can split documents to chunks <code>text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)</code></li>
<li>Convert each chunk and store as Embeddings in a Chroma DB <code>Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)</code></li>
<li>Use the database as retriever to get relevant text (context), and based on ‚Äòquestion‚Äô, use OpenAI‚Äôs gpt-3.5-turbo (ChatGPT) model to answer question based on context.</li>
</ol>
<pre><code>ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),
    chain_type="stuff",
    retriever=db.as_retriever(),
    memory=memory,
    verbose=False,
)</code></pre>
<ol start="5" type="1">
<li>Also store conversation as chat history in memory <code>memory = ConversationBufferMemory(memory_key="chat_history", return_messages=False)</code></li>
</ol>
<p>The <code>CharacterTextSplitter</code> used above splits texts based using regex and a separator. The separator in this case is <code>'\n\n'</code>. Thus, anytime there are two line breaks, our text splitter will split documents. Internally, in LangChain to split a text, <code>_split_text_with_regex</code> is being called.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simplified version without `keep_separator`</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _split_text_with_regex(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    text: <span class="bu">str</span>, separator: <span class="bu">str</span>, keep_separator: <span class="bu">bool</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> List[<span class="bu">str</span>]:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now that we have the separator, split the text</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> separator:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>                splits <span class="op">=</span> re.split(separator, text)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        splits <span class="op">=</span> <span class="bu">list</span>(text)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [s <span class="cf">for</span> s <span class="kw">in</span> splits <span class="cf">if</span> s <span class="op">!=</span> <span class="st">""</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Also, for our embedding model - we are using <code>OpenAIEmbeddings</code>.</p>
<p>This class get‚Äôs invoked in <code>db = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)</code>. Essentially we are creating a Chroma database from our chunks after converting chunks to embeddings.</p>
<p>What goes under the hood is that we first instantiate a <a href="https://docs.trychroma.com/getting-started">chroma-db <code>collection</code></a>. Next, we use collection‚Äôs <code>upsert</code> method passing in embeddings and texts. And this way, we have created our vector database that can be used to find nearest chunks from our documents based on ‚Äúquery‚Äù using similarity-search.</p>
<p>Once our vector-db is setup, we can query it using <code>collection.query</code> passing in the <code>query_texts</code>, and get <code>n_results</code> back. It‚Äôs really that simple!</p>
<pre><code>results = collection.query(
    query_texts=["This is a query document"],
    n_results=2
)</code></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>‚ùì Some questions here to ask would be</p>
<ol type="1">
<li>Would results look different or better if we used Cohere Embeddings? What would be the price difference?</li>
<li>What would the quality of results be like if we used open source models like Llama-v2 released a few days ago?</li>
<li>What if we used <code>sentence-transformers</code>?</li>
<li>Do we really need a vector database? Can we store the embeddings as a <code>np.array</code> and use cosine-similarity to find nearest embeddings?</li>
</ol>
</div>
</div>
<p>Now, that we have stored our texts and embeddings in a vector database (in our case, a chroma-db), what happens next is that we need to be able to pass our ‚Äúcontext‚Äù with our ‚Äúquery‚Äù to a large language model, and prompt it to return the answer.</p>
<p>In langchain, all of this happens under the hood in <code>ConversationalRetrievalChain</code> which is the main topic of this blog post too. We instantiate an instance of the class using <code>@classmethod</code> called <code>from_llm</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>qa <span class="op">=</span> ConversationalRetrievalChain.from_llm(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    llm<span class="op">=</span>OpenAIChat(temperature<span class="op">=</span><span class="dv">0</span>, max_tokens<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    chain_type<span class="op">=</span><span class="st">"stuff"</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    retriever<span class="op">=</span>db.as_retriever(),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    memory<span class="op">=</span>memory,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    get_chat_history<span class="op">=</span><span class="kw">lambda</span> x: x,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> qa({</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"question"</span>: <span class="st">"Why is it so hard to find a rental property in Australia in June 2023?"</span>, </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"chat_history"</span>: []</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>There are two things that go on inside a conversational retrieval chain.</p>
<p>A conversational retrieval chain can take in a query, and based on the input query (question) and chat-history, it updates it to a new question.</p>
<p>Then, we find the nearest chunks for the new question, and the nearest chunks - referred to as ‚Äúcontext‚Äù get passed to a large language model (such as <code>gpt-3.5-turbo</code> or ChatGPT), to retrieve the answer.</p>
<p>Internally - <code>ConversationalRetrievalChain</code> consists of two chains:</p>
<ol type="1">
<li>A question generator chain, which updates input query/question based on chat history (<code>LLMChain</code>)</li>
<li>And a document chain to join retrieved documents/chunks together (<code>StuffDocumentsChain</code>)</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled" title="On `LLMChain`">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
On <code>LLMChain</code>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Good news! We have already covered <code>LLMChain</code>s in our previous blog post before <a href="https://amaarora.github.io/posts/2023-07-25-llmchain.html">here</a>. In essence, given a prompt, the <code>LLMChain</code> can be used to generate an answer based on the prompt.</p>
<p>Going forward, I am going to assume that the reader has read the previous blog post and has a solid understanding of <code>LLMChain</code>s &amp; <code>Chain</code>s in general.</p>
</div>
</div>
<p>From our previous blog post, we know that anytime we call any chain in langchain, the <code>__call__</code> method from <code>Chain</code> class gets invoked which in turn makes a call to <code>_call</code> method of derived class.</p>
<p>The <code>ConversationalRetrievalChain</code> is a subclass of <code>BaseConversationalRetrievalChain</code> which in turn is a subclass of <code>Chain</code>.</p>
<p>The <code>_call</code> method is implemented inside <code>BaseConversationalRetrievalChain</code> and it looks like below:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _call(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        inputs: Dict[<span class="bu">str</span>, Any],</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        run_manager: Optional[CallbackManagerForChainRun] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        _run_manager <span class="op">=</span> run_manager <span class="kw">or</span> CallbackManagerForChainRun.get_noop_manager()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        question <span class="op">=</span> inputs[<span class="st">"question"</span>]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        get_chat_history <span class="op">=</span> <span class="va">self</span>.get_chat_history <span class="kw">or</span> _get_chat_history</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        chat_history_str <span class="op">=</span> get_chat_history(inputs[<span class="st">"chat_history"</span>])</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> chat_history_str:</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            callbacks <span class="op">=</span> _run_manager.get_child()</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            new_question <span class="op">=</span> <span class="va">self</span>.question_generator.run(</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>                question<span class="op">=</span>question, chat_history<span class="op">=</span>chat_history_str, callbacks<span class="op">=</span>callbacks</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>            new_question <span class="op">=</span> question</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        accepts_run_manager <span class="op">=</span> (</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"run_manager"</span> <span class="kw">in</span> inspect.signature(<span class="va">self</span>._get_docs).parameters</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accepts_run_manager:</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            docs <span class="op">=</span> <span class="va">self</span>._get_docs(new_question, inputs, run_manager<span class="op">=</span>_run_manager)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>            docs <span class="op">=</span> <span class="va">self</span>._get_docs(new_question, inputs)  <span class="co"># type: ignore[call-arg]</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        new_inputs <span class="op">=</span> inputs.copy()</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.rephrase_question:</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>            new_inputs[<span class="st">"question"</span>] <span class="op">=</span> new_question</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        new_inputs[<span class="st">"chat_history"</span>] <span class="op">=</span> chat_history_str</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> <span class="va">self</span>.combine_docs_chain.run(</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            input_documents<span class="op">=</span>docs, callbacks<span class="op">=</span>_run_manager.get_child(), <span class="op">**</span>new_inputs</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        output: Dict[<span class="bu">str</span>, Any] <span class="op">=</span> {<span class="va">self</span>.output_key: answer}</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.return_source_documents:</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>            output[<span class="st">"source_documents"</span>] <span class="op">=</span> docs</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.return_generated_question:</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>            output[<span class="st">"generated_question"</span>] <span class="op">=</span> new_question</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In simple terms, first, the <code>question_generator</code> chain is called that updates the input question/query based on chat history.</p>
<p>Next, we retrieve the documents based on our <code>new_question</code> using similarity search.</p>
<p>These retrieved docs, then get passed to <code>combine_docs_chain</code> which combines the retrieved chunks and passes them over to a large language model (in this case <code>gpt-3.5-turbo</code>) to get back the answer.</p>
<section id="question-generator-chain-inside-conversationalretrievalchain" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="question-generator-chain-inside-conversationalretrievalchain"><span class="header-section-number">0.1</span> Question generator chain inside <code>ConversationalRetrievalChain</code></h3>
<p>Remember that the <code>question_generator</code> is an instance of <code>LLMChain</code>. From our previous blog post if you remember, a <code>LLMChain</code> takes in a prompt (or prompt template with inputs, like in the previous post, it took input ‚Äúproduct‚Äù) and based on the prompt, returns a reply by asking LLM to generate a response.</p>
<p>In this case the prompt for <code>LLMChain</code> is <code>CONDENSE_QUESTION_PROMPT</code> which looks like:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>_template <span class="op">=</span> <span class="st">"""Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="st">Chat History:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="sc">{chat_history}</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="st">Follow Up Input: </span><span class="sc">{question}</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="st">Standalone question:"""</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>CONDENSE_QUESTION_PROMPT <span class="op">=</span> PromptTemplate.from_template(_template)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So taking in a <code>chat_history</code> and the original question (from the user), internally a new question get‚Äôs generated! Let‚Äôs see it in action. We can get the newly generated question by passing in <code>return_generated_question=True</code> to the <code>ConversationRetrievalChain</code>. Remember, the first time a new question is not generated because there is no chat history.</p>
<p>The new question only get‚Äôs generated from the second point forward.</p>
</section>
<section id="document-chain" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="document-chain"><span class="header-section-number">0.2</span> Document Chain</h3>


</section>

<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css"><div id="mc_embed_signup">
    <form action="https://github.us4.list-manage.com/subscribe/post?u=e847230346a7c78d4745ae796&amp;id=7a63b2b273&amp;f_id=005f58e8f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2 class="anchored">Subscribe to Aman Arora's blog:</h2>
        <div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
<div hidden="true"><input type="hidden" name="tags" value="7232948"></div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e847230346a7c78d4745ae796_7a63b2b273" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
                <p class="brandingLogo"><a href="http://eepurl.com/il3baM" title="Mailchimp - email marketing made easy and fun"><img src="https://eep.io/mc-cdn-images/template_images/branding_logo_text_dark_dtp.svg"></a></p>
            </div>
        </div>
    </div>
</form>
</div><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>