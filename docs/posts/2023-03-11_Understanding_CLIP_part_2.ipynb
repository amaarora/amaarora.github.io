{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fc0e271a",
   "metadata": {},
   "source": [
    "---\n",
    "title: The Annotated CLIP (Part-2)\n",
    "subtitle: Learning Transferable Visual Models From Natural Language Supervision\n",
    "description: | \n",
    "    This post is part-2 of the two series blog posts on CLIP. In this blog, we present the PyTorch code behind CLIP for model building and training. \n",
    "categories:\n",
    "  - Multimodal\n",
    "  - Transformers\n",
    "author: Aman Arora\n",
    "date: \"03/11/2023\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "title-block-banner: true\n",
    "bibliography: ../references.bib\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "code-fold: false\n",
    "image: ../images/clip.png\n",
    "format: ipynb\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc50575",
   "metadata": {},
   "source": [
    "## Pre-requisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95caac",
   "metadata": {},
   "source": [
    "As part of this blog post, I assume that the reader understands the following: \n",
    "\n",
    "1. ResNet Architecture \n",
    "2. Vision Trasnsformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3095026",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nspf00KpU-g\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eae02a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As part of this blog post we will be uncovering the inner workings of CLIP - [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (@clip) by looking at it's PyTorch implementation. For a gentle introduction to CLIP, please refer to [part-1](https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html) of the blog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ec454",
   "metadata": {},
   "source": [
    "## CLIP Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556bd2cd",
   "metadata": {},
   "source": [
    "![Summary of CLIP approach](../images/clip.png){#fig-clip}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5005bbe",
   "metadata": {},
   "source": [
    "From @fig-clip, we can see that we have a text encoder and image encoder. These encoders are responsible for taking in the image and the text and and converting them to an embedding space. \n",
    "\n",
    "The image encoder encodes images to embeddings $I_1, I_2, I_2 ... I_N$, and the text encoder encodes respective image captions to $T_1, T_2, T_3 ... T_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a120e2c",
   "metadata": {},
   "source": [
    "But, how do these text and image encoders look like? Let's start with the image encoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd050f",
   "metadata": {},
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c9487",
   "metadata": {},
   "source": [
    "*We consider two different architectures for the image encoder. For the first, we use ResNet-50 (@resnet) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from @bag_of_tricks and the antialiased rect-2 blur pooling from @blurpool. We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (@vit). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4494e4",
   "metadata": {},
   "source": [
    "### Modified ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8872b",
   "metadata": {},
   "source": [
    "Let's start with the first architecture. \n",
    "\n",
    "*For the first, we use ResNet-50 (@resnet) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from @bag_of_tricks and the antialiased rect-2 blur pooling from @blurpool. We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78841ff5",
   "metadata": {},
   "source": [
    "There are 3 major changes as mentioned to the ResNet architecture in CLIP: \n",
    "\n",
    "- There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "- Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "- The final pooling layer is a QKV attention instead of an average pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f851a",
   "metadata": {},
   "source": [
    "#### ResNet stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0fae1",
   "metadata": {},
   "source": [
    "Let's look at all of them one by one in code. First, we start with *There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0087f6",
   "metadata": {},
   "source": [
    "-- Add image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72079aec",
   "metadata": {},
   "source": [
    "In the vanilla ResNet architecture, the stem consists of a 7x7 stride-2 convolution. This is what the stem looks like in the vanilla ResNet architecture. \n",
    "\n",
    "```python\n",
    "class VanillaResNet:\n",
    "    def __init__(...):\n",
    "        self.stem = nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2f6b8",
   "metadata": {},
   "source": [
    "However, in the paper @bag_of_tricks, where at the time, the authors raised *ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet*. From the paper, one of the tweaks used in the architecture: \n",
    "\n",
    "*A 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab451a4",
   "metadata": {},
   "source": [
    "-- Add image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e48356",
   "metadata": {},
   "source": [
    "In code this looks like: \n",
    "\n",
    "```python \n",
    "class ModifiedResNet:\n",
    "    def __init__(...):\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.act2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.act3 = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "\n",
    "\n",
    "    def stem(self, x):\n",
    "        x = self.act1(self.bn1(self.conv1(x)))\n",
    "        x = self.act2(self.bn2(self.conv2(x)))\n",
    "        x = self.act3(self.bn3(self.conv3(x)))\n",
    "        x = self.avgpool(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615d2c6",
   "metadata": {},
   "source": [
    "#### Blur Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ffffc",
   "metadata": {},
   "source": [
    "The next change is to use `BlurPooling` - *Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469df1e5",
   "metadata": {},
   "source": [
    "In this section I will introduce BlurPooling and share how it is implemented in the `ModifiedResNet` architecture. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed92846",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HjewNBZz00w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec49ce",
   "metadata": {},
   "source": [
    "From the research paper, \n",
    "\n",
    "*Modern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bfc20",
   "metadata": {},
   "source": [
    "#### Final pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf030f77",
   "metadata": {},
   "source": [
    "The last change in the network architecture is to use QKV attention instead of an average pool. *We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a23ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x, key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0.,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0f5ec",
   "metadata": {},
   "source": [
    "### Modified ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7b53b",
   "metadata": {},
   "source": [
    "From the paper:\n",
    "\n",
    "*For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (@vit). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0903243",
   "metadata": {},
   "source": [
    "Since the architecture is very similar to vanilla Vision Transformer, with a very minor change of adding LayerNorm after combining Patch embeddings and positional embeddings, I will not be covering the architecture in detail in this blog post. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7b85f",
   "metadata": {},
   "source": [
    "For reference to ViT, please refer to my previous blog post that covers the architecture in detail with PyTorch code implementation - [Vision Transformer](https://amaarora.github.io/posts/2021-01-18-ViT.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85271924",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00d42f",
   "metadata": {},
   "source": [
    "In the previous section, we covered the two types of Image Encoders used in CLIP. As mentioned in part-1 of the blog series on CLIP, the image encoders are actually architecture agnostic - that is, any standard architecture can be used to extract embeddings from images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519089e",
   "metadata": {},
   "source": [
    "In this section, let's look at the text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad0241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
