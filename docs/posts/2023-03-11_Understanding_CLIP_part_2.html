<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Arora">
<meta name="dcterms.date" content="2023-03-11">
<meta name="description" content="This post is part-2 of the two series blog posts on CLIP (for part-1, please refer to my previous blog post). In this blog, we present the PyTorch code behind CLIP for model building and training. This blog post is in itself a working Jupyter Notebook.">

<title>The Annotated CLIP (Part-2)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href=".././images/logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-158677010-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="twitter:title" content="The Annotated CLIP (Part-2)">
<meta name="twitter:description" content="This post is part-2 of the two series blog posts on CLIP (for part-1, please refer to my previous blog post). In this blog, we present the PyTorch code behind CLIP for model building and training. This blog post is in itself a working Jupyter Notebook.">
<meta name="twitter:image" content="../images/clip.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Aman Arora’s Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">Aman Arora</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/amaarora" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/amaarora" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aroraaman/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Annotated CLIP (Part-2)</h1>
            <p class="subtitle lead">Learning Transferable Visual Models From Natural Language Supervision</p>
                  <div>
        <div class="description">
          <p>This post is part-2 of the two series blog posts on CLIP (for part-1, please refer to my previous blog post). In this blog, we present the PyTorch code behind CLIP for model building and training. This blog post is in itself a working Jupyter Notebook.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Multimodal</div>
                <div class="quarto-category">Transformers</div>
                <div class="quarto-category">Clip</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Arora </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 11, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#prerequisites" id="toc-prerequisites" class="nav-link" data-scroll-target="#prerequisites"><span class="header-section-number">2</span> Prerequisites</a></li>
  <li><a href="#data-download-using-img2dataset-and-preparation-using-webdataset" id="toc-data-download-using-img2dataset-and-preparation-using-webdataset" class="nav-link" data-scroll-target="#data-download-using-img2dataset-and-preparation-using-webdataset"><span class="header-section-number">3</span> Data download using <code>img2dataset</code> and preparation using <code>webdataset</code></a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="header-section-number">4</span> Training</a></li>
  <li><a href="#clip-architecture" id="toc-clip-architecture" class="nav-link" data-scroll-target="#clip-architecture"><span class="header-section-number">5</span> CLIP Architecture</a></li>
  <li><a href="#sec-img-encoder" id="toc-sec-img-encoder" class="nav-link" data-scroll-target="#sec-img-encoder"><span class="header-section-number">6</span> Image Encoder</a>
  <ul class="collapse">
  <li><a href="#modified-resnet" id="toc-modified-resnet" class="nav-link" data-scroll-target="#modified-resnet"><span class="header-section-number">6.1</span> Modified ResNet</a></li>
  <li><a href="#modified-vit" id="toc-modified-vit" class="nav-link" data-scroll-target="#modified-vit"><span class="header-section-number">6.2</span> Modified ViT</a></li>
  </ul></li>
  <li><a href="#text-encoder" id="toc-text-encoder" class="nav-link" data-scroll-target="#text-encoder"><span class="header-section-number">7</span> Text Encoder</a></li>
  <li><a href="#sec-contrastive-loss" id="toc-sec-contrastive-loss" class="nav-link" data-scroll-target="#sec-contrastive-loss"><span class="header-section-number">8</span> Contrastive Loss</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">9</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<section id="sec-intro" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sec-intro"><span class="header-section-number">1</span> Introduction</h2>
<p>As part of this blog post we will be uncovering the inner workings of CLIP - <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a> by looking at it’s PyTorch implementation. For a gentle introduction to CLIP, please refer to <a href="https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html">part-1</a> of the blog.</p>
<div id="fig-clip" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/clip.png" class="img-fluid figure-img" width="500"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Summary of CLIP approach</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>All code referenced in this blog post has been copied (and sometimes modified) from the wonderful <a href="https://github.com/mlfoundations/open_clip">Open CLIP repository</a>.</p>
<p>Also note that code-folding has been set in this blog post, so you will have to unfold code to have a look at it. :)</p>
</div>
</div>
<p>In this blog post, we will be covering the following with references to further resources where necessary:</p>
<ol type="1">
<li><em>Data download and preparation</em></li>
<li><em>CLIP architecture in code</em></li>
<li><em>CLIP image encoder</em></li>
<li><em>CLIP text encoder</em></li>
<li><em>CLIP loss function</em></li>
</ol>
<blockquote class="blockquote">
<p>From the <a href="https://github.com/mlfoundations/open_clip">open clip repository</a>, I found the most complex part to be data preparation. That in itself could be a separate blog post, and therefore, I have only covered it partly here as the main focus is to look at the CLIP architecture. <strong>As part of this blog post we are going to assume that there is some magic function that can read the input images and texts and return tensors of shape <code>[N, 3, 224, 224]</code> &amp; <code>[N, 77]</code> respectively, where <span class="math inline">\(N\)</span> is the batch size.</strong></p>
</blockquote>
</section>
<section id="prerequisites" class="level2 page-columns page-full" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="prerequisites"><span class="header-section-number">2</span> Prerequisites</h2>
<p>As part of this blog post, I am going to assume that the reader has a good understanding of the ResNet architecture (<span class="citation" data-cites="resnet">K. He et al. (<a href="#ref-resnet" role="doc-biblioref">2015</a>)</span>) and Vision Transformer (<span class="citation" data-cites="vit">Dosovitskiy et al. (<a href="#ref-vit" role="doc-biblioref">2020</a>)</span>).</p>
<div class="no-row-height column-margin column-container"></div><p>I am also going to assume that the reader also has a good basic understanding of CLIP after having read <a href="https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html">part-1</a> of this blog series.</p>
<p>If the reader would like a refresher on the ResNet architecture, please refer to the following video from paper reading group, that I hosted at <a href="https://wandb.ai/">Weights and Biases</a>.</p>
<div style="text-align: center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/nspf00KpU-g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="">
</iframe>
</div>
<p>Also, the authors use Vision Transformer as one of the backbones in Image Encoder. For complete understanding of ViT with PyTorch code implementation, refer to my previous blog post (in collaboration with <a href="https://twitter.com/dr_hb_ai">Dr Habib Bukhari</a>) - <a href="https://amaarora.github.io/posts/2021-01-18-ViT.html">Vision Transformer</a>. We won’t be covering ViT architecture as part of this blog post.</p>
<p>For the text encoder, the authors used the GPT-2 architecture. I have previously covered the entirety of the model with PyTorch code implementation at <a href="https://amaarora.github.io/posts/2020-02-18-annotatedGPT2.html">The annotated GPT-2</a>.</p>
<p>Now, with prerequisites and introductions out of the way, let’s get started with the first item which is <strong>“Data download and preparation”.</strong></p>
</section>
<section id="data-download-using-img2dataset-and-preparation-using-webdataset" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="data-download-using-img2dataset-and-preparation-using-webdataset"><span class="header-section-number">3</span> Data download using <code>img2dataset</code> and preparation using <code>webdataset</code></h2>
<p>Before we can start training any models, we need data. In this part of the blog post we are looking at data preparation part of CLIP. Remember, that CLIP was trained on 400M (image, text) pairs.</p>
<p>From the paper:</p>
<p><em>We create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision.</em></p>
<p>So, how does one create these image text pairs in practice? One of the easiest ways to train CLIP using Open CLIP is to generate the dataset in the form of <code>webdataset</code> using <code>img2dataset</code>.</p>
<p><strong>We will only be creating a tiny version consisting of only 1,000 (image, text) and not the complete 400M dataset used in CLIP.</strong></p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install img2dataset </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> img2dataset <span class="im">import</span> download</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shutil</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> os.path.abspath(<span class="st">"sample"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(output_dir):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    shutil.rmtree(output_dir)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>download(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    processes_count<span class="op">=</span>os.cpu_count(),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    thread_count<span class="op">=</span>os.cpu_count()<span class="op">*</span><span class="dv">2</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    url_list<span class="op">=</span><span class="st">"/home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_1000.parquet"</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    image_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    output_folder<span class="op">=</span>output_dir,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    output_format<span class="op">=</span><span class="st">"webdataset"</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    input_format<span class="op">=</span><span class="st">"parquet"</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    url_col<span class="op">=</span><span class="st">"URL"</span>,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    caption_col<span class="op">=</span><span class="st">"TEXT"</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    enable_wandb<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    number_sample_per_shard<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    distributor<span class="op">=</span><span class="st">"multiprocessing"</span>,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Starting the downloading of this file
Sharding file number 1 of 1 called /home/ubuntu/GIT_REPOS/data/img2dataset/tests/test_files/test_1000.parquet
File sharded in 10 shards
Downloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!
worker  - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 12 - count: 100
total   - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 12 - count: 100
worker  - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 12 - count: 100
total   - success: 0.845 - failed to download: 0.145 - failed to resize: 0.010 - images per sec: 23 - count: 200
worker  - success: 0.850 - failed to download: 0.140 - failed to resize: 0.010 - images per sec: 8 - count: 100
total   - success: 0.847 - failed to download: 0.143 - failed to resize: 0.010 - images per sec: 23 - count: 300
worker  - success: 0.850 - failed to download: 0.150 - failed to resize: 0.000 - images per sec: 9 - count: 100
total   - success: 0.848 - failed to download: 0.145 - failed to resize: 0.007 - images per sec: 30 - count: 400
worker  - success: 0.840 - failed to download: 0.160 - failed to resize: 0.000 - images per sec: 10 - count: 100
total   - success: 0.846 - failed to download: 0.148 - failed to resize: 0.006 - images per sec: 38 - count: 500
worker  - success: 0.830 - failed to download: 0.160 - failed to resize: 0.010 - images per sec: 10 - count: 100
total   - success: 0.843 - failed to download: 0.150 - failed to resize: 0.007 - images per sec: 31 - count: 600
worker  - success: 0.830 - failed to download: 0.150 - failed to resize: 0.020 - images per sec: 9 - count: 100
total   - success: 0.841 - failed to download: 0.150 - failed to resize: 0.009 - images per sec: 35 - count: 700
worker  - success: 0.880 - failed to download: 0.100 - failed to resize: 0.020 - images per sec: 6 - count: 100
total   - success: 0.846 - failed to download: 0.144 - failed to resize: 0.010 - images per sec: 40 - count: 800
worker  - success: 0.840 - failed to download: 0.150 - failed to resize: 0.010 - images per sec: 4 - count: 100
total   - success: 0.846 - failed to download: 0.144 - failed to resize: 0.010 - images per sec: 34 - count: 900
worker  - success: 0.900 - failed to download: 0.100 - failed to resize: 0.000 - images per sec: 4 - count: 100
total   - success: 0.851 - failed to download: 0.140 - failed to resize: 0.009 - images per sec: 38 - count: 1000
CPU times: user 71.6 ms, sys: 51 ms, total: 123 ms
Wall time: 32.6 s</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>0it [00:00, ?it/s]10it [00:31,  3.19s/it]</code></pre>
</div>
</div>
<p>So it takes ~35 seconds to create the tiny dataset on my 8 core machine. Please refer to <a href="https://github.com/rom1504/img2dataset">img2dataset</a> for information on other available (image, text) pair datasets.</p>
<p>But, what do the downloads look like? Let’s find out.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>np.array(<span class="bu">sorted</span>(<span class="bu">list</span>(Path(<span class="st">'./sample/'</span>).glob(<span class="st">'*tar'</span>))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([PosixPath('sample/00000.tar'), PosixPath('sample/00001.tar'),
       PosixPath('sample/00002.tar'), PosixPath('sample/00003.tar'),
       PosixPath('sample/00004.tar'), PosixPath('sample/00005.tar'),
       PosixPath('sample/00006.tar'), PosixPath('sample/00007.tar'),
       PosixPath('sample/00008.tar'), PosixPath('sample/00009.tar')],
      dtype=object)</code></pre>
</div>
</div>
<p>As we can see above, the script from <code>img2dataset</code> downloads <code>{00000...00009).tar</code> files. What’s in these <code>.tar</code> files? Answer lies in the documentation of <a href="https://webdataset.github.io/webdataset/">webdataset</a>. I won’t be covering more details as part of this blog post as we have a lot to cover stil!</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further reading
</div>
</div>
<div class="callout-body-container callout-body">
<p>One key thing that I haven’t covered as part of this blog post, is that how do these <code>.tar</code> files get converted to inputs to the CLIP model? Let me share briefly here and point to the right directions.</p>
<p>First, a data pipeline is created using <code>wds</code> (webdataset) package. You can find the pipeline being created <a href="https://github.com/mlfoundations/open_clip/blob/main/src/training/data.py#L349">here</a>.</p>
<p>This pipeline takes in a tokenizer that’s <code>HFTokenizer</code>, see <a href="https://github.com/mlfoundations/open_clip/blob/main/src/training/main.py#L337">here</a>. This <code>HFTokenizer</code> tokenizes the input and returns <code>input_ids</code> of <code>context_length</code> = 77.</p>
</div>
</div>
</section>
<section id="training" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="training"><span class="header-section-number">4</span> Training</h2>
<p>Now, to train a CLIP model of your choice on a single GPU, simply clone the <a href="https://github.com/mlfoundations/open_clip">Open Clip repository</a> and run the following command in your terminal in the <code>src/</code> directory:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>python <span class="op">-</span>m training.main <span class="op">\</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>save<span class="op">-</span>frequency <span class="dv">1</span> <span class="op">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>zeroshot<span class="op">-</span>frequency <span class="dv">1</span> <span class="op">\</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>train<span class="op">-</span>data<span class="op">=</span><span class="st">"/home/ubuntu/GIT_REPOS/amaarora.github.io/posts/sample/{00000..00009}.tar"</span> \</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>warmup <span class="dv">10</span> <span class="op">\</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>batch<span class="op">-</span>size<span class="op">=</span><span class="dv">16</span> <span class="op">\</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>lr<span class="op">=</span><span class="fl">1e-3</span> <span class="op">\</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>wd<span class="op">=</span><span class="fl">0.1</span> <span class="op">\</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>epochs<span class="op">=</span><span class="dv">30</span> <span class="op">\</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>workers<span class="op">=</span><span class="dv">8</span> <span class="op">\</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>model RN50 <span class="op">\</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>train<span class="op">-</span>num<span class="op">-</span>samples <span class="dv">852</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This should kick off training on your machine. Now, that we can train CLIP models on our own machines, let’s look at some of the details of training scrip and the CLIP architecture.</p>
</section>
<section id="clip-architecture" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="clip-architecture"><span class="header-section-number">5</span> CLIP Architecture</h2>
<div id="fig-clip" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/clip.png" class="img-fluid figure-img" width="500"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Summary of CLIP approach</figcaption>
</figure>
</div>
<p>From <a href="#fig-clip">Figure&nbsp;2</a>, we can see that we have a text encoder and image encoder. These encoders are responsible for taking in the image and the text and and converting them to an embedding space.</p>
<p>As mentioned in <a href="#sec-intro">Section&nbsp;1</a>, we will assume that there is some magic function that can read the <code>.tar</code> files and return tensors of shape <code>[1, 3, 224, 224]</code> for each image, and <code>[1, 77]</code>, for each text.</p>
<p>These inputs then get encoded to embedding space using image and text encoder respectively.</p>
<p>The image encoder encodes images to embeddings <span class="math inline">\(I_1, I_2, I_2 ... I_N\)</span>, and the text encoder encodes respective image captions to <span class="math inline">\(T_1, T_2, T_3 ... T_N\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In practice, the embedding size in CLIP is 1024. Therefore is our batch size <span class="math inline">\(N = 8\)</span>, the 8 input images will get encoded to a tensor of shape <span class="math inline">\((8, 1024)\)</span>, and also the 8 input texts will get encoded to a tensor of shape <span class="math inline">\((8, 1024)\)</span>.</p>
</div>
</div>
<p>Let’s start by looking at the inputs and outputs of the overall CLIP model.</p>
<p>First, we load the config, as part of this blog post we will work with <code>ResNet-50</code> architecture as an example. So, let’s start by loading the corresponding config.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json, torch</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> open_clip.model <span class="im">import</span> CLIP</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'../../open_clip/src/open_clip/model_configs/RN50.json'</span>) <span class="im">as</span> o:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    cfg <span class="op">=</span> json.load(o)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>cfg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>{'embed_dim': 1024,
 'vision_cfg': {'image_size': 224,
  'layers': [3, 4, 6, 3],
  'width': 64,
  'patch_size': None},
 'text_cfg': {'context_length': 77,
  'vocab_size': 49408,
  'width': 512,
  'heads': 8,
  'layers': 12}}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CLIP(<span class="op">**</span>cfg).to(device)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>).to(device)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>text  <span class="op">=</span> torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span>cfg[<span class="st">'text_cfg'</span>][<span class="st">'vocab_size'</span>], size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">77</span>)).to(device)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>image_features, text_features, logit_scale   <span class="op">=</span> model(image, text)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>image_features.shape, text_features.shape, logit_scale</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>(torch.Size([1, 1024]),
 torch.Size([1, 1024]),
 tensor(14.2857, device='cuda:0', grad_fn=&lt;ExpBackward0&gt;))</code></pre>
</div>
</div>
<p>As can be seen above, the model expects <code>image</code> and <code>text</code> as inputs where in this case:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>).to(device)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>text  <span class="op">=</span> torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span>cfg[<span class="st">'text_cfg'</span>][<span class="st">'vocab_size'</span>], size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">77</span>)).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You might recognize image shape easily - it represents 1 image with 3 channels (RGB) and (H,W) = 224. For text, each text is tokenized to max length 77. The integer numbers represent <code>token_id</code> with max value of <code>cfg['text_cfg']['vocab_size']</code>.</p>
<p>Makes sense so far?</p>
<p>As for the outputs, the model returns three outputs - <code>image_features</code>, <code>text_features</code> and <code>logit_scale</code>.</p>
<p><code>logit_scale</code> has been covered in more detail in <a href="#sec-contrastive-loss">Section&nbsp;8</a> of this blog post. For now, think of it as a learnable parameter during model training.</p>
<p>As for <code>image_features</code> &amp; <code>text_features</code>, these are the respective embeddings <span class="math inline">\(I_1, I_2, I_2 ... I_N\)</span>, and the text encoder encodes respective image captions to <span class="math inline">\(T_1, T_2, T_3 ... T_N\)</span> with reference to <a href="#fig-clip">Figure&nbsp;2</a>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before you proceed further, remember, the encoders are responsible for encoding the input image and text to embeddings of dimension - <span class="math inline">\(1024\)</span>.</p>
<p>We could have also done something like:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CLIP(<span class="op">**</span>cfg).to(device)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>).to(device)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>text  <span class="op">=</span> torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span>cfg[<span class="st">'text_cfg'</span>][<span class="st">'vocab_size'</span>], size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">77</span>)).to(device)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>image_features <span class="op">=</span> model.encode_image(image)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> model.encode_text(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Next, let’s look at the respective architectures of Image and Text encoders in more detail.</p>
</section>
<section id="sec-img-encoder" class="level2 page-columns page-full" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="sec-img-encoder"><span class="header-section-number">6</span> Image Encoder</h2>
<p>From the paper:</p>
<div class="page-columns page-full"><p><em>We consider two different architectures for the image encoder. For the first, we use ResNet-50 (<span class="citation" data-cites="resnet">K. He et al. (<a href="#ref-resnet" role="doc-biblioref">2015</a>)</span>) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from <span class="citation" data-cites="bag_of_tricks">T. He et al. (<a href="#ref-bag_of_tricks" role="doc-biblioref">2018</a>)</span> and the antialiased rect-2 blur pooling from <span class="citation" data-cites="blurpool">Zhang (<a href="#ref-blurpool" role="doc-biblioref">2019</a>)</span>. We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (<span class="citation" data-cites="vit">Dosovitskiy et al. (<a href="#ref-vit" role="doc-biblioref">2020</a>)</span>). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.</em></p><div class="no-row-height column-margin column-container"></div></div>
<section id="modified-resnet" class="level3 page-columns page-full" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="modified-resnet"><span class="header-section-number">6.1</span> Modified ResNet</h3>
<p>Let’s start with the first architecture.</p>
<div class="page-columns page-full"><p><em>For the first, we use ResNet-50 (<span class="citation" data-cites="resnet">K. He et al. (<a href="#ref-resnet" role="doc-biblioref">2015</a>)</span>) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from <span class="citation" data-cites="bag_of_tricks">T. He et al. (<a href="#ref-bag_of_tricks" role="doc-biblioref">2018</a>)</span> and the antialiased rect-2 blur pooling from <span class="citation" data-cites="blurpool">Zhang (<a href="#ref-blurpool" role="doc-biblioref">2019</a>)</span>. We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.</em></p><div class="no-row-height column-margin column-container"><div id="ref-resnet" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. <span>“Deep Residual Learning for Image Recognition.”</span> <em>CoRR</em> abs/1512.03385. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.
</div></div></div>
<p>There are 3 major changes as mentioned to the ResNet architecture in CLIP:</p>
<ul>
<li>There are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.</li>
<li>Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1</li>
<li>The final pooling layer is a QKV attention instead of an average pool</li>
</ul>
<section id="sec-resnet-stem" class="level4 page-columns page-full" data-number="6.1.1">
<h4 data-number="6.1.1" class="anchored" data-anchor-id="sec-resnet-stem"><span class="header-section-number">6.1.1</span> ResNet stem</h4>
<p>Let’s look at all of them one by one in code. First, we start with <em>There are now 3 “stem” convolutions as opposed to 1, with an average pool instead of a max pool.</em></p>
<div id="fig-resnet-arch" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/resnet-arch.png" class="img-fluid figure-img" width="500"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Overview of ResNet architecture</figcaption>
</figure>
</div>
<p>In the vanilla ResNet architecture, the stem consists of a 7x7 stride-2 convolution. This is what the stem looks like in the vanilla ResNet architecture.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VanillaResNet:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(...):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stem <span class="op">=</span> nn.Conv2d(in_chans, inplanes, kernel_size<span class="op">=</span><span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>However, in the paper <span class="citation" data-cites="bag_of_tricks">T. He et al. (<a href="#ref-bag_of_tricks" role="doc-biblioref">2018</a>)</span>, where at the time, the authors raised <em>ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet</em>. From the paper, one of the tweaks used in the architecture:</p>
<div class="no-row-height column-margin column-container"></div><p><em>A 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions.</em></p>
<div id="fig-resnet-tweak" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/resnet-tweak.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Overview of ResNet tweak</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>On top of replacing <span class="math inline">\(7x7\)</span> stride-2 convolution with 3 consecutive <span class="math inline">\(3x3\)</span> convolutions, the authors also replaced max pooling with average pooling, but image above shows max pooling as it has been directly copied from <span class="citation" data-cites="bag_of_tricks">T. He et al. (<a href="#ref-bag_of_tricks" role="doc-biblioref">2018</a>)</span>.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-bag_of_tricks" class="csl-entry" role="listitem">
He, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. <span>“Bag of Tricks for Image Classification with Convolutional Neural Networks.”</span> <em>CoRR</em> abs/1812.01187. <a href="http://arxiv.org/abs/1812.01187">http://arxiv.org/abs/1812.01187</a>.
</div></div><p>In code this looks like:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModifiedResNet:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(...):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, width <span class="op">//</span> <span class="dv">2</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm2d(width <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act1 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(width <span class="op">//</span> <span class="dv">2</span>, width <span class="op">//</span> <span class="dv">2</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> nn.BatchNorm2d(width <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act2 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(width <span class="op">//</span> <span class="dv">2</span>, width, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn3 <span class="op">=</span> nn.BatchNorm2d(width)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act3 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avgpool <span class="op">=</span> nn.AvgPool2d(<span class="dv">2</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> stem(<span class="va">self</span>, x):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.act1(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x)))</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.act2(<span class="va">self</span>.bn2(<span class="va">self</span>.conv2(x)))</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.act3(<span class="va">self</span>.bn3(<span class="va">self</span>.conv3(x)))</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.stem(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="blur-pool" class="level4 page-columns page-full" data-number="6.1.2">
<h4 data-number="6.1.2" class="anchored" data-anchor-id="blur-pool"><span class="header-section-number">6.1.2</span> Blur Pool</h4>
<p>The next change is to use <code>BlurPooling</code> - <em>Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride &gt; 1</em>. This change has been adopted from <span class="citation" data-cites="blurpool">Zhang (<a href="#ref-blurpool" role="doc-biblioref">2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>In this section I will introduce BlurPooling and share how it is implemented in the <code>ModifiedResNet</code> architecture.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HjewNBZz00w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="">
</iframe>
<p>From the research paper,</p>
<p><em>Modern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling.</em></p>
<p>Blur Pooling in CLIP has been implemented inside the <code>Bottleneck</code> block as below:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Bottleneck(nn.Module):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    expansion <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inplanes, planes, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stem <span class="op">=</span> create_stem() <span class="co"># stem consists of 3 3x3 convs instead of 1 7x7 stride-2 conv</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> stride <span class="op">&gt;</span> <span class="dv">1</span> <span class="kw">or</span> inplanes <span class="op">!=</span> planes <span class="op">*</span> Bottleneck.expansion:</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.downsample <span class="op">=</span> nn.Sequential(OrderedDict([</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>                (<span class="st">"-1"</span>, nn.AvgPool2d(stride)),</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>                (<span class="st">"0"</span>, nn.Conv2d(inplanes, planes <span class="op">*</span> <span class="va">self</span>.expansion, <span class="dv">1</span>, stride<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)),</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>                (<span class="st">"1"</span>, nn.BatchNorm2d(planes <span class="op">*</span> <span class="va">self</span>.expansion))</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>            ]))</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.stem()</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> identity</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.act3(out)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now the blurring occurs in <code>downsample</code>. Previously, as can be seen in <a href="#fig-resnet-arch">Figure&nbsp;3</a>, this downsample layer would be a stride-2 <span class="math inline">\(1x1\)</span> convolution.</p>
<p>In <code>ModifiedResnet</code>, this downsample consists of:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.downsample <span class="op">=</span> nn.Sequential(OrderedDict([</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"-1"</span>, nn.AvgPool2d(stride)),</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"0"</span>, nn.Conv2d(inplanes, planes <span class="op">*</span> <span class="va">self</span>.expansion, <span class="dv">1</span>, stride<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)),</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"1"</span>, nn.BatchNorm2d(planes <span class="op">*</span> <span class="va">self</span>.expansion))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    ]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Where, the convolution is stride-1.</p>
<p>The blurring occurs in <code>nn.AvgPool2d(stride)</code>. How? See example below:</p>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>pool <span class="op">=</span> nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>img  <span class="op">=</span> np.array(Image.<span class="bu">open</span>(<span class="st">'../images/clip.png'</span>))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>x    <span class="op">=</span> torch.tensor(img, dtype<span class="op">=</span>torch.float64).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>out  <span class="op">=</span> pool(pool(x))</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">8</span>))</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(x.<span class="bu">int</span>().permute(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Input image before average pooling"</span>)<span class="op">;</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(out.<span class="bu">int</span>().permute(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Input image after average pooling"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2023-03-11_Understanding_CLIP_part_2_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As can be seen above, passing the input image through a <code>nn.AvgPool2d</code> blurs the image, and therefore, anti-aliases the input as per <span class="citation" data-cites="blurpool">Zhang (<a href="#ref-blurpool" role="doc-biblioref">2019</a>)</span>. As a result, the resulting model is translation invariant.</p>
<div class="no-row-height column-margin column-container"><div id="ref-blurpool" class="csl-entry" role="listitem">
Zhang, Richard. 2019. <span>“Making Convolutional Networks Shift-Invariant Again.”</span> <em>CoRR</em> abs/1904.11486. <a href="http://arxiv.org/abs/1904.11486">http://arxiv.org/abs/1904.11486</a>.
</div></div></section>
<section id="final-pooling-layer" class="level4 page-columns page-full" data-number="6.1.3">
<h4 data-number="6.1.3" class="anchored" data-anchor-id="final-pooling-layer"><span class="header-section-number">6.1.3</span> Final pooling layer</h4>
<p>This brings us to the final change in <code>ModifiedResnet</code>.</p>
<p>The last change in the network architecture is to use QKV attention instead of an average pool. From the paper:</p>
<p><em>We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image.</em></p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionPool2d(nn.Module):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, spacial_dim: <span class="bu">int</span>, embed_dim: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, output_dim: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> nn.Parameter(torch.randn(spacial_dim <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, embed_dim) <span class="op">/</span> embed_dim <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k_proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q_proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(embed_dim, output_dim <span class="kw">or</span> embed_dim)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(x.shape[<span class="dv">0</span>], x.shape[<span class="dv">1</span>], x.shape[<span class="dv">2</span>] <span class="op">*</span> x.shape[<span class="dv">3</span>]).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># NCHW -&gt; (HW)NC</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>), x], dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># (HW+1)NC</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.positional_embedding[:, <span class="va">None</span>, :].to(x.dtype)  <span class="co"># (HW+1)NC</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> F.multi_head_attention_forward(</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            query<span class="op">=</span>x, key<span class="op">=</span>x, value<span class="op">=</span>x,</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>            embed_dim_to_check<span class="op">=</span>x.shape[<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            q_proj_weight<span class="op">=</span><span class="va">self</span>.q_proj.weight,</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            k_proj_weight<span class="op">=</span><span class="va">self</span>.k_proj.weight,</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            v_proj_weight<span class="op">=</span><span class="va">self</span>.v_proj.weight,</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>            in_proj_weight<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>            in_proj_bias<span class="op">=</span>torch.cat([<span class="va">self</span>.q_proj.bias, <span class="va">self</span>.k_proj.bias, <span class="va">self</span>.v_proj.bias]),</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>            bias_k<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            bias_v<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>            add_zero_attn<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>            dropout_p<span class="op">=</span><span class="fl">0.</span>,</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>            out_proj_weight<span class="op">=</span><span class="va">self</span>.c_proj.weight,</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>            out_proj_bias<span class="op">=</span><span class="va">self</span>.c_proj.bias,</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>            use_separate_proj_weight<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span><span class="va">self</span>.training,</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>            need_weights<span class="op">=</span><span class="va">False</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>As can be seen from the code above, we perform multi head self-attention from <span class="citation" data-cites="attention">Vaswani et al. (<a href="#ref-attention" role="doc-biblioref">2017</a>)</span>, on <code>x</code>. One key thing to note above in the <code>forward</code> method is :</p>
<div class="no-row-height column-margin column-container"></div><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>), x], dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># (HW+1)NC</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This corresponds to <em>“query is conditioned on the global average-pooled representation of the image”</em> from the paper because the final output that is returned is <code>x[0]</code> and <code>x[0]</code> is global average pooled representation of the image.</p>
</section>
</section>
<section id="modified-vit" class="level3 page-columns page-full" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="modified-vit"><span class="header-section-number">6.2</span> Modified ViT</h3>
<p>From the paper:</p>
<div class="page-columns page-full"><p><em>For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (<span class="citation" data-cites="vit">Dosovitskiy et al. (<a href="#ref-vit" role="doc-biblioref">2020</a>)</span>). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.</em></p><div class="no-row-height column-margin column-container"><div id="ref-vit" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div></div></div>
<p>Since the architecture is very similar to vanilla Vision Transformer, with a very minor change of adding LayerNorm after combining Patch embeddings and positional embeddings, I will not be covering the architecture in detail in this blog post.</p>
<p>For reference to ViT, please refer to my previous blog post that covers the architecture in detail with PyTorch code implementation - <a href="https://amaarora.github.io/posts/2021-01-18-ViT.html">Vision Transformer</a></p>
<p>Having covered both Image encoders used in CLIP architecture, it is now time to move on to the text encoder.</p>
</section>
</section>
<section id="text-encoder" class="level2 page-columns page-full" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="text-encoder"><span class="header-section-number">7</span> Text Encoder</h2>
<p>In this section, let’s look at the text encoder of CLIP architecture. From the paper:</p>
<div class="page-columns page-full"><p><em>The text encoder is a Transformer (<span class="citation" data-cites="attention">Vaswani et al. (<a href="#ref-attention" role="doc-biblioref">2017</a>)</span>) with the architecture modifications described in Radford et al.&nbsp;(2019). As a base size we use a 63M-parameter 12layer 512-wide model with 8 attention heads. The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015). For computational efficiency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary objective, though exploration of this is left as future work.</em></p><div class="no-row-height column-margin column-container"><div id="ref-attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>CoRR</em> abs/1706.03762. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.
</div></div></div>
<p>I have previously covered the complete GPT-2 architecture used as text encoder in my previous blog post at <a href="https://amaarora.github.io/posts/2020-02-18-annotatedGPT2.html">The annotated GPT-2</a> and therefore, won’t be covering it here in this blog post.</p>
</section>
<section id="sec-contrastive-loss" class="level2 page-columns page-full" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="sec-contrastive-loss"><span class="header-section-number">8</span> Contrastive Loss</h2>
<p>One thing that I was most interested in when I started to write the CLIP blog post was to look at Contrastive Loss and understand it in PyTorch code.</p>
<p>In this section, that is exactly what we will be doing.</p>
<p>If you remember from <a href="#sec-img-encoder">Section&nbsp;6</a>, the images get encoded as image features to shape <code>torch.Size([16, 1024])</code> and texts get encoded to text features of shape <code>torch.Size([16, 1024])</code>.</p>
<p>Let’s look at the inputs and outputs of <code>ClipLoss</code> before implementing ourselves.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> open_clip.loss <span class="im">import</span> ClipLoss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>image_features <span class="op">=</span> torch.randn(<span class="dv">16</span>, <span class="dv">1024</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>text_features  <span class="op">=</span> torch.randn(<span class="dv">16</span>, <span class="dv">1024</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>loss_fn        <span class="op">=</span> ClipLoss()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>logit_scale    <span class="op">=</span> nn.Parameter(torch.tensor(np.log(<span class="dv">1</span><span class="op">/</span><span class="fl">0.07</span>)))</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> loss_fn(image_features, text_features, logit_scale)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor(134.1310, grad_fn=&lt;DivBackward0&gt;)</code></pre>
</div>
</div>
<p>One thing you’ll notice is that the ClipLoss expects a parameter called <code>logit_scale</code>. Now, what is this <code>logit_scale</code> parameter?</p>
<p>From the paper:</p>
<div class="page-columns page-full"><p><em>The learnable temperature parameter <span class="math inline">\(τ\)</span> was initialized to the equivalent of 0.07 from (<span class="citation" data-cites="rotation_equivalent_cnn">Veeling et al. (<a href="#ref-rotation_equivalent_cnn" role="doc-biblioref">2018</a>)</span>) and clipped to prevent scaling the logits by more than 100 which we found necessary to prevent training instability.</em></p><div class="no-row-height column-margin column-container"><div id="ref-rotation_equivalent_cnn" class="csl-entry" role="listitem">
Veeling, Bastiaan S., Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. 2018. <span>“Rotation Equivariant CNNs for Digital Pathology.”</span> <a href="https://doi.org/10.48550/ARXIV.1806.03962">https://doi.org/10.48550/ARXIV.1806.03962</a>.
</div></div></div>
<p>But, rather than being initialised to <span class="math inline">\(0.07\)</span>, this temperature parameter <span class="math inline">\(τ\)</span> get’s initialized as <code>nn.Parameter(torch.tensor(np.log(1/0.07)))</code>. For further explanation, see this issue <a href="https://github.com/openai/CLIP/issues/46">here</a>.</p>
<p>Now, having looked at the inputs and outputs and also <code>logit_scale</code>, it is time to look at the source code. Remember contrastive loss and what it does from <a href="https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html#summary-with-pseudo-code">part-1</a> of the blog post? As a quick revision, let me re-post the image here too.</p>
<div id="fig-cosine-similarity" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/cosine_similarity.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Cosine similarity between text and image features</figcaption>
</figure>
</div>
<p>Contrastive loss is trying to maximise the cosine similarity on the diagonal and minimise it elsewhere. But, how? In pseudo-code this looked something like:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># symmetric loss function </span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.arange(n) </span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>loss_i <span class="op">=</span> cross_entropy_loss(logits, labels, axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>loss_t <span class="op">=</span> cross_entropy_loss(logits, labels, axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (loss_i <span class="op">+</span> loss_t)<span class="op">/</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once, we have logits, which is the <span class="math inline">\(8 x 8\)</span> matrix as in <a href="#fig-cosine-similarity">Figure&nbsp;5</a> above, we calculate Cross Entropy Loss once for <code>axis=0</code> and once for <code>axis=1</code>, this way, we are trying to match the diagonal to corresponding image and text because the labels are aligned on both the axis.</p>
<p>But, how does this look like in code? Let’s see.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The below implementation of <code>ClipLoss</code> is a minimalistic version of the complete implementation from <a href="https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/loss.py#L66">open clip</a>.</p>
</div>
</div>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ClipLoss(nn.Module):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_ground_truth(<span class="va">self</span>, device, num_logits) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.arange(num_logits, device<span class="op">=</span>device, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> labels</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_logits(<span class="va">self</span>, image_features, text_features, logit_scale):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        logits_per_image <span class="op">=</span> logit_scale <span class="op">*</span> image_features <span class="op">@</span> text_features.T</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        logits_per_text <span class="op">=</span> logit_scale <span class="op">*</span> text_features <span class="op">@</span> image_features.T        </span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits_per_image, logits_per_text</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features, logit_scale, output_dict<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> image_features.device</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        logits_per_image, logits_per_text <span class="op">=</span> <span class="va">self</span>.get_logits(image_features, text_features, logit_scale)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> <span class="va">self</span>.get_ground_truth(device, logits_per_image.shape[<span class="dv">0</span>])</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> (</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>            F.cross_entropy(logits_per_image, labels) <span class="op">+</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>            F.cross_entropy(logits_per_text, labels)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        ) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"contrastive_loss"</span>: total_loss}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>image_features <span class="op">=</span> torch.randn(<span class="dv">16</span>, <span class="dv">1024</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>text_features  <span class="op">=</span> torch.randn(<span class="dv">16</span>, <span class="dv">1024</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>loss_fn        <span class="op">=</span> ClipLoss()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>logit_scale    <span class="op">=</span> nn.Parameter(torch.tensor(np.log(<span class="dv">1</span><span class="op">/</span><span class="fl">0.07</span>)))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> loss_fn(image_features, text_features, logit_scale)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>{'contrastive_loss': tensor(140.2225, grad_fn=&lt;DivBackward0&gt;)}</code></pre>
</div>
</div>
<p>So, how does the above implementation match pseudo-code?</p>
<p>Let’s start with labels. Since the labels are aligned, that is the <span class="math inline">\(0th\)</span> image on <code>axis=0</code> corresponds to <span class="math inline">\(0th\)</span> text on <code>axis=1</code>, therefore, we can just say that <code>labels = torch.arange(num_logits, device=device, dtype=torch.long)</code>. In this case the value of labels comes out to be <code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7], device='cuda:0')</code> based on <a href="#fig-cosine-similarity">Figure&nbsp;5</a>. By minimising Cross Entropy loss for these labels once for <code>axis=0</code> and once for <code>axis=1</code>, we are making sure that cosine-similarity on the diagonal is maximum and lower otherwise.</p>
<p>In code (as opposed to pseudo-code), rather than minimising cross entropy for <code>axis=0</code>, and <code>axis=1</code>, we calculate:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>    logits_per_image <span class="op">=</span> logit_scale <span class="op">*</span> image_features <span class="op">@</span> text_features.T</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    logits_per_text <span class="op">=</span> logit_scale <span class="op">*</span> text_features <span class="op">@</span> image_features.T    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is same as calculating logits once for <code>axis=1</code>, and once for <code>axis=0</code>, therefore, our total loss is:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> (</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>        F.cross_entropy(logits_per_image, labels) <span class="op">+</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>        F.cross_entropy(logits_per_text, labels)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">/</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is equivalent to pseudo code from paper:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> np.arange(n) </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> cross_entropy_loss(logits, labels, axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> cross_entropy_loss(logits, labels, axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> (loss_i <span class="op">+</span> loss_t)<span class="op">/</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="conclusion" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">9</span> Conclusion</h2>
<p>As part of this blog post I built upon <a href="https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html">part-1</a> of this blog series on CLIP.</p>
<p>We also briefly looked at data preparation as (image, text) pairs for CLIP training using <code>img2dataset</code> and data loading using <code>webdataset</code> packages.</p>
<p>We took a deep dive into the Image Encoder section and looked at all the three tweaks in <code>ModifiedResnet</code> compared to vanilla ResNet architecture.</p>
<p>Finally, we also took a deep dive in contrastive loss and compared the actual PyTorch implementation with pseudo-code from the CLIP research paper.</p>
<p>If you enjoyed reading this blog post, please consider subscribing to my blog for more!</p>



</section>

<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css"><div id="mc_embed_signup">
    <form action="https://github.us4.list-manage.com/subscribe/post?u=e847230346a7c78d4745ae796&amp;id=7a63b2b273&amp;f_id=005f58e8f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2 class="anchored">Subscribe to Aman Arora's blog:</h2>
        <div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
<div hidden="true"><input type="hidden" name="tags" value="7232948"></div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e847230346a7c78d4745ae796_7a63b2b273" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
                <p class="brandingLogo"><a href="http://eepurl.com/il3baM" title="Mailchimp - email marketing made easy and fun"><img src="https://eep.io/mc-cdn-images/template_images/branding_logo_text_dark_dtp.svg"></a></p>
            </div>
        </div>
    </div>
</form>
</div><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>