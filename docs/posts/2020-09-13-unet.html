<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Arora">
<meta name="dcterms.date" content="2020-08-30">
<meta name="description" content="As part of this blog post we will implement the U-Net architecture in PyTorch in 60 lines of code.">

<title>U-Net A PyTorch Implementation in 60 lines of Code</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Aman Arora’s Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/amaarora"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/amaarora"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">U-Net A PyTorch Implementation in 60 lines of Code</h1>
            <p class="subtitle lead">U-Net Convolutional Networks for Biomedical Image Segmentation</p>
                  <div>
        <div class="description">
          <p>As part of this blog post we will implement the U-Net architecture in PyTorch in 60 lines of code.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Computer Vision</div>
                <div class="quarto-category">Model Architecture</div>
                <div class="quarto-category">Image Segmentation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Arora </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 30, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#understanding-input-and-output-shapes-in-u-net" id="toc-understanding-input-and-output-shapes-in-u-net" class="nav-link" data-scroll-target="#understanding-input-and-output-shapes-in-u-net"><span class="toc-section-number">2</span>  Understanding Input and Output shapes in U-Net</a></li>
  <li><a href="#the-factory-production-line-analogy" id="toc-the-factory-production-line-analogy" class="nav-link" data-scroll-target="#the-factory-production-line-analogy"><span class="toc-section-number">3</span>  The Factory Production Line Analogy</a></li>
  <li><a href="#the-black-dots-block" id="toc-the-black-dots-block" class="nav-link" data-scroll-target="#the-black-dots-block"><span class="toc-section-number">4</span>  The Black Dots / Block</a></li>
  <li><a href="#the-encoder" id="toc-the-encoder" class="nav-link" data-scroll-target="#the-encoder"><span class="toc-section-number">5</span>  The Encoder</a></li>
  <li><a href="#the-decoder" id="toc-the-decoder" class="nav-link" data-scroll-target="#the-decoder"><span class="toc-section-number">6</span>  The Decoder</a></li>
  <li><a href="#u-net" id="toc-u-net" class="nav-link" data-scroll-target="#u-net"><span class="toc-section-number">7</span>  U-Net</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">8</span>  Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Today’s blog post is going to be short and sweet. Today, we will be looking at how to implement the <a href="https://arxiv.org/abs/1505.04597">U-Net architecture</a> in PyTorch in 60 lines of code.</p>
<p>This blog is not an introduction to Image Segmentation or theoretical explanation of the U-Net architecture, for that, I would like to refer the reader to <a href="https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47">this wonderful article</a> by Harshall Lamba. Rather, this blog post is a step-by-step explaination of how to implement U-Net from scratch in PyTorch.</p>
<p>In this blogpost - first, we will understand the U-Net architecture - specifically, the input and output shapes of each block. We look at the U-Net Architecture with a factory production line analogy to keep things simple and easy to digest. Next, we will translate our understanding of U-Net architecture to concise PyTorch code.</p>
<p>I also share a working notebook to train this implementation of U-Net of <a href="https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation">SIIM ACR Pneumothorax</a> Kaggle competition <a href="https://github.com/amaarora/amaarora.github.io/blob/master/nbs/Training.ipynb">here</a>. Should be as simple as to switch and model in to the implemention mentioned in this blog post.</p>
<p>So, let’s get started.</p>
</section>
<section id="understanding-input-and-output-shapes-in-u-net" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="understanding-input-and-output-shapes-in-u-net"><span class="header-section-number">2</span> Understanding Input and Output shapes in U-Net</h2>
<p><img src="../images/unet.png" title="fig-1 The U-Net Architecture" class="img-fluid"></p>
<p>As can be seen from <code>fig-1</code>, the architecture is “U-shaped”, hence the name “U-Net”. The complete architecture consists of two parts - the <strong>Encoder</strong> and the <strong>Decoder</strong>. The part on the left of <code>fig-1</code> (yellow highlight in <code>fig-2</code>) is the <strong>Encoder</strong> whereas the part on the right is the <strong>Decoder</strong> (orange highlight in <code>fig-2</code>).</p>
<p>From the paper: &gt; The network architecture is illustrated in Fig-1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network.</p>
<p>The Encoder is like any standard CNN - such as <a href="https://arxiv.org/abs/1512.03385">ResNet</a>, that extracts a meaningful feature map from an input image. As is standard practice for a CNN, the Encoder, <em>doubles the number of channels at every step and halves the spatial dimension</em>.</p>
<p>Next, the Decoder actually upsamples the feature maps, where at every step, it doubles the spatial dimension and halves the number of channels (opposite to that of what an Encoder does).</p>
</section>
<section id="the-factory-production-line-analogy" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="the-factory-production-line-analogy"><span class="header-section-number">3</span> The Factory Production Line Analogy</h2>
<p>Let’s now look at the <strong>U-Net</strong> with a Factory Production Line analogy as in <code>fig-2</code>.</p>
<p><img src="../images/Unet-self.png" title="fig-2 The U-Net Architecture: Simplified" class="img-fluid"></p>
<p>We can think of this whole architecture as a factory line where the Black dots represents assembly stations and the path itself is a conveyor belt where different actions take place to the <strong>Image</strong> on the conveyor belt depending on whether the conveyor belt is Yellow or Orange.</p>
<p>If it’s Yellow, we downsample the Image, using <strong>Max Pooling 2x2</strong> operation that reduces both the Height and Width of the Image by half. If the conveyor belt is Orange, we apply an upsampling operation <code>ConvTranspose2d</code> which doubles the height and width of the image and reduces the number of channels by half. Therefore, the Orange conveyor built performs an operation that is opposite to that performed by Yellow belt.</p>
<p>Also, it is key to note that in the Decoder side, at every assembly station (Black dot), the outputs from the <code>Encoder</code> assembly stations are also concatenated to the inputs.</p>
<p>Let’s start to now turn this simple understanding to PyTorch Code.</p>
</section>
<section id="the-black-dots-block" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="the-black-dots-block"><span class="header-section-number">4</span> The Black Dots / Block</h2>
<p>All assembly stations - the black dots in <code>fig-2</code> have two <code>Conv2D</code> operations with <code>ReLU</code> activation between them. The Convolution operations have kernel size of 3, and no padding. Therefore, the output feature map doesn’t have the same Height and Width as the input feature map.</p>
<p>From the paper: &gt; The contractive path consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.</p>
<p>Let’s write <code>Block</code> in code.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_ch, out_ch):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_ch, out_ch, <span class="dv">3</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu  <span class="op">=</span> nn.ReLU()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(out_ch, out_ch, <span class="dv">3</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(<span class="va">self</span>.relu(<span class="va">self</span>.conv1(x))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s simple - two convolution operations, one that doubles the number of channels from <code>in_ch</code> to <code>out_ch</code> and another that goes from <code>out_ch</code> to <code>out_ch</code>. Both are 2D convolutions with kernel size 3 and no padding as mentioned in the paper followed by <code>ReLU</code> activation.</p>
<p>Let’s make sure this works.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>enc_block <span class="op">=</span> Block(<span class="dv">1</span>, <span class="dv">64</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x         <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">572</span>, <span class="dv">572</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>enc_block(x).shape</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> torch.Size([<span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">568</span>, <span class="dv">568</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So this is looking good, the output size matches that in <code>fig-1</code> top-left. Given an input image with shape <code>1x572x572</code> the output is of shape <code>64x568x568</code>.</p>
</section>
<section id="the-encoder" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="the-encoder"><span class="header-section-number">5</span> The Encoder</h2>
<p>Now that we have implemented the <code>Block</code>s or the black dots in <code>fig-2</code>, we are ready to implement the <strong>Encoder</strong>. The <strong>Encoder</strong> is the contractive path of the U-Net Architecture.</p>
<p>So far we have implemented the convolution operations but not the downsampling part. As mentioned in the paper: &gt; Each block is followed by a 2x2 max pooling operation with stride 2 for downsampling.</p>
<p>So that’s all we need to do, we need add <strong>MaxPooling</strong> operation (Yellow conveyor belt in <code>fig-2</code>) that is performed between two <code>Block</code> operations.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chs<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>,<span class="dv">512</span>,<span class="dv">1024</span>)):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_blocks <span class="op">=</span> nn.ModuleList([Block(chs[i], chs[i<span class="op">+</span><span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(chs)<span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool       <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        ftrs <span class="op">=</span> []</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.enc_blocks:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> block(x)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            ftrs.append(x)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ftrs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s all we have done. On the <strong>Encoder</strong> side, the encoder block or <code>self.enc_blocks</code> is a list of <code>Block</code> operations. Next, we perform the <strong>MaxPooling</strong> operation to the outputs of every block. Since, we also need to store the outputs of the block, we store them in a list called <code>ftrs</code> and return this list.</p>
<p>Let’s make sure this implementation works.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># input image</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>x    <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">572</span>, <span class="dv">572</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>ftrs <span class="op">=</span> encoder(x)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ftr <span class="kw">in</span> ftrs: <span class="bu">print</span>(ftr.shape)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">568</span>, <span class="dv">568</span>])</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">1</span>, <span class="dv">128</span>, <span class="dv">280</span>, <span class="dv">280</span>])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">1</span>, <span class="dv">256</span>, <span class="dv">136</span>, <span class="dv">136</span>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">1</span>, <span class="dv">512</span>, <span class="dv">64</span>, <span class="dv">64</span>])</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">1</span>, <span class="dv">1024</span>, <span class="dv">28</span>, <span class="dv">28</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The output shapes exactly match the shapes mentioned in <code>fig-1</code> - so far, so good. Having implemented the <strong>Encoder</strong>, we are now ready to move on the <strong>Decoder</strong>.</p>
</section>
<section id="the-decoder" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="the-decoder"><span class="header-section-number">6</span> The Decoder</h2>
<p>The <strong>Decoder</strong>, is the expansive path of the U-Net Architecture.</p>
<p>From the paper: &gt; Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution.</p>
<p>Note that we have already implemented the part where two 3x3 convolutions occur followed by ReLU activation in <code>Block</code>. All we need to do to implement the <strong>Decoder</strong> is to add the “up-convolution” (the Orange highlight in <code>fig-2</code>) and the feature concatenation with correspondingly cropped feature map from the contracting path (the gray arrows in <code>fig-2</code>).</p>
<p>Note that in PyTorch, the <a href="https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html">ConvTranspose2d</a> operation performs the “up-convolution”. It accepts parameters like <code>in_channels</code>, <code>out_channels</code>, <code>kernel_size</code> and <code>stride</code> amongst others.</p>
<p>Since the <code>in_channels</code> and <code>out_channels</code> values are different in the <strong>Decoder</strong> depending on where this operation is performed, in the implementation, the “up-convolution” operations are also stored as a list. Stride and kernel size are always 2 as mentioned in the paper.</p>
<p>Now, all we need is to perform feature concatenation. Let’s look at the implementation of the <strong>Decoder</strong> to understand how all this works more clearly -</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chs<span class="op">=</span>(<span class="dv">1024</span>, <span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">64</span>)):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.chs         <span class="op">=</span> chs</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.upconvs    <span class="op">=</span> nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i<span class="op">+</span><span class="dv">1</span>], <span class="dv">2</span>, <span class="dv">2</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(chs)<span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dec_blocks <span class="op">=</span> nn.ModuleList([Block(chs[i], chs[i<span class="op">+</span><span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(chs)<span class="op">-</span><span class="dv">1</span>)]) </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, encoder_features):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.chs)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            x        <span class="op">=</span> <span class="va">self</span>.upconvs[i](x)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            enc_ftrs <span class="op">=</span> <span class="va">self</span>.crop(encoder_features[i], x)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            x        <span class="op">=</span> torch.cat([x, enc_ftrs], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            x        <span class="op">=</span> <span class="va">self</span>.dec_blocks[i](x)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> crop(<span class="va">self</span>, enc_ftrs, x):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        _, _, H, W <span class="op">=</span> x.shape</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        enc_ftrs   <span class="op">=</span> torchvision.transforms.CenterCrop([H, W])(enc_ftrs)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> enc_ftrs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So the <code>self.dec_blocks</code> is a list of <strong>Decoder</strong> <code>Block</code>s that perform the two conv + ReLU operation as mentioned in the paper. The <code>self.upconvs</code> is a list of <code>ConvTranspose2d</code> operations that perform the “up-convolution” operations. And finally, in the <code>forward</code> function, the decoder accepts the <code>encoder_features</code> which were output by the <strong>Encoder</strong> to perform the concatenation operation before passing the result to <code>Block</code>.</p>
<p>That’s really all there is inside the <strong>Decoder</strong> of a U-Net. Let’s make sure this implementation works:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> Decoder()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">1024</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>decoder(x, ftrs[::<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>:]).shape</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> (torch.Size([<span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">388</span>, <span class="dv">388</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And there it is, the final feature map is of size <code>64x388x388</code> which matches that of <code>fig-1</code>. We have just successfully implemented both the <code>Encoder</code> and the <code>Decoder</code> so far.</p>
<p>You might ask why do we do <code>ftrs[::-1][1:]</code>?</p>
<p>Do you remember the shapes of the outputs of the <strong>Encoder</strong>? They were:</p>
<pre><code>torch.Size([1, 64, 568, 568]) #0
torch.Size([1, 128, 280, 280]) #1
torch.Size([1, 256, 136, 136]) #2
torch.Size([1, 512, 64, 64]) #3
torch.Size([1, 1024, 28, 28]) #4</code></pre>
<p>Now, from <code>fig-1</code>, we can see that the feature map with shape <code>torch.Size([1, 1024, 28, 28])</code> is never really concatenated but only a “up-convolution” operation is performed on it. Also, the 1st <strong>Decoder</strong> block in <code>fig-1</code> accepts the inputs from the 3rd position <strong>Encoder</strong> block. Similarly, the 2nd <strong>Decoder</strong> block accepts the inputs from the 2nd position <strong>Encoder</strong> block and so on. Therefore, the <code>encoder_features</code> are reversed before passing them to the Decoder and since the feature map with shape <code>torch.Size([1, 1024, 28, 28])</code> is not concatenated to the <strong>Decoder</strong> blocks, it is not passed.</p>
<p>Hence, the input to the decoder is <code>ftrs[::-1][1:]</code>.</p>
</section>
<section id="u-net" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="u-net"><span class="header-section-number">7</span> U-Net</h2>
<p>Great, we have so far implemented both the <strong>Encoder</strong> and the <strong>Decoder</strong> of U-Net architecture. Let’s put it all together to complete our implementation of U-Net.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UNet(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_chs<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>,<span class="dv">512</span>,<span class="dv">1024</span>), dec_chs<span class="op">=</span>(<span class="dv">1024</span>, <span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">64</span>), num_class<span class="op">=</span><span class="dv">1</span>, retain_dim<span class="op">=</span><span class="va">False</span>, out_sz<span class="op">=</span>(<span class="dv">572</span>,<span class="dv">572</span>)):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder     <span class="op">=</span> Encoder(enc_chs)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder     <span class="op">=</span> Decoder(dec_chs)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head        <span class="op">=</span> nn.Conv2d(dec_chs[<span class="op">-</span><span class="dv">1</span>], num_class, <span class="dv">1</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.retain_dim  <span class="op">=</span> retain_dim</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        enc_ftrs <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        out      <span class="op">=</span> <span class="va">self</span>.decoder(enc_ftrs[::<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>], enc_ftrs[::<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>:])</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        out      <span class="op">=</span> <span class="va">self</span>.head(out)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.retain_dim:</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> F.interpolate(out, out_sz)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s make sure this implementation works:</p>
<pre><code>unet = UNet()
x    = torch.randn(1, 3, 572, 572)
unet(x).shape

&gt;&gt; torch.Size([4, 1, 388, 388])</code></pre>
<p>The output shape matches that of <code>fig-1</code>.</p>
<p>As mentioned before, since the <em>convolution</em> operations are 3x3 without padding, the output feature map size is not the same as the input feature map size. Also, as shown in <code>fig-1</code> the final output is of shape <code>1x388x388</code> while the input Image had dimensions <code>572x572</code>. This can create problems when calculating <code>BCELoss</code> in PyTorch as it expects the input and output feature maps to have the same shape.</p>
<p>Therefore, if we want to <code>retain_dim</code>, I have added <code>F.interpolate</code> operation to the <code>U-Net</code> to make the output size same as the input Image size.</p>
<p>Great, that was all! We have just successfully implemented the <code>U-Net</code> architecture in PyTorch. Everything put together, this looks something like:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_ch, out_ch):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_ch, out_ch, <span class="dv">3</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu  <span class="op">=</span> nn.ReLU()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(out_ch, out_ch, <span class="dv">3</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.conv2(<span class="va">self</span>.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chs<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>,<span class="dv">512</span>,<span class="dv">1024</span>)):</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_blocks <span class="op">=</span> nn.ModuleList([Block(chs[i], chs[i<span class="op">+</span><span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(chs)<span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool       <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        ftrs <span class="op">=</span> []</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.enc_blocks:</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> block(x)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>            ftrs.append(x)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ftrs</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chs<span class="op">=</span>(<span class="dv">1024</span>, <span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">64</span>)):</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.chs         <span class="op">=</span> chs</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.upconvs    <span class="op">=</span> nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i<span class="op">+</span><span class="dv">1</span>], <span class="dv">2</span>, <span class="dv">2</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(chs)<span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dec_blocks <span class="op">=</span> nn.ModuleList([Block(chs[i], chs[i<span class="op">+</span><span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(chs)<span class="op">-</span><span class="dv">1</span>)]) </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, encoder_features):</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.chs)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>            x        <span class="op">=</span> <span class="va">self</span>.upconvs[i](x)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>            enc_ftrs <span class="op">=</span> <span class="va">self</span>.crop(encoder_features[i], x)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>            x        <span class="op">=</span> torch.cat([x, enc_ftrs], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>            x        <span class="op">=</span> <span class="va">self</span>.dec_blocks[i](x)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> crop(<span class="va">self</span>, enc_ftrs, x):</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        _, _, H, W <span class="op">=</span> x.shape</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        enc_ftrs   <span class="op">=</span> torchvision.transforms.CenterCrop([H, W])(enc_ftrs)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> enc_ftrs</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UNet(nn.Module):</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_chs<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>,<span class="dv">512</span>,<span class="dv">1024</span>), dec_chs<span class="op">=</span>(<span class="dv">1024</span>, <span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">64</span>), num_class<span class="op">=</span><span class="dv">1</span>, retain_dim<span class="op">=</span><span class="va">False</span>, out_sz<span class="op">=</span>(<span class="dv">572</span>,<span class="dv">572</span>)):</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder     <span class="op">=</span> Encoder(enc_chs)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder     <span class="op">=</span> Decoder(dec_chs)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head        <span class="op">=</span> nn.Conv2d(dec_chs[<span class="op">-</span><span class="dv">1</span>], num_class, <span class="dv">1</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.retain_dim  <span class="op">=</span> retain_dim</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>        enc_ftrs <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>        out      <span class="op">=</span> <span class="va">self</span>.decoder(enc_ftrs[::<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>], enc_ftrs[::<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>:])</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>        out      <span class="op">=</span> <span class="va">self</span>.head(out)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.retain_dim:</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> F.interpolate(out, out_sz)</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="conclusion" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">8</span> Conclusion</h2>
<p>I hope that today, I was able to provide a concise and easy to digest implementation of the <strong>U-Net architecture</strong> with proper explanations of every line of code.</p>
<p>For a complete working notebook to train this implementation, refer <a href="https://github.com/amaarora/amaarora.github.io/blob/master/nbs/Training.ipynb">here</a>.</p>
<p>As usual, in case I have missed anything or to provide feedback, please feel free to reach out to me at <a href="https://twitter.com/amaarora"><span class="citation" data-cites="amaarora">@amaarora</span></a>.</p>
<p>Also, feel free to <a href="https://amaarora.github.io/subscribe">subscribe to my blog here</a> to receive regular updates regarding new blog posts. Thanks for reading!</p>


</section>

<link href="//cdn-images.mailchimp.com/embedcode/classic-071822.css" rel="stylesheet" type="text/css"><div id="mc_embed_signup">
    <form action="https://github.us4.list-manage.com/subscribe/post?u=e847230346a7c78d4745ae796&amp;id=7a63b2b273&amp;f_id=005f58e8f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
        <div id="mc_embed_signup_scroll">
        <h2 class="anchored">Subscribe to Aman Arora's blog:</h2>
        <div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
    <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" required="">
    <span id="mce-EMAIL-HELPERTEXT" class="helper_text"></span>
</div>
<div hidden="true"><input type="hidden" name="tags" value="7232948"></div>
    <div id="mce-responses" class="clear foot">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e847230346a7c78d4745ae796_7a63b2b273" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
                <p class="brandingLogo"><a href="http://eepurl.com/il3baM" title="Mailchimp - email marketing made easy and fun"><img src="https://eep.io/mc-cdn-images/template_images/branding_logo_text_dark_dtp.svg"></a></p>
            </div>
        </div>
    </div>
</form>
</div><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>