{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9c126062",
   "metadata": {},
   "source": [
    "---\n",
    "title: Image retrieval app using Apple's 4M-21 any-to-any vision model\n",
    "subtitle: 4M-21 An Any-to-Any Vision Model for Tens of Tasks and Modalities\n",
    "description: | \n",
    "    As part of this blog post we are going to build an image retriever app that can take in three inputs - caption, brightness and number of items per image to retrieve the most similar image from a database based on their values. \n",
    "categories:\n",
    "    - VLM\n",
    "author: Aman Arora\n",
    "date: \"07/01/2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "title-block-banner: true\n",
    "bibliography: ../references.bib\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "code-fold: false\n",
    "image: ../images/4m-21.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3adf7f",
   "metadata": {},
   "source": [
    "# Image retrieval App"
   ]
  },
  {
   "cell_type": "raw",
   "id": "315cec69",
   "metadata": {},
   "source": [
    "::: {#fig-demo}\n",
    "\n",
    "{{< video ../images/4m-21-demo.mp4 >}}\n",
    "\n",
    "Image retrieval using 4M-21 any-to-any vision model\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f77ce9",
   "metadata": {},
   "source": [
    "Before, we get started, let's take a moment to understand what's going on in the demo video above:\n",
    "\n",
    "1. This a demo of an image retrieval app that is capable of retrieving most similar images from any given database. It is built on top of gradio.\n",
    "2. As part of this demo, we are able to retrieve images based on the following filters:\n",
    "    a. *Caption* (description of the image)\n",
    "    b. *Brightness* (brightness in the image, lower represents a darker image)\n",
    "    c. *Number of items* (lower represents fewer number of items in the image)\n",
    "3. Starting with a 5/255 brightness & a 5/50 items per image for dining room, we were able to retrieve an almost empty & dark image of a dining room. \n",
    "4. Increasing the number of items to 50 retrieves a dark image of a dining room but with chairs and a dining table.\n",
    "5. As you'll later see, we can also add a lot more filters such as an input image, image segmentation mask, image boundary, number of humans and more but we have limited ourselves to three for the purpose of this demo.\n",
    "\n",
    "With this understanding of the demo app, let's get started and build one ourselves! If you'd like to skip over all the details, python code for this app has been shared in @sec-code."
   ]
  },
  {
   "cell_type": "raw",
   "id": "93e0bb5d",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "Thank you **jarvislabs.ai** for the compute, this blog post would not have been possible without the credits. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a0d72",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372cf6b7",
   "metadata": {},
   "source": [
    "As part of this blog post, we are going to assume that the reader has a basic understanding of embeddings, Vision Language Models and image retreival using cosine similarity search. \n",
    "\n",
    "Some good resources to get the readers going are shared below:\n",
    "\n",
    "1. [Image Similarity with Hugging Face Datasets and Transformers](https://huggingface.co/blog/image-similarity) by Sayak Paul\n",
    "2. [The illustrated word2vec](https://jalammar.github.io/illustrated-word2vec/) by Jay Alammar\n",
    "3. [Vision Language Models Explained](https://huggingface.co/blog/vlms) by Merve Noyan & Edward Beeching "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1ec9f",
   "metadata": {},
   "source": [
    "# 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61d0a8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f402e64",
   "metadata": {},
   "source": [
    "As part of this blog post we will be utilising Apple's **4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities** @4m-21 paper to build a real-time search engine that is capable of using caption, brightness & number of items per image as filters to query an image database of a total of 15 images. Though this technique can easily be expanded to a million or more images. If you have a big database of images, take a look at [faiss](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) for similarity search.\n",
    "\n",
    "We will build an app using Gradio and also deploy it to Huggingface Hub for anyone to use.\n",
    "\n",
    "The 4M-21 paper is the second in the 4M series (Massively Multimodal Masked Modeling) by Apple, the first paper was also an any-to-any vision model capable of working with 7 modalities - [4M: Massively Multimodal Masked Modeling](https://arxiv.org/abs/2312.06647) @4m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26179b",
   "metadata": {},
   "source": [
    "![4M-21 any-to-any vision model](../images/4m-21.png){#fig-1 fig-align=\"center\" width=500}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac84c77",
   "metadata": {},
   "source": [
    "As shown in the image above, the model can work with with multiple modalities. It can take all modalities as inputs and output any or all of the modalities using single or subset of modalities! Unbelievable right? Not anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996fb25",
   "metadata": {},
   "source": [
    "See the conditional generation example below as shared in the paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186f91a",
   "metadata": {},
   "source": [
    "![One to all generation](../images/4m-21-one-to-all.png){#fig-9 fig-align=\"center\" width=500}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd5ce7",
   "metadata": {},
   "source": [
    "As part of this blog post we will focus more on retrieval rather than generation. But, the basic concepts remain the same. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b6bfbe5",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "The authors have open sourced all code here - [https://github.com/apple/ml-4m](https://github.com/apple/ml-4m).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b7f48",
   "metadata": {},
   "source": [
    "As part of this blog post we will focus more on retrieval rather than generation. But, the basic concepts remain the same. With that being said, let's get started with image retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8920eb",
   "metadata": {},
   "source": [
    "## Image retrieval using 4M-21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723912ad",
   "metadata": {},
   "source": [
    "![Different modes of multimodal retrieval](../images/4m-21-retreival.png){#fig-2 fig-align=\"center\" width=500}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a3bb9",
   "metadata": {},
   "source": [
    "As can be seen above, the authors showcased that the model is capable of using any or subet of 21 modalities as input to retrieve similar images from a database. The query consist of one or more modalities from @fig-1. As part of this blog post, we will be focusing on the \"caption + metadata -> RGB\" retrieval example. \n",
    "\n",
    "In @fig-2, given the inputs \"a fancy mansion\" and \"brightness 200/255\", the model was able to return very bright images of a mansion. Doing the same for \"brightness 30/255\" returns darker images of mansions. We will be replicating this functionality as part of this blog post."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f58e1793",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "@fig-2 above is what prompted the idea for this blog post. What if we can utilise some or all of the modalities to query our own custom databases?\n",
    "\n",
    "Thank you dear authors for answering all my [questions](https://github.com/apple/ml-4m/issues/2#issuecomment-2192932207).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed9763",
   "metadata": {},
   "source": [
    "We could have also added any of the input modalities from @fig-2 to our demo but we will leave this to the reader as an exercise to build on top of the code shared in this blog post. We have kept the input modalities limited to two as part of this blog post:\n",
    "\n",
    "1. Image description (caption)\n",
    "2. Metadata (such as brightness, number of items per image)\n",
    "\n",
    "As part of writing this blog post, we did [experiment with other modalities](https://github.com/apple/ml-4m/issues/9) such as:\n",
    "\n",
    "1. Input image (using an image to find similar images)\n",
    "2. Colour palette (using color palette to find similar images matching the colour schema)\n",
    "\n",
    "Please refer to @sec-appendixa for our findings on our custom database. Using color palette was not giving satisying results. We tried both `EPFL-VILAB/4M-21_XL` and `EPFL-VILAB/4M-21_L` models for the same. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e55ae4da",
   "metadata": {},
   "source": [
    "::: {.callout-caution collapse=\"true\"}\n",
    "\n",
    "## Expand to learn more about how to use color-palette and more metadata as inputs to the model\n",
    "\n",
    "In @sec-appendixa we will share with the reader how to extend the app to add color palette as an input the model on top of what has been shared in the demo. \n",
    "\n",
    "We also showcase to the reader how to extend the app to use other metadata such as:\n",
    "\n",
    "1. Crowdedness score: number of humans\n",
    "2. SAM clutter score: number of SAM instances\n",
    "3. COCO clutter score: number of COCO [55] instances\n",
    "4. COCO instance diversity: number of unique COCO instance classes\n",
    "5. Walkability score: % of pixels belonging to walkable COCO semantic classes such as ‘road’\n",
    "6. Semantic diversity: number of unique COCO semantic classes\n",
    "7. Caption length: length of the caption in characters, words, and sentences\n",
    "8. Geometric complexity: angular variance of surface normals \n",
    "9. Occlusion score: % of occlusion edges over a fixed threshold\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db63ccc",
   "metadata": {},
   "source": [
    "Having said that, let's dig deep into the paper and understand how this model is able to distill information from multiple modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836981f",
   "metadata": {},
   "source": [
    "As part of the training, each modality from @fig-1 was encoded using modality specific tokenizers. From the paper:\n",
    "\n",
    "*We employ suitable tokenization schemes for different modalities based on their format and performance. For image-like modalities and feature maps, we use spatial VQ-VAEs with optional diffusion decoders for detail rich modalities like RGB. For non-spatial modalities like global tokens or parameterized poses, we compress them to a fixed number of discrete tokens using Memcodes with MLP encoders and decoders. All sequence modalities are encoded as text using WordPiece.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fdd12",
   "metadata": {},
   "source": [
    "![Tokenization overview](../images/4m-21-tokenizer.png){#fig-3 fig-align=\"center\" width=500}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941e3b7",
   "metadata": {},
   "source": [
    "What this means is that authors were able to represent information into a limited number of tokens for multiple modalities. By training modality specific tokenizers, the authors were able to transform different modalities into a common representation. After converting all modalities to a common representation, the authors were able to train a standard encoder-decoder transformer. During training, random subsets of these tokens are selected from all modalities as inputs and targets, and the objective is to predict one subset from the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7b9a7",
   "metadata": {},
   "source": [
    "![Method overview](../images/4m-21-overview.png){#fig-4 fig-align=\"center\" width=500}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fb18d",
   "metadata": {},
   "source": [
    "The complete method overview was shared by the authors in the 4M: Massively Multimodal Masked Modeling @4m paper.\n",
    "\n",
    "As can be seen, here's what's exactly going on:\n",
    "\n",
    "1. First the different modalities are converted to a number of tokens using modality specific tokenizers \n",
    "2. A random subset of tokens are selected as input \n",
    "3. A random subset of tokens are selected as output \n",
    "\n",
    "By doing so, the model learns to take in all or a subset of input modalities and predicts all or a subset of output modalities thus it is termed an **\"any-to-any vision model\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c49e4",
   "metadata": {},
   "source": [
    "Now that we have a basic understanding of how the model works, let' start building the retrieval app in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fb0fd",
   "metadata": {},
   "source": [
    "## Python code for image retrieval {#sec-code}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312dd10",
   "metadata": {},
   "source": [
    "We will closely follow the [demo notebook](https://github.com/apple/ml-4m/blob/main/notebooks/generation_4M-21.ipynb) shared by the authors and build the retrieval sytem on top of it using a custom database (in this case a sample of 15 images). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2330be",
   "metadata": {},
   "source": [
    "As also mentioned in the paper,\n",
    "\n",
    "*Our model can also perform multimodal retrievals by predicting global embeddings of DINOv2 and ImageBind from any (subset) of the input modalities. Once the global embeddings are obtained, the retrieval is done by finding the retrieval set samples with the smallest cosine distance to the query.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573b43e",
   "metadata": {},
   "source": [
    "We can utilize either Imagebind or Dino-V2 to encode images as embeddings, as part of this demo we utilise DINOv2 global embeddings for retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328fabc",
   "metadata": {},
   "source": [
    "**ImageBind: One Embedding Space To Bind Them All** @imagebind and **DINOv2: Learning Robust Visual Features without Supervision** @dinov2 are both multi-modal vision models released previously by Meta. They are both capable of representing images to an embedding space. We donot dig deeper into these models as part of this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f864b",
   "metadata": {},
   "source": [
    "### Building the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d4a5c",
   "metadata": {},
   "source": [
    "Since we wanted to showcase image description, brightness and number of items, our database consists of 15 images downloaded manually using [google image search](https://images.google.com.au/). The complete database can be found - [here](https://huggingface.co/datasets/aroraaman/4m-21-demo)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0790de4",
   "metadata": {},
   "source": [
    "::: {.callout-caution collapse=\"true\"}\n",
    "\n",
    "## Expand to learn more about creating your own database\n",
    "\n",
    "Creating your own Huggingface dataset using an image folder is as simple as:\n",
    "\n",
    "```python \n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/data\")\n",
    "dataset.push_to_hub()\n",
    "```\n",
    "\n",
    "You can read more about it [here](https://huggingface.co/docs/datasets/en/image_dataset).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e05c9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"aroraaman/4m-21-demo\")\n",
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc75788",
   "metadata": {},
   "source": [
    "The dataset consists of a mix of dark and bright images of dining room and swimming pool. Some images contain lot of items and are cluttered while others look more \"empty\". Images are of type .png, .jpg, .webp we & .jpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be90f4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=852x1200>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x853>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=P size=1500x1284>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=564x846>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=736x552>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=275x183>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x168>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=194x259>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=275x183>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=616x462>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=605x694>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=612x408>,\n",
       " <PIL.Image.Image image mode=RGB size=635x272>,\n",
       " <PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=800x533>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2399x3229>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76459924",
   "metadata": {},
   "source": [
    "Now that we have a list of images that we want to use as our database, let's use DINOv2 to convert them to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc4cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d59bc",
   "metadata": {},
   "source": [
    "Now, let's load the DINOv2 model as our feature extractor. Speicifically we will be using the ViT-B14 version as mentioned in the @4m-21 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c5b25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "INFO:dinov2:using MLP layer as FFN\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "feature_extractor = feature_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c5189",
   "metadata": {},
   "source": [
    "We transform every image by downsizing each image such that the shortest side is of size 224 pixels. We then center crop the image such that all images are of size 224x224. We use [Albumentations library](https://albumentations.ai/docs/api_reference/augmentations/geometric/resize/) @albu for the transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd9147ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractionDataset(Dataset):\n",
    "    def __init__(self, feature_extractor: nn.Module, path: str, img_sz=224):\n",
    "        super().__init__()\n",
    "        self.feature_extractor=feature_extractor\n",
    "        self.path = Path(path)\n",
    "        self.files = list(self.path.rglob(\"*\"))\n",
    "        self.tfms = A.Compose([\n",
    "            A.SmallestMaxSize(img_sz),\n",
    "            A.CenterCrop(img_sz, img_sz)\n",
    "        ])\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def __len__(self): return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "        img = self.tfms(image=img)['image']\n",
    "        img = torch.tensor(img, dtype=torch.float32)/255.\n",
    "        img = img.permute(2,0,1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647d7b8",
   "metadata": {},
   "source": [
    "Next, we can simply build the dataset, dataloader and store the image embeddings as a PyTorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cce57",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create the Dataset\n",
    "dataset = FeatureExtractionDataset(\n",
    "    feature_extractor=feature_extractor, \n",
    "    path=\"/path/to/data\"\n",
    ")\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,  \n",
    "    shuffle=False,\n",
    "    num_workers=16,  \n",
    "    pin_memory=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b6e91",
   "metadata": {},
   "source": [
    "Finally we can extract the features from each image and store as a PyTorch Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bb471",
   "metadata": {},
   "source": [
    "```python\n",
    "features = []\n",
    "for i,batch in tqdm(enumerate(dataloader), total=(len(dataset)//batch_size)+1):\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        _f = feature_extractor(batch)\n",
    "    _f = _f.to(\"cpu\")\n",
    "    features.append(_f)\n",
    "    \n",
    "features = torch.concat(features)\n",
    "torch.save(features, \"./image_embeddings.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ca0f7",
   "metadata": {},
   "source": [
    "And that's it! We have successfully created our image database that we will retrieve similar images from based on a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a65e3",
   "metadata": {},
   "source": [
    "### Inference with 4M-21model `EPFL-VILAB/4M-21_L` to get most similar image {#sec-inference}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d2027",
   "metadata": {},
   "source": [
    "So now that we have the database, our next step is to actually be able to use inputs such as \"caption\", \"brightness\" and \"number of items\" to get an embedding that will be used as our \"query\". \n",
    "\n",
    "We will closely follow the [demo notebook](https://github.com/apple/ml-4m/blob/main/notebooks/generation_4M-21.ipynb) shared by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e69663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fourm.models.fm import FM\n",
    "from fourm.vq.vqvae import VQVAE\n",
    "from tokenizers import Tokenizer\n",
    "from fourm.models.generate import (\n",
    "    GenerationSampler,\n",
    "    build_chained_generation_schedules,\n",
    "    init_empty_target_modality,\n",
    "    custom_text,\n",
    ")\n",
    "from fourm.data.modality_info import MODALITY_INFO\n",
    "from fourm.utils.plotting_utils import decode_dict\n",
    "from fourm.vq.vqvae import VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037caafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_SIZE = 224\n",
    "TOKENIZER_PATH = \"./fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json\"\n",
    "FM_MODEL_PATH = \"EPFL-VILAB/4M-21_L\"\n",
    "IMAGE_DATASET_PATH = \"/home/ubuntu/GIT_REPOS/ml-4m/data/custom_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161f0ca",
   "metadata": {},
   "source": [
    "All tokenizers have been made available on the hub - [EPFL VILAB](https://huggingface.co/EPFL-VILAB). For our demo, we only need the text tokenizer, since we are using captions and metadata as inputs (both as text). We will also need to the fourm model to create the sampler that is able to create query embedding using input caption & metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bed193",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "fm_model = FM.from_pretrained(FM_MODEL_PATH).eval().to(DEVICE)\n",
    "sampler = GenerationSampler(fm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df1ef",
   "metadata": {},
   "source": [
    "Below, our input conditional domains are `caption` & `metadata`. And our target domain is `tok_dinov2_global`. As discussed in the paper, we want to obtain the global embeddings of DINOv2 using input modalities for retrieval."
   ]
  },
  {
   "cell_type": "raw",
   "id": "729d9e4e",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "The authors shared how to do multimodal retrieval in code [here](https://github.com/apple/ml-4m/issues/2#issuecomment-2194141824).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1055a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation configurations\n",
    "cond_domains = [\"caption\", \"metadata\"]\n",
    "target_domains = [\"tok_dinov2_global\",]\n",
    "tokens_per_target = [16]\n",
    "generation_config = {\n",
    "    \"autoregression_schemes\": [\"roar\"],\n",
    "    \"decoding_steps\": [1],\n",
    "    \"token_decoding_schedules\": [\"linear\"],\n",
    "    \"temps\": [2.0],\n",
    "    \"temp_schedules\": [\"onex:0.5:0.5\"],\n",
    "    \"cfg_scales\": [1.0],\n",
    "    \"cfg_schedules\": [\"constant\"],\n",
    "    \"cfg_grow_conditioning\": True,\n",
    "}\n",
    "top_p, top_k = 0.8, 0.0\n",
    "\n",
    "schedule = build_chained_generation_schedules(\n",
    "    cond_domains=cond_domains,\n",
    "    target_domains=target_domains,\n",
    "    tokens_per_target=tokens_per_target,\n",
    "    **generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f8de6",
   "metadata": {},
   "source": [
    "Now that we have a generation schedule to use `caption` and `metadata` as inputs to generate target `tok_dinov2_global`, we can create our dictionary of input and target modalities. let's initialise the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0517bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_sample = {}\n",
    "for target_mod, ntoks in zip(target_domains, tokens_per_target):\n",
    "    batched_sample = init_empty_target_modality(\n",
    "        batched_sample, MODALITY_INFO, target_mod, 1, ntoks, DEVICE\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b5eaf",
   "metadata": {},
   "source": [
    "Let's say that we want to retrieve a dark image of a swimming pool. So our input caption would be 'swimming pool', and metadata is passed in combination of V1 and V0s. \n",
    "\n",
    "V1 represents which metadata to pass in, the encoding for each metadata type is [here](https://github.com/apple/ml-4m/blob/777c0d2fb388fbd0f177375bf74d606c4ae7e9e1/fourm/data/modality_transforms.py#L876-L898).\n",
    "\n",
    "Brightness is encoded with number 10, and takes in range of values from 0-255. 0 represents a dark image whereas 255 represents a bright image. So to represent a brightness of 50/255, we will write `V1=10 V0=50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2500ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"Swimming pool\"\n",
    "metadata = \"v1=10 v0=68\" #brightness 68/255 as metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac1a97",
   "metadata": {},
   "source": [
    "Let's create the required dictionaries by the model as input using `custom_text` method as in the demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c59c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_sample = custom_text(\n",
    "    batched_sample,\n",
    "    input_text=caption,\n",
    "    eos_token=\"[EOS]\",\n",
    "    key=\"caption\",\n",
    "    device=DEVICE,\n",
    "    text_tokenizer=text_tokenizer,\n",
    ")\n",
    "batched_sample = custom_text(\n",
    "    batched_sample,\n",
    "    input_text=metadata,\n",
    "    eos_token=\"[EOS]\",\n",
    "    key=\"metadata\",\n",
    "    device=DEVICE,\n",
    "    text_tokenizer=text_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cbca3",
   "metadata": {},
   "source": [
    "Now, we can utilise the `sampler` that we created before to get the output from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "796c4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "out_dict = sampler.generate(\n",
    "    batched_sample,\n",
    "    schedule,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    verbose=True,\n",
    "    seed=0,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3cc02",
   "metadata": {},
   "source": [
    "This output dictionary consists of `tok_dinov2_global` as key and the `tensor` represents the token IDs that make up the representation of the DINOv2 global embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "696d3d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5426, 6424, 5294, 5716,  189, 4065, 7631, 8145, 3108, 7638, 4331, 7005,\n",
       "         5675, 1472, 3069, 5687]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict['tok_dinov2_global']['tensor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e50d3f",
   "metadata": {},
   "source": [
    "Let's now use the decoder to get a 768 representation embedding for the image that becomes our \"query\" for retrieval purposes.  To decode the tokens to the respective embedding, we will need to load the necessary VQ-VAE as well that was used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68ff0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VQVAE_PATH = \"EPFL-VILAB/4M_tokenizers_DINOv2-B14-global_8k_16_224\"\n",
    "vqvae = VQVAE.from_pretrained(VQVAE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2721cde",
   "metadata": {},
   "source": [
    "Let's now get the image embeddings using `decode_dict` as in the demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "079b8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dec_dict = decode_dict(\n",
    "        out_dict, {\"tok_dinov2_global\": vqvae.to(DEVICE)}, text_tokenizer, \n",
    "        image_size=IMG_SIZE, patch_size=16, decoding_steps=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c8c888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_dict[\"tok_dinov2_global\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c211913",
   "metadata": {},
   "source": [
    "As can be seen we have an embedding of size 768 which is our query embedding. Using cosine similarity, we can retrieve the most similar embedding from our image database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286fdc0",
   "metadata": {},
   "source": [
    "![Image retrieval using EPFL-VILAB/4M-21_L for \"swimming pool\" and 68/255 brightness](../images/4m-21-query.png){#fig-5 fig-align=\"center\" width=500}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cf239",
   "metadata": {},
   "source": [
    "As can be seen, the model sucessfully returns the image of a swimming pool for low brightness. If we increased the brightness to 255/255 we get the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65676f2",
   "metadata": {},
   "source": [
    "![Image retrieval using EPFL-VILAB/4M-21_L for \"swimming pool\" and 255/255 brightness](../images/4m-21-query-2.png){#fig-6 fig-align=\"center\" width=500}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb64da",
   "metadata": {},
   "source": [
    "### Gradio app with required filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5917c6a",
   "metadata": {},
   "source": [
    "Now that we have all the underlying code, we can simply build a Gradio interface for the same! Why? This makes it very easy for all to use and play with the 4M-21 model. Feel free to create your own apps too. If you do, please don't forget to let me know about it on my Twitter - https://x.com/amaarora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24281df1",
   "metadata": {},
   "source": [
    "The code for the gradio app is pretty simple, I actually used Claude 3.5 Sonnet to help me build the app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa0895",
   "metadata": {},
   "source": [
    "```python\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Image Retrieval using 4M-21: An Any-to-Any Vision Model\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            caption = gr.Textbox(\n",
    "                label=\"Caption Description\", placeholder=\"Enter image description...\"\n",
    "            )\n",
    "            brightness = gr.Slider(\n",
    "                minimum=0, maximum=255, value=5, step=1, \n",
    "                label=\"Brightness\", info=\"Adjust image brightness (0-255)\"\n",
    "            )\n",
    "            num_items = gr.Slider(\n",
    "                minimum=0, maximum=50, value=5, step=1, \n",
    "                label=\"Number of Items\", info=\"Number of COCO instances in image (0-50)\"\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            output_images = gr.Gallery(\n",
    "                label=\"Retrieved Images\",\n",
    "                show_label=True,\n",
    "                elem_id=\"gallery\",\n",
    "                columns=2,\n",
    "                rows=2,\n",
    "                height=512,\n",
    "            )\n",
    "    submit_btn = gr.Button(\"Retrieve Most Similar Image\")\n",
    "    submit_btn.click(\n",
    "        fn=get_similar_images,\n",
    "        inputs=[caption, brightness, num_items],\n",
    "        outputs=output_images,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26259c70",
   "metadata": {},
   "source": [
    "Using above code, allows us to create the Gradio app that was shared in @fig-demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c167868",
   "metadata": {},
   "source": [
    "![caption & metadata retrieval using EPFL-VILAB/4M-21_L](../images/4m-21-demo.png){#fig-7 fig-align=\"center\" width=500}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141af0e",
   "metadata": {},
   "source": [
    "### Deploy app to HuggingFace hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ea24b",
   "metadata": {},
   "source": [
    "We have deployed the app sucessfully to huggingface Spaces. We followed the documentation [here](https://huggingface.co/docs/hub/en/spaces-overview).\n",
    "\n",
    "**You can find the huggingface space [here](https://huggingface.co/spaces/aroraaman/image-retrieval-using-apple-4M-21).**\n",
    "\n",
    "Somem minor changes that we had to do between local and for the app to deployed on huggingface spaces:\n",
    "\n",
    "1. All binary files had to be tracked by git-lfs. Read more about it [here](https://github.com/git-lfs/git-lfs/blob/main/README.md)\n",
    "2. Convert dataset to a huggingface dataset, as we were not able to upload `.jpg`, `.png` or other files\n",
    "3. The complete source code for the gradio app that works on HF spaces can be found [here](https://huggingface.co/spaces/aroraaman/image-retrieval-using-apple-4M-21/blob/main/app.py).\n",
    "\n",
    "Overall it was pretty straightforward and easy to deploy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78007e",
   "metadata": {},
   "source": [
    "## Appendix A {#sec-appendixa}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51991f0",
   "metadata": {},
   "source": [
    "We used **EPFL-VILAB/4M-21_L** for all our experiments and image retrieval due to memory constraints. We found **EPFL-VILAB/4M-21_XL** requires around 28GB of VRAM along with respective tokenizers, and runtimes were slow on a A100 40GB instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f925543",
   "metadata": {},
   "source": [
    "![rgb-to-any retrieval using EPFL-VILAB/4M-21_L](../images/4m-finding-1.png){#fig-20 fig-align=\"center\" width=500}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec1faa",
   "metadata": {},
   "source": [
    "### Adding color palette as inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a86b1",
   "metadata": {},
   "source": [
    "From the paper:\n",
    "\n",
    "*For every RGB image, we extract between one and seven color palettes using [PyPalette](https://github.com/adamgrieger/pypalette). During training, we randomly sample one of the color palettes to enable users to input palettes with different levels of granularity.*\n",
    "\n",
    "*color palette sequence is formed as color = c R = r G = g B = b R = r, ... where c takes a value between 1 and 7 and specifies the number of colors in the palette and r, g, b takes values between 0-255.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046a482",
   "metadata": {},
   "source": [
    "We can write a small python function to convert any of seaborn [color palettes](https://seaborn.pydata.org/tutorial/color_palettes.html) to the required format. Also, as per the [color palette transform](https://github.com/apple/ml-4m/blob/777c0d2fb388fbd0f177375bf74d606c4ae7e9e1/fourm/data/modality_transforms.py#L1180-L1185), the tokenizer expexts \"color\" to be replaced by \"v1\" and r,g,b with \"v0\".\n",
    "\n",
    "Therefore, a color palette represented by `color=1 r=166 g=206 b=227` should be transformed to `v1=1 v0=166 v0=206 v0=227`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7298bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70d5de32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg  width=\"110\" height=\"55\"><rect x=\"0\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#a6cee3;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"55\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#1f78b4;stroke-width:2;stroke:rgb(255,255,255)\"/></svg>"
      ],
      "text/plain": [
       "[(0.6509803921568628, 0.807843137254902, 0.8901960784313725),\n",
       " (0.12156862745098039, 0.47058823529411764, 0.7058823529411765)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_color_palette(num_colors=2, palette_name=\"Paired\"):\n",
    "    palette = sns.color_palette(palette_name, num_colors)\n",
    "    rgb_values = [(int(r*255), int(g*255), int(b*255)) for r, g, b in palette]\n",
    "    color_strings = [f\"v0={r} v0={g} v0={b}\" for r, g, b in rgb_values]\n",
    "    color_palette = f\"v1={num_colors} \" + \" \".join(color_strings)\n",
    "    return palette, color_palette\n",
    "\n",
    "palette, color_palette_string = generate_color_palette(num_colors=2)\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4deb7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v1=2 v0=166 v0=206 v0=227 v0=31 v0=120 v0=180'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_palette_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c819a",
   "metadata": {},
   "source": [
    "Now we can simply pass in the string above as input and use `custom_text` function on our `batched_sample` to prepare batch for input to the model. We also need to add `color_palette` to the conditional domain as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ebb83",
   "metadata": {},
   "source": [
    "```python\n",
    "cond_domains = [\"caption\", \"metadata\", \"color_palette\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcbdd3",
   "metadata": {},
   "source": [
    "Once that's done, we can now take our `color_palette_string` as input and created the `batched_sample` as before in @sec-inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece5466",
   "metadata": {},
   "source": [
    "```python\n",
    "batched_sample = custom_text(\n",
    "    batched_sample,\n",
    "    input_text=caption,\n",
    "    eos_token=\"[EOS]\",\n",
    "    key=\"caption\",\n",
    "    device=DEVICE,\n",
    "    text_tokenizer=text_tokenizer,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9c85a",
   "metadata": {},
   "source": [
    "And that's it! Everything else remains the same! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955f47f",
   "metadata": {},
   "source": [
    "### Adding more metadata as input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb4642",
   "metadata": {},
   "source": [
    "In the demo application, we only utilised brightness and number of items as metadata inputs. But as descriped in the paper, we could have used many more metadata as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3380a",
   "metadata": {},
   "source": [
    "To pass in any of the metadata available [here](https://github.com/apple/ml-4m/blob/777c0d2fb388fbd0f177375bf74d606c4ae7e9e1/fourm/data/modality_transforms.py#L877-L897), just pass in `v1=[key] v0=[val]` to the input string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42830c9",
   "metadata": {},
   "source": [
    "For example, to add in metadata: \"brightness 50/255 contrast 50/127 walkability 25/50\", simply write it as:\n",
    "\n",
    "```v1=10 v0=50 v1=11 v0=50 v1=14 v0=25```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85202ff0",
   "metadata": {},
   "source": [
    "We simply replace the words by their corresponding metadata key and add the value with `v0=[val]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7641fcd",
   "metadata": {},
   "source": [
    "And that's it! Now the reader can also add any of the 20 metadata filters that the authors have trained the 4M-21 model on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f44b1d",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef3cc1",
   "metadata": {},
   "source": [
    "As part of this blog post, we looked into the 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities @4m-21 paper and built an image retriever app on top as a real world application.\n",
    "\n",
    "In @sec-code, we also looked at the Python code to be able to build such an app on any custom database. We built a gradio app for demo purpose and also deployed it to Huggingface Spaces!\n",
    "\n",
    "All code and corresponding files can be found [here](https://huggingface.co/spaces/aroraaman/image-retrieval-using-apple-4M-21/tree/main).\n",
    "\n",
    "Finally in @sec-appendixa, we looked at ways of extending the demo and adding color palettes and more metadata as input filters for retrieval!\n",
    "\n",
    "Thank you readers for your time. If you have any feedback, please feel free to share it with me [here](https://amaarora.github.io/about.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
