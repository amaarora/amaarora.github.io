{
 "cells": [
  {
   "cell_type": "raw",
   "id": "29b40b6e",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sliding Window Attention\n",
    "subtitle:  Longformer - The Long-Document Transformer \n",
    "description: | \n",
    "    In this post, we take a deep dive into Sliding Window Attention that allowed transformers to have long context length. We do this with the help of animations and also implement it from scrath in PyTorch code.\n",
    "categories:\n",
    "    - LLM\n",
    "author: Aman Arora\n",
    "date: \"07/04/2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "title-block-banner: true\n",
    "bibliography: ../references.bib\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "code-fold: false\n",
    "image: ../images/swa.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e819be1",
   "metadata": {},
   "source": [
    "As part of this blog post, we will look take a deep dive into **Sliding Window Attention (SWA)** that was introduced as part of the Longformer architecture (@longformer), and also understand how it's implemented in PyTorch!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9ce50",
   "metadata": {},
   "source": [
    "When I first started looking into sliding window attention, below tweet kind of summarises my journey. O thought it's pretty complicated and hard to implement. But, as is usual with many things, the more time you spend on it, the easier it gets. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "34fafe7b",
   "metadata": {},
   "source": [
    "<blockquote class=\"twitter-tweet tw-align-center\"><p lang=\"en\" dir=\"ltr\">OMG! &quot;Sliding Window Attention&quot; is seriously a wild concept to wrap your head around! ðŸ¤¯<a href=\"https://t.co/mCVhqS4Fn4\">https://t.co/mCVhqS4Fn4</a> <a href=\"https://t.co/UQNtxLUxSY\">pic.twitter.com/UQNtxLUxSY</a></p>&mdash; Aman Arora (@amaarora) <a href=\"https://twitter.com/amaarora/status/1808494422260437042?ref_src=twsrc%5Etfw\">July 3, 2024</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11394e08",
   "metadata": {},
   "source": [
    "Having spent some time on digging through the [LongerFormer implementation in Huggingface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/longformer/modeling_longformer.py#L488), I have realised that it's really not that hard. But, first, let's understand what sliding window attention really is and how it's different from full-attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20966b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1503f8",
   "metadata": {},
   "source": [
    "![Comparing the full self-attention pattern and the configuration of attention patterns in Longformer](../images/swa.png){#fig-1 fig-align=\"center\" width=500}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022c5d9",
   "metadata": {},
   "source": [
    "The above image from the Longformer paper (@longformer), summarises the difference between Full $n^2$ attention & Sliding window attention.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb876a90",
   "metadata": {},
   "source": [
    "In the traditional sense, \n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Each token in the Query vector $Q$ can attend to all tokens in the Key vector $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e31dc",
   "metadata": {},
   "source": [
    "But, this leads to a computational complexity of $O(n^2)$. As a result, memory requirements grow by a factor of $n^2$ for a sequence of length $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f584282",
   "metadata": {},
   "source": [
    "This limits the traditional Transformer architecture from having long context length. The solution is to use Sliding window attention where each token in the Query vector $Q$ only attends to it's neighbouring tokens with an overlap of window length $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf2620",
   "metadata": {},
   "source": [
    "So, a token at position $i$ in $Q$, can attend to tokens in range $(i-w, i+w)$ in $K$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8445b4",
   "metadata": {},
   "source": [
    "## Matrix multiplication using `torch.einsum` {#sec-matmul}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d222f70e",
   "metadata": {},
   "source": [
    "::: {#fig-matmul}\n",
    "\n",
    "{{< video ../images/matrix-mul.mp4 width=\"300\" >}}\n",
    "\n",
    "Visualisation of matrix multiplication from http://matrixmultiplication.xyz/\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf030a4",
   "metadata": {},
   "source": [
    "Before we get started with Sliding Window Attention, let's implement $Q.K^T$ matrix multiplication with the help of `torch.einsum`. \n",
    "\n",
    "For a refresher/introduction to matrix multiplication and [torch.einsum](https://pytorch.org/docs/stable/generated/torch.einsum.html), I recommend the below amazing lecture by [Jeremy Howard](https://x.com/jeremyphoward)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "11bc6e26",
   "metadata": {},
   "source": [
    "{{< video https://youtu.be/_xIzPbCgutY start=\"652\" width=\"300\">}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01e414",
   "metadata": {},
   "source": [
    "To implement, $Q.K^T$ using Einstein summation is as easy as doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ab7d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "q = torch.arange(1, 9).reshape(4,2)\n",
    "k = torch.arange(1, 9).reshape(4,2)\n",
    "out = torch.einsum('xd,yd->xy', q, k)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fe7339a",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## More on `torch.einsum`\n",
    "\n",
    "I would recommend the readers to play around with `torch.einsum` notation, try writing simple matrix multiplications and see the results for yourself to get an intuition. \n",
    "\n",
    "```python\n",
    "x = torch.tensor([7,6,5,4]).unsqueeze(1)\n",
    "y = torch.arange(start=1, end=5).reshape(1,4)\n",
    "torch.einsum(\"ij, jk\", x,y)\n",
    "```\n",
    "\n",
    "As for why `torch.einsum('xd,yd->xy', q, k)` represents $Q.K^T$, here's a detailed explanation: \n",
    "\n",
    "- \"xd, yd -> xy\" specifies the operation: x and y represent the outer dimensions (4x4) & d represents the inner dimension for multiplication (2, in this case)\n",
    "- The result is a 4x4 tensor where each element is the dot product of a row from $q$ with a column from $k$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132cbe6",
   "metadata": {},
   "source": [
    "Before moving on the next section, I would recommend that the readers make sure that they can correlate below outputs with @fig-matmul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c58682b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4],\n",
       "         [5, 6],\n",
       "         [7, 8]]),\n",
       " tensor([[1, 3, 5, 7],\n",
       "         [2, 4, 6, 8]]),\n",
       " tensor([[  5,  11,  17,  23],\n",
       "         [ 11,  25,  39,  53],\n",
       "         [ 17,  39,  61,  83],\n",
       "         [ 23,  53,  83, 113]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k.T,out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797eca3",
   "metadata": {},
   "source": [
    "Great, now that we know what Sliding Window Attention is, and how to use einstum summation to do matrix multiplication, we are ready to see how Sliding Window Attention can be implemented in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53012cbf",
   "metadata": {},
   "source": [
    "## Sliding window attention in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacbfb9",
   "metadata": {},
   "source": [
    "From Appendix A of the [LongFormer paper](https://arxiv.org/abs/2004.05150) (implementation detail, text slightly modified to match implementation):\n",
    "\n",
    "*Longformer-chunks only supports the nondilated case. It chunks Q and K into overlapping blocks of size $2*w$ and overlap of size $w$, multiplies the blocks, then mask out the diagonals. This is very compute efficient because it uses a single matrix multiplication operation from PyTorch, but it consumes $2x$ the amount of memory a perfectly optimized implementation should consume because it computes some of the zero values. Because of the compute efficiency, this implementation is most suitable for the pretrain/finetune case. We didnâ€™t find the increase in memory to be a problem for this setting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9cdaa",
   "metadata": {},
   "source": [
    "> To explain further, to achieve the same results as @fig-1 (b), it is possible to divide the Query $Q$ and Key $K$ vectors to chunks of size $2*w$, where $w$ represents the window length or the overlap size. Then, we can perform the attention operation and get scores by doing $Q.K^T$ within the chunks themselves! This way, it's very efficient as it only involves a single matrix multiplication operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e78b8d",
   "metadata": {},
   "source": [
    "Let's see how the above translates to PyTorch code. Let's define a query $q$ and a key $k$ vector of batch size 1, sequence length 8 and embedding size 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dafe356",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(1, 8, 768)\n",
    "k = torch.randn(1, 8, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c3c39",
   "metadata": {},
   "source": [
    "Let's assume a query and key vector of batch size 1, sequence length 8 and embedding size of 768. These can be converted to overlapping chunks using the `_chunk` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6f45b",
   "metadata": {},
   "source": [
    "![Chunking overview](../images/chunks.png){#fig-3 fig-align=\"center\" width=300}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156ec2b",
   "metadata": {},
   "source": [
    "Given a reference image above, in PyTorch implementation, we don't really need to create three separate vectors, but instead we can create one called `overlapping_chunks` with the right shape and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a618e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunk(hidden_states, window_overlap):\n",
    "    \"\"\"convert into overlapping chunks. Chunk size = 2w, overlap = w\"\"\"\n",
    "    chunk_size = [\n",
    "        hidden_states.size(0), #bs\n",
    "        torch.div(hidden_states.size(1), window_overlap, rounding_mode=\"trunc\") - 1, #n_chunks\n",
    "        window_overlap * 2,\n",
    "        hidden_states.size(2),\n",
    "    ]\n",
    "\n",
    "    overlapping_chunks = torch.empty(chunk_size, device=hidden_states.device)\n",
    "    for chunk in range(chunk_size[1]):\n",
    "        overlapping_chunks[:, chunk, :, :] = hidden_states[\n",
    "            :, chunk * window_overlap : chunk * window_overlap + 2 * window_overlap, :\n",
    "        ]\n",
    "    return overlapping_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89551491",
   "metadata": {},
   "source": [
    "Let's check the key & query shapes after chunking. In total we have 3 chunks, where the chunk size is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e4287e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4, 768]), torch.Size([1, 3, 4, 768]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = _chunk(q, window_overlap=2)\n",
    "key   = _chunk(k, window_overlap=2)\n",
    "query.shape, key.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7256d4e",
   "metadata": {},
   "source": [
    "Finally, we can now perform sliding window attention using `torch.einsum`. This is where the matrix multiplication of between query $Q$ and key (transposed) $K^T$ occurs using `torch.einsum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13655af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagonal_chunked_attention_scores = torch.einsum(\"bcxd,bcyd->bcxy\", (query, key)) \n",
    "diagonal_chunked_attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225918eb",
   "metadata": {},
   "source": [
    "By performing matrix multiplication $Q.K^T$ within chunks, we have succesfully replicated @fig-1 (b) in PyTorch. Had we not created any chunks, and done our matmul operation on all of $Q$ and $K^T$, it would have been equivalent to @fig-1 (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93855d73",
   "metadata": {},
   "source": [
    "And that's really it! This is all the magic behind Sliding Window Attention from the Longformer architecture. (@longformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945b7da",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77219dd3",
   "metadata": {},
   "source": [
    "As part of this blog post, we first looked at the difference full-attention with complexity $O(n^2)$ and sliding window attention. @fig-1\n",
    "\n",
    "Next, we learnt how to easily perform $Q.K^T$ using `torch.einsum`. Finally, we saw that by converting Query $Q$ and Key $K$ to chunks, we can easily implement sliding window attention in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996ed83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
