# Top 100 solution - SIIM-ACR Pneumothorax Segmentation

1. TOC 
{:toc}

## Introduction
This week I spent most of my time implementing a solution for [SIIM-ACR Pneumothorax Segmentation](kaggle.com/c/siim-acr-pneumothorax-segmentation/submissions) Kaggle competition and in today's blog post, we will looking at how to work on Image Segmentation based problems in Pytorch with this competition serving as a useful example. 

This blog post assumes the reader to have some idea about [**Image Segmentation**](https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/), [**U-Net**](https://arxiv.org/abs/1505.04597) and [**PyTorch**](https://pytorch.org/). 

This post is purefuly code-focused with an aim to provide a reporoducible training script with explainations that could then be used for other projects or Kaggle competitions. We train the model using pure PyTorch.

For a complete working notebook to follow along step-by-step refer [here](https://github.com/amaarora/amaarora.github.io/blob/master/nbs/Training.ipynb).

First, we download the dataset from Kaggle and convert the `.dcm` files to `.png` images for both masks and radiography images. Next, we create a PyTorch Dataset that returns the image and mask as a dictionary. Finally, we create the model using the wonderful [Pytorch Image Segmentation library](https://github.com/qubvel/segmentation_models.pytorch) by Pavel Yakubovskiy(https://github.com/qubvel). Once we have the model, we create our training and validation scripts and train it using `ComboLoss` that was also used by the competition winner - [Anuar Aimoldin](https://www.linkedin.com/in/anuar-aimoldin/). Winning solution can be referenced [here](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/107824).


While I found lots of blog posts showing Image Segmentation using the wonderful [fastai library](https://docs.fast.ai/), there were fewer blog posts that attempted to do this task using pure Pytorch. I try the latter, because I have done the former many times and want to understand the minor details of training an Image Segmentation model in PyTorch. While it was much harder than training a model in fastai, getting stuck in problems helped me further expand my understanding on Image Segmentation. 

Using pure PyTorch and some help from existing scripts, I was able to create a working solution that then landed in top-100 position on the private leaderboard with a dice score of 0.8421. 

With that being said, let's get started and look at the code pieces required to create such a solution step-by-step. 

## Problem Statement

### What is Image Segmentation?
Before we look at what we are trying to solve in SIIM-ACR Competition, let's first look at Image Segmentation. Image segmentation can also be thought of as pixel-wise classification where, for each pixel in the input image, we are predicting whether it is a part of the segmentation mask or not. We predict `1` if we think the pixel is a part of the segmentation mask and `0` otherwise. 

![](/images/acr_intro.png "fig-1 Eye Segmentation Model")

Let's say we have a 8x8 image, therefore as shown in `fig-1`, the idea is to predict a label - 0 or 1 for each pixel in the input image. This is precisely what Image Segmentation is. 

### SIIM-ACR Overview
For the **SIIM-ACR Competition**, the competitors were asked to build an image-segmentation model that can classify and segment pneumothorax. From the [description on Kaggle](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/overview) for **SIIM-ACR Competition**:

> In this competition, you’ll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images.

Specfically, given a chest radiographic image (`fig-2` left), the idea is to predict a mask that shows the position of pneumothorax in an image (`fig-2` right).

![](/images/acr_intro.png "fig-2 Sample Image and Pneumothorax Segmentation Mask")


## Preparing the Dataset
The first step is almost always getting the data ready for Image Segmentation. In the competition, the data has been provided as `.dcm` files. 

[DICOM](https://en.wikipedia.org/wiki/DICOM) is standard image format for medical-imaging information and related data. It consists of the image and associated metadata of the patient such as sex, age etc as well. 


### Downloading the `.dcm` files and converting to `.png`
The first step therefore, would be to extract `.png` files from `.dcm` files provided in this competition. 

This task has been made pretty simple with `pydicom` library. Pseudo-code for this task for a single `.dcm` file looks like: 

```
# read dcm file
image_and_meta_data = pydicom.dcmread('<path_to_dcm_file>')
# get image data
image = image_and_meta_data.pixel_array
# resize image if needed
image = resize(image, height=target_height, width=target_width)
# write to `.png` file
cv2.imwrite('<path_to_target_fname.png>', image)
```

If you're planning on following along with this blog post, the following lines of code will download and prepare the data for you (make sure you're Kaggle API works): 

```bash
mkdir siim-acr && cd siim-acr 
mkdir data && cd data
kaggle datasets download -d seesee/siim-train-test
unzip siim-train-test.zip 
mv siim/* . 
rmdir siim
mkdir ../src/ && cd ../src
git clone https://github.com/sneddy/pneumothorax-segmentation
python pneumothorax-segmentation/unet_pipeline/utils/prepare_png.py -train_path ../data/dicom-images-train/ -test_path ../data/dicom-images-test/ -out_path ../data/dataset512 -img_size 512 -rle_path ../data/train-rle.csv
```

Running the above, should create a folder `siim-acr` in your current directory that has the following structure: 

```
├── data
│   ├── dataset512
│   ├── dicom-images-test
│   └── dicom-images-train
└── src
    └── pneumothorax-segmentation
```

The `dataset512` now consists of the `train`, `mask` and `test` files in `.png` format.

We made use of [Anuar Aimoldin's](https://www.kaggle.com/sneddy) - winner of [SIIM-ACR Pneumothorax Segmentation](kaggle.com/c/siim-acr-pneumothorax-segmentation/submissions) - `prepare_png.py` script to convert dicom files to `.png` image files. 

In essence, the script first globs the `train_path` directory provided in the command as arg to get a list of all train files in `.dcm` format. Next, it iterates over each of them and saves the `.png` file (by extracting `.png` image as in the pseudo) to `train` directory. Same steps are followed for test files. For masks, we get the labels from the `train-rle.csv` file provided by Kaggle and save masks as `png` files too. 

### PyTorch Dataset
Now that all our files are ready in `.png` format, we are ready to create the `Dataset` class as below.

```python
class Dataset():
    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None):
        self.df             = rle_df
        self.image_base_dir = image_base_dir
        self.masks_base_dir = masks_base_dir
        self.image_ids      = rle_df.ImageId.values
        self.augmentation   = augmentation
    
    def __getitem__(self, i):
        image_id  = self.image_ids[i]
        img_path  = os.path.join(self.image_base_dir, image_id+'.png') 
        mask_path = os.path.join(self.masks_base_dir, image_id+'.png')
        image     = cv2.imread(img_path, 1)
        mask      = cv2.imread(mask_path, 0)     
        
        # apply augmentations
        if self.augmentation:
            sample = {"image": image, "mask": mask}
            sample = self.augmentation(**sample)
            image, mask = sample['image'], sample['mask']

        return {
            'image': image, 
            'mask' : mask
        }
        
    def __len__(self):
        return len(self.image_ids)
```

The above `Dataset` class is pretty lean and basic. It reads a DataFrame object similar to the train dataframe provided in the competition which consists of `ImageId` column that get's stores in `self.image_ids` attribute. Everytime, we get a new item, the `__getitem__`, first get's an image id, then creates the path by combining th base dir with the image id and adding a `.png` suffix. Finally, it also returns the mask based on the image id. 

Through this class, if we have divided the given DataFrame `train-rle.csv` into train and val splits then we can create `train` and `val` datasets by passing these dataframes in. 

### Five-fold splits
Now that we have the dataset class ready, we can perform the train-val split. For this competition, we will doing a five-fold split using `StratifiedKFold` from `sklearn`.

```python
RLE_DF = pd.read_csv('<path_to_train_rle.csv'>)
kf = StratifiedKFold()
 
if CREATE_FIVE_FOLDS:
    RLE_DF['has_mask'] = 0
    RLE_DF.loc[RLE_DF.EncodedPixels!='-1', 'has_mask'] = 1
    RLE_DF['kfold']=-1
    for fold, (train_index, test_index) in enumerate(kf.split(X=RLE_DF.ImageId, y=RLE_DF.has_mask)):
            RLE_DF.loc[test_index, 'kfold'] = fold
    RLE_DF.to_csv('<path_to_target_file.csv>', index=False)
```

Once we have the five-fold splits, we can get `train` and `val` data using fold id like so:

```python
TRAIN_DF = RLE_DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)
VAL_DF   = RLE_DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)
len(TRAIN_DF), len(VAL_DF)

>> (10364, 2590)
```

### Train and Val Datasets and Dataloaders
Now that we have our train and validation dataframes, creating train and val datasets and dataloaders is a piece of cake. 

```python 
TRAIN_BATCH_SIZE = 8 
VALID_BATCH_SIZE = 16

# datasets
train_dataset = Dataset(TRAIN_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>') 
val_dataset   = Dataset(VAL_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>')
# dataloaders
train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, 
                              shuffle=True, num_workers=4)
val_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=4)
```

### Train and Valid Augmentations
We could also add image augmentations by simply passing augmentations to the `Dataset` class. 

Let's first define train and validation augmentations using `albumentations` library. 

```python
# Train transforms
TRN_TFMS = albu.Compose([
    albu.HorizontalFlip(),
    albu.Rotate(10),
    albu.Normalize(),
    ToTensor(),
])
​
# Test transforms
VAL_TFMS = albu.Compose([
    albu.Normalize(),
    ToTensor(),
])
```

And now, we can create the datasets and dataloaders with augmentations like so: 

```python 
TRAIN_BATCH_SIZE = 8 
VALID_BATCH_SIZE = 16

# datasets
train_dataset = Dataset(TRAIN_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>', augmentation=TRN_TFMS) 
val_dataset   = Dataset(VAL_DF, '<path_to_train_png_image_dir>', '<path_to_train_png_masks_dir>', augmentation=VAL_TFMS)
# dataloaders
train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, 
                              shuffle=True, num_workers=4)
val_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=4)
```

So far so good, now we have our train and validation dataset and dataloaders ready. 

### Visualize 

Let's visualize some images now. First, we create simple helper functions from `torchvision` examples like so: 

```python
def matplotlib_imshow(img, one_channel=False):
    fig,ax = plt.subplots(figsize=(10,6))
    ax.imshow(img.permute(1,2,0).numpy())

def visualize(**images):
    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy 
    n = len(images)
    plt.figure(figsize=(16, 8))
    image, mask = images['image'], images['mask']
    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)
    if mask.max()>0:
        plt.imshow(mask.squeeze(0), alpha=0.25)
    plt.show()
```

And now visualizing an image with mask is as simple as:

```python
# plot one image with mask 
visualize(**train_dataset[1])
```

![](/images/acr_img_mask.png "fig-2 Example Image with Segmentation Mask")

And to visualize multiple train images, we could do something like: 

```python
images, masks = next(iter(train_dataloader))['image'], next(iter(train_dataloader))['mask']
img_grid = torchvision.utils.make_grid(images[:9], nrow=3, normalize=True)
matplotlib_imshow(img_grid)
```

![](/images/acr_img_grid.png "fig-3 Train image grid")


As can be seen from `fig-3`, some images are rotated, some are flipped - this due to Image Augmentations. This way, our model sees a slightly different version of the same image every time in an epoch and will therefore be able to generalize better. 

Now, that our dataset and dataloaders with image augmentations are done, we are ready to look into model building, training and validation. 

## Model - Training and Validation

To create our Unet based model, we will be using the wonderful **Segmentation Models** library in PyTorch by [Pavel Yakubovskiy](https://github.com/qubvel). I promise to do a future blog post on creating a Unet structure from scratch with a ResNet encoder branch, but for simplicity, for this introductory post, using the **Segmentation Models** library is a much easier option. If you're keen to create a Unet architecture from scratch, [here](https://www.youtube.com/watch?v=u1loyDCoGbE) is an excellent video by Abhishek Thakur. 

Thanks to the library, creating our model architecture is as simple as: 

```python
model = smp.Unet(
    encoder_name='se_resnext50_32x4d', 
    encoder_weights='imagenet', 
    classes=1, 
    activation=None,
)
```

Usually, there is a `Sigmoid` activation after the Unet, but in our case, we do not add any activation because the `Sigmoid` activation is already a part of the loss function that we use to train the model.

Given an input batch of images of size `(8, 3, 512, 512)` the model outputs masks of batch dimensions `(8, 1, 512, 512)`.

Usually, one of the common loss functions for Image Segmentation is pixel wise Cross Entropy but for our introductory post today we will be using the same loss function as the winner of the competition. 

### Loss Function 

```python
import sys; sys.path.append('<path to `pneumothorax-segmentation/unet_pipeline/`>')
from Losses import ComboLoss

criterion = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})
```

This loss function is a combination of Binary Cross Entropy, Dice Loss and Focal Loss. I have written about Focal Loss before [here](https://amaarora.github.io/2020/06/29/FocalLoss.html).

From my understanding, we use a combination loss for stable training. Since the evaluation metric is `dice`, we are using `dice` loss here. We use `bce` for pixel wise comparison between predictions and ground truth mask, and `focal` loss due to the high class imbalance - the actual mask is only a tiny part of the whole image. 

Let's also quickly define the optimizer and scheduler.

```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
scheduler = torch.optim.lr_scheduler.MultiStepLR(
    optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)
```

### Model Training
Now, we have all the necessary things to begin training. 

I reused the same training loop as in the [ISIC competition](https://amaarora.github.io/2020/08/23/siimisic.html) below:  

```python 
class AverageMeter:
    "keep record of batch scores and loss"
    def __init__(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=1, device='cuda'):
    losses = AverageMeter()
    model = model.to(device)
    model.train()
    if accumulation_steps > 1: 
        optimizer.zero_grad()
    tk0 = tqdm(train_loader, total=len(train_loader))
    for b_idx, data in enumerate(tk0):
        for key, value in data.items():
            data[key] = value.to(device)
        if accumulation_steps == 1 and b_idx == 0:
            optimizer.zero_grad()
        out  = model(data['image'])
        loss = loss_fn(out, data['mask'])
        with torch.set_grad_enabled(True):
            loss.backward()
            if (b_idx + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
        losses.update(loss.item(), train_loader.batch_size)
        tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])
    return losses.avg
```

The above training loop is a basic one with possible gradient accumulation to the small batch size in Image Segmentation. I personally trained my models without gradient accumulation, but you could very well train your models by passing in `accumulation_steps=N` where `N` is the number of times you want to accumulate the gradients. 

The `train_one_epoch` function accepts a `dataloader`, `model`, `optimizer` and `loss_fn`. We first move the model to `cuda` and put the model in train mode. Next, we iterate over the dataloader to get images and masks and move them to `cuda` as well.

Finally, we pass the batch of images to get predictions. Remember the output size is `(8, 1, 512, 512)` for three channel input images of height and width 512 and batch size 8. Once we have the predictions, as is standard for any training loop, we can calculate the loss and backpropogate to update the model weights. Finally, we also display the average loss in the training progress bar using `tqdm`.

That's really it.

To train the model we can simply run: 

```python
train_loss = train_one_epoch(train_dataloader, model, optimizer, criterion)
```

### Model Validation
Hardly ever would you just be concerned with model training. It is imperative to have a good validation step to check model performance. In this step we create the model validation loop which is similar to the training loop. 

```python
def evaluate(valid_loader, model, device='cuda', metric=dice_metric):
    losses = AverageMeter()
    model = model.to(device)
    model.eval()
    tk0 = tqdm(valid_loader, total=len(valid_loader))
    with torch.no_grad():
        for b_idx, data in enumerate(tk0):
            for key, value in data.items():
                data[key] = value.to(device)
            out   = model(data['image'])
            out   = torch.sigmoid(out)
            dice  = metric(out, data['mask']).cpu()
            losses.update(dice.mean().item(), valid_loader.batch_size)
            tk0.set_postfix(dice_score=losses.avg)
    return losses.avg
```

The above validation loop accepts a valid dataloader, trained model some metric to check model performance. It first moves the model to GPU, and puts the model in `eval` mode. Next we iterate over the dataloader to get the images and masks and pass the images through the model to get mask predictions. Finally, we take the sigmoid to convert raw logits to be in range (0, 1). Finally, we can calculate the dice score and display it in the validation progress bar with `tqdm`. 

## Conclusion
I hope that today, I was able to provide a good introductory and reporoducible script to train Image Segmentation based models. This was also my first experience with training image segmentation models and hope to write more about Unet architecture, loss functions and review image segmentation research papers in the future. 

For a complete working notebook with a submission script to Kaggle and pretrained models, refer [here](https://github.com/amaarora/amaarora.github.io/blob/master/nbs/Training.ipynb). Running the script should take around 2 minutes on a P100.

## Credits
We made use of `prepare_png.py` script from [Anuar Aymoldin's 1st place solution](https://github.com/sneddy/pneumothorax-segmentation) and also directly used the loss function. 

The model was created using [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) library. 