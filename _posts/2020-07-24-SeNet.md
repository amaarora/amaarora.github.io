# Squeeze and Excitation Networks Explained with PyTorch Implementation

1. TOC 
{:toc}

## Introduction
In this blog post, we will be looking at SeNet architecture from the research paper [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) and re-implement it from scratch in [PyTorch](https://pytorch.org/). 

From the paper:
> Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%.

Remember [ResNet](https://arxiv.org/abs/1512.03385)? SeNet builds on top of the popular ResNet architecture to also add weights to each channel of a convolution block, so that the network can also perform feature recalibration - or also take into account the relationship between channels.

## Squeeze-and-Excitation Block

The main idea behind the paper can be explained using the image below:

![](/images/senet_block.png "fig1: Squeeze-and-Excitation Block")

1. Take an input image `X` and perform a convolution operation to generate `U`. 
2. Perform the **squeeze** operation - reduce the image from `C x H x W` to `C x 1 x 1`  
3. Perform the **excitation** operation - take the output of **squeeze** operation and generate per-channel weights
4. Apply the weights to the feature map `U` to generate the SE Block output.

In other words, what an SE Block does is that it adds a set of weights to each channel of the input. So unlike a CNN where, each channel has equal weights, the output of SE Block is weighted based on the channel weights. 

The authors suggest that such a weighting for different channels performs different operations at dufferent depths inside the network. From the paper:
> In earlier layers, it excites infor- mative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to dif- ferent inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network.

Also, from the paper:
> We expect the learning of convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able to increase its sensitivity to informative features which can be exploited by subsequent transformations.

Let's quickly look at the **Squeeze** and **Excitation** blocks in a little more detail.

### Squeeze: Global Information Embedding
When performing convolution operation, since each filter operates on a local receptive field, it is unable to look at the information outside this region.

To mitigate this problem, the authors proposed to **squeeze** the global spatial information into a channel descriptor. This is achieved by using global average pooling to generate channel-wise statistics. 

### Excitation: Adaptive Recalibration
To make use of the global information, the authors followed the **squeeze** operation with **excitation** operation which aims to generate weights per channel and capture channel-wise dependencies. From the paper:

> To meet these criteria, we opt to employ a simple gating mechanism with a sigmoid activation:

![](/images/gating_operation.png "fig3: Excitation/gating operation")

where:
`δ` refers to ReLU operation
`σ` refers to Sigmoid operation 
`W1` and `W2` are two fully-connected layers

The two FC layers form a bottleneck-architecture, that is, the first `W1` layer is used for dimensionality reduction by a ratio `r` and the second `W2` layer is a dimensionality-increasing layer returning to the channel dimension of `U`.

Since, the Sigmoid layer would return numbers between 0-1, these are the channel weights and the final output of the block is obtained by:

![](/images/senet_output.png "fig4: SeNet output")

That is a weighted sum based on the channel weights.

Let's implement this in PyTorch to bolster our understanding.

## Squeeze-and-Excitation Block in PyTorch

```python
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)
```