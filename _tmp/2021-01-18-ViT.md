# (TIMM SERIES) ViT - AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

This blog is the beginning of some **truly exciting times** - want to know why? 

1. This is the first blog in the newly announced [TIMM SERIES](https://twitter.com/amaarora/status/1350410770052313088) and over time we will be covering off **all the models** that exist in one of my favourite libraries - [timm](https://github.com/rwightman/pytorch-image-models), used by thousands all over the world. Thanks [Ross Wightman](https://twitter.com/wightmanr) for creating such a wonderful library! 

> This means that not only would we be looking at the architectures in detail, but we will also understand how they are implemented in [timm](https://github.com/rwightman/pytorch-image-models), thus, looking at the code implementation too!

2. Welcome [Dr Habib Bukhari](https://twitter.com/dr_hb_ai)! You might know of him as [DrHB](https://www.kaggle.com/drhabib) on Kaggle - currently ranked **186/154,204** on the Kaggle ranking system! I am sure we all knew that he is great at deep learning but did you also know that he can also do some kickass visualizations to explain complicated concepts easily? And that, we have decided to team up for this and many more future blog posts to explain the concepts in an easy and visual manner. Together, we hope that you like what you read and see! Much effort and planning has gone into writing this blog post.

> To be honest, when Dr Habib first reached out to me and shared his visualizations - the first word that came out of my mouth was "Wow"! As you will see in this blog post and many more future ones, it's much better to have him do the visualizations than my scribblings using the iPad in my previous posts. Thank you Dr Habib for contributing to this blog and making it even better. :)

So, let's get started! This blog post has been structured in the following way: 

1. TOC 
{:toc}

## Prerequisite
In this blog post, I assume that the reader knows about the [Transformer Architecture](https://arxiv.org/abs/1706.03762). While it will be introduced briefly as part of this post, our main focus will on understanding how the authors of [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) from Google Brain applied the [Transformer Architecture](https://arxiv.org/abs/1706.03762) to computer vision. 

Here are some of my personal favourite resources on Transformers:
1. [The illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar
2. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) by Harvard NLP 
3. [Introduction to the Transformer](https://www.youtube.com/watch?v=AFkGPmU16QA&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=18&t=0s) by Rachel Thomas and Jeremy Howard

Though the next one is a biased recommendation, I would also like to recommend the reader to my previous post [The Annotated GPT-2](https://amaarora.github.io/2020/02/18/annotatedGPT2.html), for further reading on Transformers.

## Introduction
At the time of release, the [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) received quite a bit of "attention" from the community. This was the first paper to get some astonishing results on the [ImageNet dataset](http://www.image-net.org/) using the **Transformer architecture**. While there had been attempts made in the past to apply Transformers in the context of image processing ([1](https://arxiv.org/abs/1802.05751), [2](https://arxiv.org/abs/1906.05909), [3](https://arxiv.org/abs/2003.07853)), this paper was one of the first to apply Transformers to full-sized images. 

**NOTE:** I will be using the terms `Vision Transformer` & `ViT` interchangeably throughout this blog post and both refer to architecture described in [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) paper.


## Key Contributions
The key contributions from this paper were not in terms of a new architecture, but rather the application of an existing architecture (Transformers), to the field of Computer Vision. It is the **training method** and the **dataset used to pretrain the network**, that were key for **ViT** to get excellent results compared to SOTA (State of the Art) on ImageNet.

So, there aren't a lot of new things to introduce in this post, but rather how to use the existing Transformer architecutre and apply it to Computer Vision. Thus, if the reader knows about **Transformers**, this blog post and the research paper itself should be a fairly simple read.

## The Vision Transformer
We will be using a top down approach to understand the **Visual Transformer** architecture. We will first start by looking at the overall architecture and then dig deeper into each of the five steps in the overall architecture.

As an overall method, from the paper: 
>  We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017)

![](/images/ViT.png "fig-1 The Model Overview")

The overall architecture can be described easily in five simple steps below:
1. Split an input image into patches.
2. Get linear embeddings (representation) from each patch referred to as **Patch Embeddings**.
3. Add position embeddings and a `[cls]` token to each of the Patch Embeddings. 
4. Pass through a **Transformer Encoder** and get the output values for each of the `[cls]` tokens.
5. Pass the representations of `[cls]` tokens through a `MLP Head` to get final class predictions. 

> Note that there is a `MLP` inside the Transformer Encoder and a `MLP Head` that gives the class predictions, these two are different.

Not as descriptive? Enter *Dr Habib* to the rescue.

![](/images/vit-01.png "fig-2 Simplified Model Overview")

Let's now look at the five steps again with the help of `fig-2`. Let's image that we want to classify a `3` channel (RGB) input image of a frog of size `224 x 224`. 

The **first step** is to create patches all over the image of patch size `16 x 16`. Thus we create `14 x 14` or `196` such patches. We can have these patches in a straight line as in `fig-2`. As can be seen from the figure, the patch size is `3 x 16 x 16` where `3` represents the number of channels (RGB).

In the **second step**, we pass these patches through a **linear projection layer** to get `1 x 768` long vector representation for each of the image patches and these representations have been shown in purple in the figure. In the paper, the authors refer to these representations of the patches as **Patch Embeddings**. Can you guess what's the size of this patch embedding matrix? It's `196 x 768`. Because we had a total of `196` patches and each patch has been represented as a `1 x 768` long vector. Therefore, the total size of the patch embedding matrix is `[196 x 768]`.

> You might wonder why is the vector length `768`? Well, `3 x 16 x 16 = 768`. So, we are not really losing any information as step of this process of getting these **patch embeddings**.

In the **third step**, we take this patch embedding matrix of size `196 x 768` and similar to [BERT](https://arxiv.org/abs/1810.04805), the authors add a `[class]` token to this sequence of embedded patches and also add **Position Embeddings**. As we will see [here](https://github.com/amaarora/amaarora.github.io/blob/master/_tmp/2021-01-18-ViT.md#class-token--position-embeddings), when we dig deeper into this step later in the post, the size of patch embedding matrix now becomes `197 x 768` due to the `[class]` token being added and also the size of the position embedding matrix is `197 x 768`. 

> Why do we add this class token and position embeddings? You will find a detailed answer in the original Transformer and Bert papers, but to answer briefly, the `[class]` tokens are added as a special tokens whose outputs from the `Transformer Encoder` serve as the overall image patch representation. And we add the positional embeddings to retain the positional information of the patches. The Transformer model on it's own does not know about the order of the patches unlike CNNs, thus we need to manually inject some information about the relative or absolute position of the patches.

In the **fourth step**, we pass these preprocessed patch embeddings with positional information and prepended `[class]` tokens to the **Transformer Encoder** and get the learned embeddings of the `[class]` token. Thus the output representation would be of size `[1 x 768]` which are then fed to the `MLP Head` as part of the final **fifth step** to get class predicitions.

## Patch Embeddings
In this section we will be looking at **steps one and two** in detail. That is the process of <u>getting patch embeddings from an input image</u>.

![](/images/vit-02.png "fig-3 Patch Embeddings")

So far in the blog post I have mentioned that the way we get patch embeddings from an input image is to first split an image into fixed-size patches and then linearly embed each one of them using a **linear projection layer** as shown in `fig-2`. 

But, it is actually possible to combine both steps into a single step using **2D Convolution** operation. It is also better from an implementation perspective to do it this way as our GPUs are optimized to perform the convolution operation and it takes away the need to first split an image into patches. Let's see why this works?

If we set the the number of `out_channels` to `768`, and both `kernel_size` & `stride` to `16`, then as shown in `fig-3`, once we perform the convolution operation (where the 2-D Convolution has kernel size `3 x 16 x 16`), we can get the **Patch Embeddings** matrix of size `196 x 768` like below:

```python
# input image `B, C, H, W`
x = torch.randn(1, 3, 224, 224)
# 2D conv
conv = nn.Conv2d(3, 768, 16, 16)
conv(x).reshape(-1, 196).transpose(0,1).shape

>> torch.Size([196, 768])
```

## `[class]` token & Position Embeddings
In this section, let's look at the **third step** in more detail. In this step, we add `[class]` tokens and **Positional Embeddings** to the **Patch Embeddings**.

From the paper: 
> Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embedded patches, whose state at the output of the Transformer encoder (referred to as **Z<sub>L</sub><sup>0</sup>**) serves as the image representation. Both during pre-training and fine-tuning, a classification head is attached to **Z<sub>L</sub><sup>0</sup>**. 

> Position embeddings are also added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings and the resulting sequence of embedding vectors serves as input to the encoder.

let's understand this with the help of another visualization.

![](/images/vit-03.png "fig-4 `CLS` token and Position Embeddings")

As can be seen from `fig-4`, the `[class]` token is a vector of size `1 x 768`. We **prepend** it to the** Patch Embeddings**, thus the updated size of patch embedding matrix becomes `197 x 768`. Next, we **add** a **Positional Embedding** matrix of size `197 x 768` to the updated patch embedding matrix to get combined embeddings which are then ready to be fed to the `Transformer Encoder`. This is a pretty standard step that comes from the original Transformer paper - [Attention is all you need](https://arxiv.org/abs/1706.03762).

## The Transformer Encoder
This section of the blog post is not specific to Vision Transformers but a repeat of the Transformer Architecture in brief and covers **step-4**. If you already know of the Transformer Architectures, then, feel free to skip this section. If you are not aware of the Transformer Architecture, then I recommend you to go through any of the resources mentioned in the [Prerequisite](https://github.com/amaarora/amaarora.github.io/blob/master/_tmp/2021-01-18-ViT.md#prerequisite) section.

In this section, we will be looking into the **Transformer Encoder** from `fig-1` in detail. As shown in `fig-1`, the Transformer Encoder consists of alternating layers of multiheaded self-attention and MLP blocks. Also, as shown in `fig-1`, **Layer Norm** is used before every block and residual connections after every block.

The Transformer Encoder consists of 12 such layers in the ViT-Base Architecture as shown below.

![](/images/vit-06.png "fig-5 Transformer Encoder")

We already know how to get the **combined embeddings**. These combined embeddings are then fed into a 12 layer **Transformer Encoder** where each layer consists of Multi-headed Attention and MLP.

Let's look inside a single layer and what's going on.

![](/images/vit-04.png "fig-6 Encoder Block")

Each layer accepts a set of inputs, for the first layer, these inputs are the combined embeddings of shape `[197 x 768]`. Next, these inputs are passed through a Norm layer (Layer Norm), and fed to the multi-headed attention block. The inputs are first converted to `[197 x 768*3]` shape using a Linear layer to get the **qkv** matrix. Next we reshape this **qkv** matrix into `3 x 197 x 768` where each of the three matrices of shape `[197 x768]` represent the **q**, **k** and **v** matrices. These **q**, **k** and **v** matrices are further reshaped to `197 x 12 x 64` to represent the 12 attention heads. We finally perform the **Attention operation** which is given by the equation: 

![](/images/attention.png "eq-1 Attention")

Finally the output matrix fed into a `Norm` layer before being passed through a `MLP` network to get the outputs.

![](/images/vit-05.png "fig-7 MLP")

The MLP as mentioned in the paper: 
> The MLP contains two layers with a GELU non-linearity.

As can be seen from `fig-7`, the `MLP` consists of two layers followed by GELU non-linearity. The input and output dimensions remain the same for the `MLP` layer. The `MLP` Layer exists inside the Transformer Encoder layer. 

Thus, the final output from the **Transformer Encoder** is of shape `[197 x 768]`.

## MLP Head
In this section we look at the final **fifth step** of the Vision Transformer Architecture. Once, we get the `[197 x 768]` matrix as output from the Transformer Encoder, we just keep the representations for the `[class]` token at index 0 in this matrix. Thus we extract the `[class]` token learned representations that are of size `[1 x 768]`.

This vector is then finally fed into a MLP Head which is a Linear Layer to get final class predictions.

## The Vision Transformer in PyTorch 
To understand the code implementation of ViT in [timm](https://github.com/rwightman/pytorch-image-models), refer [here]().

## Conclusion 
I hope that through this blog, both Dr Habib and I have been able to explain all the magic that goes on inside the **Vision Transformer** Architecture. 

Once thanks to Dr Habib, I have been able to reference his beautiful and simplistic visualizations to explain each step in detail. 

As usual, in case we have missed anything or to provide feedback, please feel free to reach out to me at [@amaarora](https://twitter.com/amaarora) or Dr Habib at [@dr_hb_ai](https://twitter.com/dr_hb_ai).

Also, feel free to [subscribe to my blog here](https://amaarora.github.io/subscribe) to receive regular updates regarding new blog posts. Thanks for reading!